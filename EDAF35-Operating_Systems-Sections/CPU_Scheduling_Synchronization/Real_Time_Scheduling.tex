\subsection{Real-Time Scheduling}\label{subsec:Real_Time_Scheduling}
Real-time operating systems have their own class of scheduling issues.
This depends on whether the \nameref{def:Operating_System} is a \nameref{def:Soft_Real_Time_System} or a \nameref{def:Hard_Real_Time_System}.

\begin{definition}[Soft Real-Time System]\label{def:Soft_Real_Time_System}
  \emph{Soft real-time system}s do not provide a guarantee about the scheduling of a critical real-time process.
\end{definition}

\begin{definition}[Hard Real-Time System]\label{def:Hard_Real_Time_System}
  \emph{Hard real-time system}s guarantee the execution time of a real-time process.
  These tasks will be serviced by its deadline, otherwise the process will not be executed at all.
\end{definition}

POSIX also provides support for real-time scheduling through 2 functions with 2 scheduling types.
\begin{enumerate}[noitemsep]
\item \kernelinline{pthread_attr_getsched_policy(pthread_attr_t *attr, int *policy)}
\item \kernelinline{pthread_attr_setsched_policy(pthread_attr_t *attr, int policy)}
\end{enumerate}
\begin{enumerate}[noitemsep]
\item \texttt{SCHED\_FIFO}
\item \texttt{SCHED\_RR}
\end{enumerate}

\subsubsection{Minimizing Latency}\label{subsubsec:Minimizing_Latency}
The key aspect here is the amount of time it takes for a system to respond to an event.
This is called \nameref{def:Event_Latency}.

\begin{definition}[Event Latency]\label{def:Event_Latency}
  \emph{Event latency} is the amount of time that elapses from when an event occurs to when it is serviced.
  Different events can have different event latency requirements.
\end{definition}

There are 2 factors that affect \nameref{def:Event_Latency}.
\begin{enumerate}[noitemsep]
\item Interrupt Latency.
  The amount of time from the arrival of an \nameref{def:Interrupt} to the start of the Interrupt Service Routine (ISR).
  This includes the amount of time needed to get the currently running instruction to a point where it can be switched.
  Also included is the amount of time needed to perform the switch.
\item Dispatch Latency the amount of time the scheduler needs to stop one process and start another.
  There are 2 parts that affect the value of the dispatch latency:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:Preemption} of \textbf{ANY} process running tin the kernel.
  \item Release of resources used by low-priority process for higher-priority processes.
  \end{enumerate}
\end{enumerate}

\subsubsection{Scheduling}\label{subsubsec:Real_Time_Scheduling}
In this case, there are not as many choices of \nameref{def:Scheduling_Algorithm} for real-time systems as other systems.
All algorithms must be roughly based on a priority-based system all of which must support \nameref{def:Preemption}.

Most modern \nameref{def:Operating_System}s offer support for \nameref{def:Soft_Real_Time_System}s with their scheduling priorities.
Note however, that pure priority-based algorithms only guarantee soft real-time functionality, not hard.

Processes are considered periodic; they require the CPU at constant intervals.
Once a periodic process has acquired the CPU, it has a fixed processing time $t$, a deadline $d$ by which it must be serviced by the CPU, and a period $p$.
The relationship of the processing time, the deadline, and the period can be expressed as $0 \leq t \leq d \leq p$.
The rate of a periodic task is $\frac{1}{p}$.
Schedulers can take advantage of these characteristics and assign priorities according to a processâ€™s deadline or rate requirements.
What is different about this algorithm is a process may have to announce its deadline to the scheduler.
Using an admission-control algorithm, the scheduler either admits the process, guaranteeing that the process will complete on time, or rejects the request if it cannot guarantee that the task will be serviced by its deadline.

\paragraph{Rate-Monotonic Scheduling}\label{par:Rate_Monotonic_Scheduling}
\begin{definition}[Rate-Monotonic Scheduling]\label{def:Rate_Monotonic_Scheduling}
  \emph{Rate-monotonic scheduling} is a \nameref{def:Scheduling_Algorithm} for periodic tasks that uses a static priority policy with preemption.
  If a higher-priority process arrives, and a lower priority one is running, it is immediately preempted.
  The priority is statically calculated based on the inverse of the period of the task.
  Less frequent (longer period) tasks have a lower priority, and more frequent ones have higher priority.
  Additionally, the size of the CPU burst is assumed to be constant during every period.
\end{definition}

\nameref{def:Rate_Monotonic_Scheduling} is considered optimal because given a set of processes that cannot be scheduled by this algorithm, no other static-priority algorithm can schedule them either.

The worst-case CPU utilization for scheduling $N$ processes is shown in \Cref{eq:Rate_Monotonic_Worst_Case}
\begin{equation}\label{eq:Rate_Monotonic_Worst_Case}
  N \left( 2^{\frac{1}{N}} -1 \right)
\end{equation}

\paragraph{Earliest-Deadline-First Scheduling}\label{par:Earliest_Deadline_First_Scheduling}
\begin{definition}[Earliest-Deadline-First Scheduling]\label{def:Earliest_Deadline_First_Scheduling}
  \emph{Earliest-Deadline-First scheduling} dynamically assigns priorities based on the deadlines of tasks.
  The earlier/sooner the deadline, the higher the priority.
  To ensure this works, when a process becomes runnable, it must announce its deadline requirement.
\end{definition}

Unlike \nameref{def:Rate_Monotonic_Scheduling}, \nameref{def:Earliest_Deadline_First_Scheduling} does not require that processes be periodic, nor must a process require a constant amount of CPU time per burst.
EDF is theoretically optimal; it can schedule processes so that each process can meet its deadline requirements and CPU utilization will be 100 percent.
However, it is impossible to achieve this due to the cost of \nameref{def:Context_Switch}ing between \nameref{def:Process}es and \nameref{def:Interrupt} handling.

\paragraph{Proportional Share Scheduling}\label{par:Proportional_Share_Scheduling}
\begin{definition}[Proportional Share Scheduling]\label{def:Proportional_Share_Scheduling}
  In \emph{proportional share scheduling}, the CPU execution time is split into $T$ shares of time.
  Each application receives $N$ shares of that $T$, $\frac{N}{T}$ shares of total processing time.
  In addition to this, there needs to be an admission-control policy to guarantee that an application is run only if there are enough time slots for the process to execute.
\end{definition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
