\subsection{Process/Thread Synchronization}\label{subsec:Synchronization}
The main problem that occurs in multi\nameref{def:Thread}ed programs is that there is a small portion of code that is a \nameref{def:Critical_Section}.
This leads to the development of the \nameref{subsubsec:Critical_Section_Problem}.

\begin{definition}[Critical Section]\label{def:Critical_Section}
  The \emph{critical section} of a \nameref{def:Process} is a portion where the \nameref{def:Thread} and/or \nameref{def:Process} is changing common variables, updating a table, writing a file, or other global state changes.
\end{definition}

\subsubsection{Critical Section Problem}\label{subsubsec:Critical_Section_Problem}
The \emph{Critical Section Problem} is the issue of coordinating multiple \nameref{def:Thread}s about a \nameref{def:Critical_Section} of the code.
The problem is to design a protocol that the \nameref{def:Process}es/\nameref{def:Thread}s can use to cooperate.
Each \nameref{def:Process} must request permission to enter its critical section.
The section of code implementing this request is the \nameref{def:Entry_Section}.
The critical section may be followed by an \nameref{def:Exit_Section}.
The remaining code is the \nameref{def:Remainder_Section}.

\begin{definition}[Entry Section]\label{def:Entry_Section}
  The \emph{entry section} of a \nameref{def:Process} is the portion where the request to execute the \nameref{def:Critical_Section} occurs.
  In the case of a \nameref{def:Mutex}, this is the process of aquiring the it.
  For a \nameref{def:Semaphore}, it is the process of manipulating the value it currently contains.
\end{definition}

\begin{definition}[Exit Section]\label{def:Exit_Section}
  In the \emph{exit section}, the constructs used to ensure coordination in the \nameref{def:Critical_Section} are freed.
  In the case of a \nameref{def:Mutex}, this is the process of releasing the it.
  For a \nameref{def:Semaphore}, it is the process of manipulating the value it currently contains in the opposite direction it was initially manipulated by.
\end{definition}

\begin{definition}[Remainder Section]\label{def:Remainder_Section}
  The \emph{remainder section} is the rest of the code, after this \nameref{def:Critical_Section}.
  This code may be parallelized, or not.
  It could contain further \nameref{def:Critical_Section}s.
\end{definition}

Any solution to this problem \textbf{MUST} satisfy one of the following 3 requirements:
\begin{enumerate}[noitemsep]
\item \textbf{Mutual Exclusion}.
  If \nameref{def:Process} $P_{i}$ is executing its \nameref{def:Critical_Section}, then \textbf{no other} processes can execute their critical sections.
\item \textbf{Progress}.
  If no \nameref{def:Process} is executing its \nameref{def:Critical_Section}, and some processes wish to enter their critical sections, then only those processes that \textbf{are not executing} in their \nameref{def:Remainder_Section}s can decide which will enter the \nameref{def:Critical_Section} next.
  Essentially, the only way a process gets a voice in the choice is by not having executed the critical section yet.
\item \textbf{Bounded Waiting}.
  There exists a bound on the number of times that other \nameref{def:Process}es are allowed to enter their \nameref{def:Critical_Section}s after a process has made a request to enter its critical section and before that request is granted.
\end{enumerate}

To handle the \nameref{subsubsec:Critical_Section_Problem}, there are 2 main types of \nameref{def:Kernel}s that present solutions.
\begin{enumerate}[noitemsep]
\item \nameref{def:Nonpreemptive_Kernel}s. Not used frequently today.
\item \nameref{def:Preemptive_Kernel}s. The most common type today.
\end{enumerate}

\begin{definition}[Nonpreemptive Kernel]\label{def:Nonpreemptive_Kernel}
  A \emph{nonpreemptive kernel} is a \nameref{def:Kernel} that does \textbf{NOT} use \nameref{def:Preemption} on \nameref{def:Process}es or \nameref{def:Thread}s running in kernel-mode.
\end{definition}

\begin{definition}[Preemptive Kernel]\label{def:Preemptive_Kernel}
  A \emph{preemptive kernel} is a \nameref{def:Kernel} that uses \nameref{def:Preemption} on \nameref{def:Process}es or \nameref{def:Thread}s running in kernel-mode.
  This means that we cannot say anything definitive about the state of the \nameref{def:Kernel}'s data structures at a given time, because we cannot say which process/thread is running at that time.
\end{definition}

\subsubsection{Hardware Support for Synchronization}\label{subsubsec:Hardware_Support_Synchronization}
Software-based solutions to handling multithreading and multiprocessing tends to be better than hardware-based solutions, as they are more flexible.
Many of the solutions that will be presented here are based on the idea of \textbf{\nameref{def:Lock}ing}.

\begin{definition}[Lock]\label{def:Lock}
  A \emph{lock} allows \textbf{only one} \nameref{def:Thread} to enter the portion of code that is locked.
  While a thread holds this lock no other \nameref{def:Thread} can execute on this code portion.

  \begin{remark}[Binary Semaphore]\label{rmk:Binary_Semaphore}
    Locks can be represented as \emph{binary \nameref{def:Semaphore}}s.
  \end{remark}
\end{definition}

In a single-processor system, we can solve the \nameref{subsubsec:Critical_Section_Problem} by preventing interrupts from being handled.
This would prevent the currently running instruction from being preempted in any way, and allow it to finish.
However, this does not really work on a multiprocessor system, because disabling interrupts and their handling on all processors is time consuming.

However, the idea of certain instructions being \nameref{def:Atomic} is an elegant solution to the \nameref{subsubsec:Critical_Section_Problem}.
So, most computer systems provide special hardware-level instructions that allow us to test and modify the contents of a word, or swap the contents of 2 words \nameref{def:Atomic}ally.

\begin{definition}[Atomic]\label{def:Atomic}
  An \emph{atomic} operation is one that cannot be interrupted, preempted, or altered in any way.
  As soon as an atomic operation begins, the system \textbf{MUST} finish handling it before it may do anything else.
\end{definition}

Some operations on data are possible to do at any given point in time, without affecting the potential outcome.
One example of this is \textbf{reading} from a location in memory.
However, if this location can also be written to, we need to limit the number of writers.
Additionally, if someone is waiting to write, they should get some priority over anything waiting to read.
Thus, the \nameref{def:Read_Write_Lock} was created.

\begin{definition}[Read/Write Lock]\label{def:Read_Write_Lock}
  \emph{Read/Write Lock}s allow either an unlimited number of readers \textbf{OR} 1 writer at any given time.
  Writers will be scheduled to use the lock sooner than readers, so the value is updated first, before anyone reads it again.
  But, the writer will have to wait until everyone currently reading the value is done reading, otherwise the value in memory will change underneath the readers.
\end{definition}

\subsubsection{Mutex Locks}\label{subsubsec:Mutex_Locks}
The hardware-based solutions presented in \Cref{subsubsec:Hardware_Support_Synchronization} are typically not available to application programmers.
Instead, operating system designers build software tools to handle the \nameref{subsubsec:Critical_Section_Problem}.
The simplest tool is that of a \nameref{def:Mutex}.

\begin{definition}[Mutex]\label{def:Mutex}
  A \emph{mutex} (short for \emph{\textbf{mut}ual \textbf{ex}clusion}) is the same as a \nameref{def:Lock}, \textbf{but it can be system wide (shared by multiple processes)}.
  A mutex lock protects critical regions and prevents race conditions.
  That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section.

  The \kernelinline{acquire()} function acquires the lock, preventing any other \nameref{def:Thread} and/or \nameref{def:Process} from using the thing the lock protects.
  Likewise, the \kernelinline{release()} function releases the lock, allowing another \nameref{def:Thread} and/or \nameref{def:Process} take acquire the lock and use the resource it protects.
  To perform its function correctly, the lock's \kernelinline{acquire()} and \kernelinline{release()} functions must be \nameref{def:Atomic}.

  When a \nameref{def:Thread} and/or \nameref{def:Process} attempts \texttt{acquire()} the lock, while it is already owned by someone else, it is put in a \texttt{WAITING} state.
\end{definition}

There are a variety of \nameref{def:Mutex}es, depending on the way the way they are used is implemented.
In one variety, the \nameref{def:Spinlock}, the \nameref{def:Process}/\nameref{def:Thread} that is attempting to \texttt{acquire} the lock will continuously call the \kernelinline{acquire()} function until it gets the lock.

\begin{definition}[Spinlock]\label{def:Spinlock}
  A \emph{spinlock} is one way of having a system use a \nameref{def:Mutex} lock.
  In this implementation, the \nameref{def:Process}/\nameref{def:Thread} that wants to \texttt{acquire} the lock loops continuously, calling \kernelinline{acquire()} until it gets it.
  This means that there is no \nameref{def:Context_Switch} to another task.

  This means that while a \nameref{def:Process}/\nameref{def:Thread} is not executing anything useful and may be hogging the CPU's cycles, if the lock is freed soon, then no \nameref{def:Context_Switch} back is required.
\end{definition}

\subsubsection{Semaphores}\label{subsubsec:Semaphores}
\nameref{def:Semaphore}s are a more general way of approaching resource allocation and mutual exclusion within a system.
Because of this generality, there are 2 kinds of \nameref{def:Semaphore}s:
\begin{enumerate}[noitemsep]
\item \nameref{par:Counting_Semaphore}
\item \nameref{par:Binary_Semaphore}
\end{enumerate}

\begin{definition}[Semaphore]\label{def:Semaphore}
  A \emph{semaphore} regulates the number of things (\nameref{def:Process}es or \nameref{def:Thread}s) performing operations on something (Shared Resource).
  Functionally, this is the same as a \nameref{def:Mutex} but allows $x$ \nameref{def:Process}es/\nameref{def:Thread}s to enter or use the resource at a time.
  This allows for limits on the number of CPU, I/O or RAM intensive tasks running at the same time.

  A semaphore can only be interacted with through 2 operations \kernelinline{wait()} and \kernelinline{signal()}.
  \kernelinline{wait()} is similar to a \nameref{def:Mutex}'s \kernelinline{acquire()} function and the \kernelinline{signal()} function is similar to the \nameref{def:Mutex}'s \kernelinline{release()} function.
  Here, if a \nameref{def:Process} or \nameref{def:Thread} \texttt{wait}s, if there is more of the resource, then the requester gets the resource, and the internal count of the semaphore is decremented.
  If a \nameref{def:Process} or \nameref{def:Thread} \texttt{signal}s, then it is done with the resource, and the requester loses access to the resources, and the internal count of the semaphore is incremented.
  Again, these manipulations \textbf{MUST} be \nameref{def:Atomic}.

  \begin{remark}[Confusion with Mutexes]\label{rmk:Semaphore_Mutex_Confusion}
    Technically, you can create a \nameref{def:Semaphore} that acts like a \nameref{def:Mutex} by giving it a binary value.
    However, this is typically in poor programming taste, because while both function similarly, the \nameref{def:Semaphore} is for signalling the amount of a resource available and the \nameref{def:Mutex} is for signalling if code is capable of execution.
  \end{remark}

  \begin{remark}[Correct Use of Semaphores]\label{rmk:Semaphore_Correct_Usage}
    The correct use of a \nameref{def:Semaphore} is for signaling from one task to another.
    A \nameref{def:Mutex} is meant to be taken and released, always in that order, by each task that uses the shared resource it protects.
    By contrast, tasks that use \nameref{def:Semaphore}s either signal or wait, not both.
  \end{remark}
\end{definition}

A \nameref{def:Semaphore} contains not only its integer value, but also a pointer to the list of waiting \nameref{def:Thread}s/\nameref{def:Process}es.
This way the things waiting on the \nameref{def:Semaphore}'s values can be reached.
However, we now have issues with \nameref{def:Starvation} and \nameref{def:Deadlock}.

\begin{definition}[Starvation]\label{def:Starvation}
  \emph{Starvation} is when a thing is waiting for a resource indefinitely.
  For example, if a \nameref{def:Process}/\nameref{def:Thread} is waiting for a \nameref{def:Semaphore} to become available, and these requesters are services in a Last-In First-Out order, its possible the first requester to enter the queue never leaves.
\end{definition}

\nameref{def:Semaphore}s have some additional issues to go with them:
\begin{itemize}[noitemsep]
\item A process might never release a resource once it has been granted access to the resource.
\item A process might attempt to release a resource that it never requested.
\item A process might request the same resource twice (without first releasing the resource).
\end{itemize}

\paragraph{Counting Semaphore}\label{par:Counting_Semaphore}
A \emph{counting \nameref{def:Semaphore}} allows for the value contained within the \nameref{def:Semaphore} to range over a domain of integer values.
For example, the integers from 0 to 10, allowing for 10 different users to access the same resource at once.

Once the \nameref{def:Semaphore} count reaches 0, nothing else can use the resource until one of the current users relinquishes control.
So, the first \nameref{def:Process}/\nameref{def:Thread} that uses \kernelinline{wait()} when the \nameref{def:Semaphore}'s internal value is 0 will be put in either a \nameref{def:Spinlock}-like state, or will be put into a waiting queue.

Typically, the \nameref{def:Process}es/\nameref{def:Thread}s that are using the \nameref{def:Semaphore} will be qusing it for quite some time, new requesters are put into a waiting queue/state.
Then, the \nameref{def:Scheduler} is called, performing a \nameref{def:Context_Switch} to a \nameref{def:Thread}/\nameref{def:Process} that is ready to run, and \nameref{def:Context_Switch}es.

Thus, whenever another user of the \nameref{def:Semaphore} \texttt{signal}s, one of the \nameref{def:Process}es/\nameref{def:Thread}s should be awoken from this waiting state, moved to the ready state, and be made schedule-able.
Which waiting thing is chosen depends on the way the queue is structured, and how things get scheduled, and many other factors.

\begin{blackbox}
In this system, we don't guarantee immediate execution of the thing requesting the \nameref{def:Semaphore}, only that it can be scheduled.
\end{blackbox}

How this queue is structured is different between every \nameref{def:Kernel}, but it is typically constructed out of the \nameref{def:Process_Control_Block} or the \nameref{def:Thread} control block.

\subparagraph{Negative Semaphore Values}\label{subpar:Negative_Semaphore_Val}
In some implementations of a \nameref{def:Semaphore}, the internal value can become negative.
In the strict, classical definition of a \nameref{def:Semaphore}, this internal value \textbf{CANNOT} be negative.
However, if we allow the negative value, and the \nameref{def:Semaphore} reaches it, it represents the number of things that are waiting on the \nameref{def:Semaphore} to be freed up.

\paragraph{Binary Semaphore}\label{par:Binary_Semaphore}
A \emph{binary \nameref{def:Semaphore}} only allows the internal value within the system to be zero or one.
This effectively acts as a \nameref{def:Mutex}.

\subsubsection{Priority Inversion}\label{subsubsec:Priority_Inversion}
\emph{Priority Inversion} is a scheduling challenge where a higher-priority process needs to read or modify kernel data that is currently being used by a lower-priority process.
Since the kernel data is typically protected with a \nameref{def:Lock}, the higher-priority process must wait for a lower-priority one to finish with that resource.
The situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority.

It occurs only in systems with more than two priorities.
Typically these systems solve the problem by implementing a \nameref{def:Priority_Inheritance_Protocol}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
