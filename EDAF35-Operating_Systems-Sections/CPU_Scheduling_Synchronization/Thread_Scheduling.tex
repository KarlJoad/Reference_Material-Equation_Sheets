\subsection{Thread Scheduling}\label{subsec:Thread_Scheduling}
\begin{blackbox}
  \textbf{On operating systems that support them, \nameref{def:Kernel_Thread}s, not \nameref{def:Process}es are scheduled by the operating system.}
  However, the terms ``process scheduling'' and ``thread scheduling'' are often used interchangeably.
  Process scheduling is used when discussing general scheduling concepts and thread scheduling to refer to thread-specific ideas.
\end{blackbox}

\subsubsection{User- vs. Kernel-Level Thread Scheduling}\label{subsubsec:User_vs_Kernel_Thread_Scheduling}
On systems implementing the \nameref{subsubsec:Many_To_One_Model} (\Cref{subsubsec:Many_To_One_Model}) and \nameref{subsubsec:Many_To_Many_Model} (\Cref{subsubsec:Many_To_Many_Model}), the thread library schedules user-level threads to run on an available LWP.\@

\begin{definition}[Process-Contention Scope]\label{def:Process_Contention_Scope}
  \emph{Process-Contention Scope} is where each of the \nameref{def:Thread}s \textbf{WITHIN A SINGLE \nameref{def:Process}} compete with each other for execution.
\end{definition}

In \nameref{def:Process_Contention_Scope}, the \nameref{def:Thread_Library} schedules the \nameref{def:User_Thread}s onto available \nameref{def:Lightweight_Process}es, or directly onto \nameref{def:Kernel_Thread}s.
However, this does not mean that the \nameref{def:User_Thread} are being scheduled for execution.
For that to happen, the \nameref{def:Kernel} must be involved.

To decide what \nameref{def:Thread}s can execute on the CPU, we broaden our view of contention to \nameref{def:System_Contention_Scope}.

\begin{definition}[System-Contention Scope]\label{def:System_Contention_Scope}
  \emph{System-Contention Scope} is where \textbf{ALL} of the \nameref{def:Thread}s on the system, from all \nameref{def:Process}es compete with each other for execution.
\end{definition}

If a \nameref{def:Kernel} uses the \nameref{subsubsec:One_To_One_Model}, then \textbf{ALL} threads are scheduled using \nameref{def:System_Contention_Scope}.

\subsubsection{Multiprocessor Scheduling}\label{subsubsec:Multiprocessor_Scheduling}
If multiple processors are available for use in a system, then \nameref{def:Load_Sharing} is possible.

\begin{definition}[Load Sharing]\label{def:Load_Sharing}
  \emph{Load sharing} involves splitting the load of running \nameref{def:Process}es and their \nameref{def:Thread}s between multiple processors.
  This does not limit the possibility of multithreading, rather it complements it.
\end{definition}

In this course, we consider each processor to be equivalent, or homogenous.
This means that each one will have the same basic set of functionality and can reach all resources present in the system.
This does not mean that all processors can reach all the resources with the same cost/delay.
There may be an I/O device attached to the private bus of a core, or there may be \nameref{def:Non_Uniform_Memory_Access}.

\paragraph{Approaches to Multiprocessor Scheduling}\label{par:Multiprocessor_Scheuling_Approaches}
There are 2 main approaches to multiprocessor scheduling, without taking the idea of non-uniform resource availability.
\begin{enumerate}[noitemsep]
\item \nameref{def:Asymmetric_Multiprocessor_System}
\item \nameref{def:Symmetric_Multiprocessor_System}
\end{enumerate}

In an \nameref{def:Asymmetric_Multiprocessor_System}, one of the processors acts as the master server.
It runs the scheduler, allocating jobs to each of the slave worker processors.
This reduces the need for data sharing, making coding and debugging such a thing much easier.

The other approach, \nameref{def:Symmetric_Multiprocessor_System}, allows each processor to schedule itself.
Tasks to execute may be in 2 types of queues:
\begin{enumerate}[noitemsep]
\item A common queue for all processors.
\item A private queue, one for each processor.
\end{enumerate}

Each processor must select a job from the ready queue and execute it.
We must ensure that no two processors schedule and execute the same task at the same time, and that processes are not lost from the queue.

\begin{blackbox}
  Throughout this section, and most of this document, we discuss \nameref{def:Symmetric_Multiprocessor_System} (SMP) systems.
  These are more commonly in use in the desktop space and are more interesting to discuss.
\end{blackbox}

\subsubsection{Processor Affinity}\label{subsubsec:Processor_Affinity}
\nameref{def:Process}es/\nameref{def:Thread}s can migrate between processors, no matter the type of multiprocessor system used.
However, if such a migration happens, the processor's cache must be invalidated from the origin and the destination cache must be populated.
This is a very high cost to system productivity, so we attempt to keep a \nameref{def:Process}/\nameref{def:Thread} running on the same processor for as long as possible, this is known as \nameref{def:Processor_Affinity}.

\begin{definition}[Processor Affinity]\label{def:Processor_Affinity}
  \emph{Processor affinity} is the idea of keeping one \nameref{def:Process}/\nameref{def:Thread} running on the \textbf{same} processor for as long as possible.
  This prevents relatively costly process migrations from becoming common.

  There are 2 kinds of processor affinity:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:Soft_Affinity}
  \item \nameref{def:Hard_Affinity}
  \end{enumerate}
\end{definition}

\begin{definition}[Soft Affinity]\label{def:Soft_Affinity}
  \emph{Soft affinity} is where \nameref{def:Processor_Affinity} is used, but no guarantees about the migration of a \nameref{def:Process} can be made.
  The \nameref{def:Operating_System} will attempt to keep a process running on the same processor for as long as possible, but the process can still migrate between processors.
\end{definition}

\begin{definition}[Hard Affinity]\label{def:Hard_Affinity}
  \emph{Hard affinity} is where once a \nameref{def:Process}/\nameref{def:Thread} is assigned to a set of processors, it will \textbf{ONLY EXECUTE ON THOSE}.
\end{definition}

The organization of \nameref{def:Memory} in a system will affect the performance of the \nameref{def:Processor_Affinity} choices made.
In a \nameref{def:Non_Uniform_Memory_Access} system, not all memory is \textbf{DIRECTLY} available to every processor.
In this type of system, a \nameref{def:Process}'s affinity should be set according to the memory location it inhabits.
Otherwise, there will be long delays during memory acccesses.
This is illustrated in \Cref{fig:NUMA_Multiprocessor_Scheduling}.

\begin{figure}[h!tbp]
  \centering
  \includegraphics[scale=1.00]{./Drawings/EDAF35-Operating_Systems/NUMA_Multiprocessor_Scheduling.jpg}
  \caption{\nameref*{def:Non_Uniform_Memory_Access}, \nameref*{def:Processor_Affinity}, and \nameref*{rmk:CPU_Scheduler} Choices}
  \label{fig:NUMA_Multiprocessor_Scheduling}
\end{figure}

\subsubsection{Load Balancing}\label{subsubsec:Load_Balancing}
Because we have multiple processors available to us, we need to make sure they are all doing a fair amount of work.
We do not want to have one processor sitting idle while others have very high loads with long queues.
This forms the basis of \nameref{def:Load_Balancing}.

\begin{definition}[Load Balancing]\label{def:Load_Balancing}
  \emph{Load balancing} is the process of making sure that every processor in a system is assigned an even amount of work.
  This ensures that \textbf{ALL} processors are active at one time, preventing the issue of some processors sitting idle while others have very high loads and long queues.

  There are 2 main ways to provide load balancing:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:Push_Migration}
  \item \nameref{def:Pull_Migration}
  \end{enumerate}

  \begin{remark}[Common vs. Private Queues]\label{rmk:Load_Balancing_Common_Private_Queues}
    Like stated earlier in this section, some multiprocessor designs use a common task queue for all processors, and others use private queues for each individual processor.
    In the case of a common queue, then \nameref{def:Load_Balancing} is unnecessary, because each processor will extract a job from the common queue.
    \nameref{def:Load_Balancing} is only necessary on systems that use private queues, which most multiprocessors today use to some extent.
  \end{remark}
\end{definition}

\nameref{def:Push_Migration} and \nameref{def:Pull_Migration} \textbf{ARE NOT} mutually exclusive.
In fact, most systems support both.

\begin{definition}[Push Migration]\label{def:Push_Migration}
  In \emph{push migration}, there is a task inserted into the queue that checks the load on each processor.
  If it finds a large imabalance, then that processor's tasks are evenly redistributed among all other processors.
\end{definition}

\begin{definition}[Pull Migration]\label{def:Pull_Migration}
  \emph{Pull migration} is where each processor, once it becomes idle, goes to other processors and attempts to pull a waiting task from the other processor's queue.
\end{definition}

\nameref{def:Load_Balancing} counteracts the benefits of \nameref{def:Processor_Affinity}.
The benefit of keeping a process running on the same processor is that the process can take advantage of its data
being in that processorâ€™s cache memory.
Either pulling or pushing a process from one processor to another removes this benefit.

\subsubsection{Multicore Processors}\label{subsubsec:Multicore_Processors}
In more recent times, multiple processors are being built onto the same physical chip, making a \emph{multicore processor}.
To the \nameref{def:Operating_System}, they appear as separate CPUs, because each core maintains its own architectural state.
This allows for multicore processor systems that are faster and consume less power than traditional multiprocessor systems.

The rise of multicore processors has also lead to the rise of hardware threads.
This is because a \nameref{def:CPU} spends much of its execution time waiting for \nameref{def:Memory} and its contents to become available.
When a CPU waits for memory to returns its contents, it is called a \nameref{def:Memory_Stall}.

\begin{definition}[Memory Stall]\label{def:Memory_Stall}
  A \emph{memory stall} is when a CPU must go to memory to retrieve a value.
  This could happen because of a cache miss, for instance.
\end{definition}

By having multiple hardware threads, when one of the threads has a \nameref{def:Memory_Stall}, the other thread can take over execute while the first waits for the memory to be returned.
These are called \emph{logical processors}.

\begin{blackbox}
  Each of these logical processors appears to the \nameref{def:Operating_System} as a separate physical CPU.\@
\end{blackbox}

The number of processors that the \nameref{def:Operating_System} ``sees'' can be calculated by \Cref{eq:Num_Cores}.

\begin{equation}\label{eq:Num_Cores}
  C = Pt
\end{equation}
\begin{description}[noitemsep]
\item $C$: The number of cores the \nameref{def:Operating_System} sees.
\item $P$: The number of processors present in the system.
\item $t$: The number of hardware threads present in each processor.
\end{description}

There are 2 ways to multithread a processor core:
\begin{enumerate}[noitemsep]
\item \nameref{def:Coarse_Grained_Multithreading}
\item \nameref{def:Fine_Grained_Multithreading}
\end{enumerate}

\begin{definition}[Coarse-Grained Multithreading]\label{def:Coarse_Grained_Multithreading}
  In \emph{coarse-grained multithreading}, a hardware thread will execute until it reaches a long-latency event, such as a \nameref{def:Memory_Stall}.
  The processor then switches to the other thread and continues execution while waiting for the long-latency event to return.

  This has a high cost, because the entire instruction pipeline must have the first thread's instructions emptied and the new thread's instructions fill the pipeline.
  This is quite costly.
\end{definition}

\begin{definition}[Fine-Grained Multithreading]\label{def:Fine_Grained_Multithreading}
  \emph{Fine-grained multithreading} switches between the hardware threads much more frequently and quickly, typically between every other instruction.
  To ensure this does not take too much time, hardware logic is built-in for thread switching.
\end{definition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
