\subsection{Thread Scheduling}\label{subsec:Thread_Scheduling}
\begin{blackbox}
  \textbf{On operating systems that support them, \nameref{def:Kernel_Thread}s, not \nameref{def:Process}es are scheduled by the operating system.}
  However, the terms ``process scheduling'' and ``thread scheduling'' are often used interchangeably.
  Process scheduling is used when discussing general scheduling concepts and thread scheduling to refer to thread-specific ideas.
\end{blackbox}

\subsubsection{User- vs. Kernel-Level Thread Scheduling}\label{subsubsec:User_vs_Kernel_Thread_Scheduling}
On systems implementing the \nameref{subsubsec:Many_To_One_Model} (\Cref{subsubsec:Many_To_One_Model}) and \nameref{subsubsec:Many_To_Many_Model} (\Cref{subsubsec:Many_To_Many_Model}), the thread library schedules user-level threads to run on an available LWP.\@

\begin{definition}[Process-Contention Scope]\label{def:Process_Contention_Scope}
  \emph{Process-Contention Scope} is where each of the \nameref{def:Thread}s \textbf{WITHIN A SINGLE \nameref{def:Process}} compete with each other for execution.
\end{definition}

In \nameref{def:Process_Contention_Scope}, the \nameref{def:Thread_Library} schedules the \nameref{def:User_Thread}s onto available \nameref{def:Lightweight_Process}es, or directly onto \nameref{def:Kernel_Thread}s.
However, this does not mean that the \nameref{def:User_Thread} are being scheduled for execution.
For that to happen, the \nameref{def:Kernel} must be involved.

To decide what \nameref{def:Thread}s can execute on the CPU, we broaden our view of contention to \nameref{def:System_Contention_Scope}.

\begin{definition}[System-Contention Scope]\label{def:System_Contention_Scope}
  \emph{System-Contention Scope} is where \textbf{ALL} of the \nameref{def:Thread}s on the system, from all \nameref{def:Process}es compete with each other for execution.
\end{definition}

If a \nameref{def:Kernel} uses the \nameref{subsubsec:One_To_One_Model}, then \textbf{ALL} threads are scheduled using \nameref{def:System_Contention_Scope}.

\subsubsection{Multiprocessor Scheduling}\label{subsubsec:Multiprocessor_Scheduling}
If multiple processors are available for use in a system, then \nameref{def:Load_Sharing} is possible.

\begin{definition}[Load Sharing]\label{def:Load_Sharing}
  \emph{Load sharing} involves splitting the load of running \nameref{def:Process}es and their \nameref{def:Thread}s between multiple processors.
  This does not limit the possibility of multithreading, rather it complements it.
\end{definition}

In this course, we consider each processor to be equivalent, or homogenous.
This means that each one will have the same basic set of functionality and can reach all resources present in the system.
This does not mean that all processors can reach all the resources with the same cost/delay.
There may be an I/O device attached to the private bus of a core, or there may be \nameref{def:Non_Uniform_Memory_Access}.

\paragraph{Approaches to Multiprocessor Scheduling}\label{par:Multiprocessor_Scheuling_Approaches}
There are 2 main approaches to multiprocessor scheduling, without taking the idea of non-uniform resource availability.
\begin{enumerate}[noitemsep]
\item \nameref{def:Asymmetric_Multiprocessor_System}
\item \nameref{def:Symmetric_Multiprocessor_System}
\end{enumerate}

In an \nameref{def:Asymmetric_Multiprocessor_System}, one of the processors acts as the master server.
It runs the scheduler, allocating jobs to each of the slave worker processors.
This reduces the need for data sharing, making coding and debugging such a thing much easier.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
