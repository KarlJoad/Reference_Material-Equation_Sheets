\subsection{Implicit Threading}\label{subsec:Implicit_Threading}
Because programs are starting to use so many \nameref{def:Thread}s that it is becoming hard to manage, the creation and management of threads is moving from developers to compilers/run-time libaries.
This way, the computer manages threads rather than the programmer.

\begin{definition}[Implicit Threading]\label{def:Implicit_Threading}
  \emph{Implicit Threading} is the transfer of \nameref{def:Thread} creation and management away from the programmer, and to the compiler and/or run-time libaries.
  This frees the programmer from having to think/worry about the issues that arise because of multithreading, while still allowing programs to take advantage of the benefits of multithreading.
\end{definition}

There exist 3 main methods for implementing implicit threading:
\begin{enumerate}[noitemsep]
\item \nameref{subsubsec:Thread_Pools}
\item \nameref{subsubsec:OpenMP}
\item \nameref{subsubsec:Grand_Central_Dispatch}
\end{enumerate}

\subsubsection{Thread Pools}\label{subsubsec:Thread_Pools}
In a \nameref{def:Thread_Pool} system, all (or at least most) of the \nameref{def:Thread}s available for use by a \nameref{def:Process} are created during startup.
They are then placed in a pool, and wait for work to arrive.

\begin{definition}[Thread Pool]\label{def:Thread_Pool}
  A \emph{thread pool} system is one where all \nameref{def:Thread}s that can be used by any \nameref{def:Process} on the system is in a pool, hence the name.
  When a job comes in that would use one (or more) of these threads, they are pulled out of the pool and allowed to execute.
  When they finish execution, they return to the pool.

  If there are jobs ready, but there are no threads available in the pool, they wait until one is available.
\end{definition}

A \nameref{def:Thread_Pool} offers these benefits:
\begin{enumerate}[noitemsep]
\item Servicing a request with an \textbf{existing thread} is faster than waiting \textbf{to create a thread}.
\item A thread pool limits the number of threads that exist at any one point, preventing performance degradation.
  This is particularly important on systems that cannot support a large number of concurrent threads.
\item Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task.
\end{enumerate}

Additionally, the number of \nameref{def:Thread}s available in the \nameref{def:Thread_Pool} can be set dynamically.
Some factors that can affect the number of threads in the pool are:
\begin{enumerate}[noitemsep]
\item Number of CPUs in the system
\item Amount of physical memory
\item Expected number of concurrent job requests
\end{enumerate}

There are more sophisticated architectures that offer varying benefits.
Some even allow for the shrinking of the pool as needed, to reduce the footprint of the running \nameref{def:Process}.

\subsubsection{OpenMP}\label{subsubsec:OpenMP}
OpenMP is a set of compiler directives as well as an API for programs written in C, C++, or FORTRAN that provides support for parallel programming in shared-memory environments.
OpenMP allows for parallel programming by identifying parallel regions as blocks of code that may run in parallel.
Application developers insert compiler directives into their code at parallel regions they know can be executed in parallel, and these directives instruct the OpenMP runtime library to execute the region in parallel.

It can create as many threads as are available in a system with the \kernelinline{#pragma omp parallel} directive.
There are also directives for parallelizing loops, automatically dividing the work among the spawned threads.

This system also allows developers to choose the way the OpenMP library behaves, allowing for several different levels of parallelism.
These include:
\begin{itemize}[noitemsep]
\item Setting the number of threads manually.
\item Allowing developers to identify whether data is shared between threads or are private to a thread.
\end{itemize}

\subsubsection{Grand Central Dispatch}\label{subsubsec:Grand_Central_Dispatch}
\emph{Grand Central Dispatch} (\emph{GCD}) is a \nameref{def:Thread_Pool} system developed by Apple for OSX and iOS.\@
If is a combination of C extensions, an \nameref{def:API}, and a runtime library.
This allows GCD to use POSIX threads and manage the creation/destruction of threads as needed.
This allows developers to identify sections of code that may be run in parallel.

Like in \nameref{subsubsec:OpenMP}, blocks of code that may be parallelized must be identified by the programmer with the \kernelinline{^{ code; }} syntax.
When executing, GCD will schedule blocks for execution by placing them on a dispatch queue.
When an item from the dispatch queue is removed, it is assigned to a \nameref{def:Thread} from the queue's \nameref{def:Thread_Pool}.

There are 2 types of dispatch queues:
\begin{enumerate}[noitemsep]
\item Serial (\textbf{Cannot} be executed in parallel).
  \begin{itemize}[noitemsep]
  \item Items are removed in a FIFO order.
  \item One block \textbf{MUST} finish executing before another can be drawn from the dispatch queue.
  \item Each process gets its own serial queue, the \emph{main queue}.
  \item Additional serial dipatch queues can be made for local for execution.
  \end{itemize}
\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
