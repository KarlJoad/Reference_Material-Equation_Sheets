\subsection{Contiguous Memory Allocation}\label{subsec:Contiguous_Memory_Allocation}
Main memory must contain everything for the system to run, including both the \nameref{def:Operating_System} and \nameref{def:User} \nameref{def:Process}es.
It is our jobs, as operating system engineers to make the allocation of memory for the \nameref{def:Operating_System} as efficient as possible.
The earliest and simplest method is that of \nameref{def:Contiguous_Memory_Allocation}.

Memory is usually divided into two partitions: one for the \nameref{def:Operating_System} and one for the user processes.
We can place the operating system in either low memory or high memory, depending on the location of the \nameref{def:Interrupt_Vector}.
Typically, the interrupt vector is in low addresses, so the OS is usually put there too.
Throughout this document, we assume that the OS inhabits the lowest addresses.

\begin{definition}[Contiguous Memory Allocation]\label{def:Contiguous_Memory_Allocation}
  In \emph{contiguous memory allocation}, each \nameref{def:Process} is contained in a single contiguous section of memory that is also contiguous with the next process.

  In short, this means that each \nameref{def:Process} sits in its own contiguous block and is right next to the next process.
\end{definition}

\subsubsection{Memory Protection}\label{subsubsec:Contiguous_Memory_Protection}
In \nameref{def:Contiguous_Memory_Allocation}, we can provide memory protection by using concepts from earlier.
If the \texttt{base} register is considered as the \texttt{relocation} register (because of \nameref{def:Logical_Address} conversion to \nameref{def:Physical_Address}), and use the \texttt{limit} register as before, then we can protect this \nameref{def:Process}.
Since the \nameref{def:Operating_System} is the only entity that can change the values of the \texttt{relocation} and \texttt{limit} registers, this (currently executing) process and others cannot interfere with each other.

When the \nameref{rmk:CPU_Scheduler} selects a \nameref{def:Process} for execution, the \nameref{def:Dispatcher} loads the \texttt{relocation} and \texttt{limit} registers with the correct values as part of the \nameref{def:Context_Switch}.
Because every address generated by a CPU is checked against these registers, we can protect the operating system and other usersâ€™ programs and data from being modified by this running process.
This scheme is an effective way to allow the \nameref{def:Operating_System} to change size dynamically, which is highly desireable.

\subsubsection{Memory Allocation}\label{subsubsec:Contiguous_Memory_Allocation}
Within the \nameref{def:User} memory, there are 2 main methods of \nameref{def:Process}-memory allocation:
\begin{enumerate}[noitemsep]
\item \nameref{par:Multiple_Partition_Scheme}
\item \nameref{par:Variable_Partition_Scheme}
\end{enumerate}

Both methods use the same idea of dividing all free memory into separate partitions, however the size of these partitions and how they are divided is the differentiating factor.

\paragraph{Multiple-Partition Scheme}\label{par:Multiple_Partition_Scheme}
In the \emph{multiple-partition scheme}, all free memory is statically divided into equal sized partitions.
Thus, the size of a partition is fixed throughout the execution of the \nameref{def:Operating_System}.
When a partition is free, a \nameref{def:Process} is selected from the input queue and placed into the free partition.
When the process terminates, the partition is returned to the pool of available partitions.

\paragraph{Variable-Partition Scheme}\label{par:Variable_Partition_Scheme}
In the \emph{variable-partition scheme}, all free memory is pooled together.
The free memory is called a \emph{hole}.

\begin{blackbox}
  In many ways, this mirrors the use of the heap in programs.
  Many of the principles from heap and the allocation of stuff to the heap also apply in this case as well.
\end{blackbox}

As \nameref{def:Process}es are selected from the input queue to run, their memory requirements are considered.
The system searches the set for a hole that is large enough for this process.
If the hole is too large, it is split into two parts.
The process is then given an appropriate amount of space from memory, taken from the pool of free memory; the other is returned to the set of holes.
When a process terminates, it releases its block of memory, which is then placed back in the set of holes.
If the new hole is adjacent to other holes, these adjacent holes are merged to form one larger hole.

At this point, the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes.
The next process to run has its memory requirements considered, \textbf{AND} the free memory available.
What happens next depending on the \nameref{def:Operating_System}:
\begin{itemize}[noitemsep]
\item Wait until a large enough block of memory is available for this process.
\item Skip through the input queue to find a process that can use the memory (because of smaller requirements).
\end{itemize}

Just like in heap allocation and garbage collection, this is a particular instance of the general dynamic storage-allocation problem.
In \nameref{def:Operating_System}s, there are 3 common strategies used to allocate this memory to \nameref{def:Process}es:
\begin{enumerate}[noitemsep]
\item \nameref{subpar:First_Fit}
\item \nameref{subpar:Best_Fit}
\item \nameref{subpar:Worst_Fit}
\end{enumerate}

\subparagraph{First-Fit}\label{subpar:First_Fit}
Allocate the first hole that is big enough.
Searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended.
Stop searching as soon as we find a free hole that is large enough.

\subparagraph{Best-Fit}\label{subpar:Best_Fit}
Allocate the smallest hole that is big enough, the best fitting hole.
We must search the entire list, unless the list is ordered by size.
This strategy produces the smallest leftover hole.

\subparagraph{Worst-Fit}\label{subpar:Worst_Fit}
Allocate the largest hole, the worst fitting hole.
Again, we must search the entire list, unless it is sorted by size.
This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a best-fit approach.

\subsubsection{Fragmentation}\label{subsubsec:Contiguous_Memory_Fragmentation}
When allocating space to \nameref{def:Process}es, we may run into the issue of \nameref{def:Fragmentation}.

\begin{definition}[Fragmentation]\label{def:Fragmentation}
  \emph{Fragmentation} is when memory that was allocated to a \nameref{def:Process} is \textbf{not} used.
  Technically, this is an inefficiency, not a problem.

  There are 2 kinds of fragmentation possible:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:External_Fragmentation}
  \item \nameref{def:Internal_Fragmentation}
  \end{enumerate}
\end{definition}

\paragraph{External Fragmentation}\label{par:External_Fragmentation}
Statistical analysis of \nameref{subpar:First_Fit} reveals that, given $N$ allocated blocks, another $0.5 N$ blocks will be lost to \nameref{def:External_Fragmentation}.
Meaning, one-third of memory may be unusable.
This property is known as the \emph{50-percent rule}.

\begin{definition}[External Fragmentation]\label{def:External_Fragmentation}
  \emph{External fragmentation} is when there is enough total unused memory to satisfy a request, but the unused memory is \textbf{not} contiguous.
  If the memory is fragmented into a large number of small holes, then contiguous memory requests cannot be satisfied.
  As processes are loaded and removed from memory, the free memory space is broken into little pieces.

  This is not necessarily a problem by itself, as much as it is an inefficiency.
  However, this inefficiency will lead to problems if it is not handled.
\end{definition}

One solution to the problem of \nameref{def:External_Fragmentation} is \emph{compaction} (like garbage collection for the heap).
The goal is to shuffle the memory contents so as to place all free memory together in one large block, however it cannot always be used.
When compaction is possible, we must determine its cost, because it can be quite expensive.
\begin{itemize}[noitemsep]
\item If memory locations are static and done at assembly or load time, compaction cannot be done.
\item If memory locations are dynamic and done at execution time, compaction can be done by relocation.
  \begin{itemize}[noitemsep]
  \item Relocation moves the program and data, then changes the \texttt{base}/\texttt{relocation} register to reflect the new base address.
  \end{itemize}
\end{itemize}
The simplest compaction algorithm is to move all processes toward one end of memory; all holes move in the other direction, producing one large hole of available memory.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
