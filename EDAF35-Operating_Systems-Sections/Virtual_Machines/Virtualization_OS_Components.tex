\subsection{Virtualization and OS Components}\label{subsec:Virtualization_OS_Components}
Here, I discuss how the \nameref{def:VMM} provides core \nameref{def:Operating_System} functions like \nameref{subsec:Scheduling}, I/O, and memory management.

\subsubsection{CPU Scheduling}\label{subsubsec:VM_CPU_Scheduling}
A system with \nameref{def:Virtualization}, frequently acts like a multiprocessor system.
The virtualization software presents one or more \nameref{def:Virtual_CPU}s to each of the \nameref{def:Virtual_Machine}s running on the system and then schedules the use of the physical CPUs among the virtual machines.
The significant variations among virtualization technologies make it difficult to generalize the effect of virtualization on scheduling.

First, consider the general case of \nameref{def:VMM} scheduling.
The VMM has a number of physical CPUs available and a number of threads to run on those CPUs.
The threads can be VMM threads or guest threads.
The VMM itself needs some CPU cycles for guest management and I/O management and can steal cycles from the guests by scheduling its threads across all of the system CPUs, but the impact of this action is relatively minor.
Guests are configured with a certain number of \nameref{def:Virtual_CPU}s at creation time, and can be adjusted throughout the life of the VM.\@

When there are enough CPUs to allocate the requested number to each guest, the \nameref{def:VMM} can treat the CPUs as dedicated and schedule only a given guest's threads on that guest's CPUs.
In this situation, the guests act much like native \nameref{def:Operating_System}s running on native CPUs.
In other situations, there may not be enough CPUs to go around, especially if the CPUs have \nameref{def:Overcommit}ment, where the guests are configured for more CPUs than exist in the system.
Then, a VMM can use standard \nameref{subsubsec:Scheduling_Algorithms} to make progress on each thread but can also add a fairness aspect to those algorithms.

\begin{definition}[Overcommit]\label{def:Overcommit}
  \nameref{def:Virtualization} systems allow the created virtual resources to \emph{overcommit} the system's physical resources.
  For example, a \nameref{def:Virtual_Machine} can be given 2 \nameref{def:Virtual_CPU}s, when there is only 1 physical CPU.\@
  The same can happen with memory; a guest can be configured have more memory than the host system has.
\end{definition}

Even given a \nameref{def:Scheduler} that provides fairness, any guest \nameref{def:Operating_System} scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively affected by virtualization.
Within a \nameref{def:Virtual_Machine}, the guest operating system is at the mercy of the \nameref{def:Virtualization} system as to what CPU resources it actually receives.

The net effect of such scheduling layering is that individual virtualized \nameref{def:Operating_System}s receive only a portion of the available CPU cycles, even though they believe they are receiving all of the cycles and that they are scheduling for all of those cycles.

\subsubsection{Memory Management}\label{subsubsec:VM_Memory_Management}
Efficient memory use in general-purpose operating systems is one of the major keys to performance.
In virtualized environments, there are more users of memory (the guests and their applications, as well as the VMM), leading to more pressure on memory use.
Further adding to this pressure is that VMMs typically \nameref{def:Overcommit} memory.
The extra need for efficient memory mean great measures are taken to ensure the optimal use of memory.

Before memory optimization can occur, the VMM must establish how much real memory each guest should use.
To do that:
\begin{enumerate}[noitemsep]
\item \nameref{def:VMM} evaluates the maximum memory size of each guest as dictated when it is configured.
  \begin{itemize}[noitemsep]
  \item General-purpose operating systems do not expect the amount of memory in the system to change, so VMMs must maintain the illusion that the guest has that amount of memory.
  \end{itemize}
\item \nameref{def:VMM} computes a target real memory allocation for each guest based on the configured memory for that guest and other factors, such as \nameref{def:Overcommit}ment and system load.
\item Then three low-level mechanisms are used to reclaim memory from the guests.
  \begin{enumerate}[noitemsep]
  \item \nameref{par:VMM_Page_Replacement}
  \item \nameref{par:VM_Mem_Kernel_Module}
  \item \nameref{par:Hash_VMem_Remap}
  \end{enumerate}
\end{enumerate}

The overall effect is to enable guests to behave and perform as if they had the full amount of memory requested although in reality they have less.

\paragraph{VMM Page Replacement}\label{par:VMM_Page_Replacement}
A guest believes it controls memory allocation via its \nameref{def:Page_Table} management, whereas in reality the \nameref{def:VMM} maintains a nested page table that re-translates the guest page table to the real page table.
The VMM can use this extra level of indirection to optimize the guest’s use of memory without the guest’s knowledge or help.
One approach is to provide double paging, in which the VMM has its own page-replacement algorithms.
The VMM can swap pages to the \nameref{def:Backing_Store} pages that the guest believes are in-memory.
The VMM knows less about the guest's memory access patterns than the guest does, so its paging is less efficient, creating performance problems.

VMMs use this method when other methods are not available or are not providing enough free memory.
However, it is not the preferred approach.

\paragraph{Guest Pseudo-Device Driver}\label{par:VM_Mem_Kernel_Module}
A common solution is for the VMM to install in each guest a \nameref{def:Pseudo_Device_Driver} or \nameref{def:Kernel_Module} that it controls.

\begin{definition}[Pseudo-Device Driver]\label{def:Pseudo_Device_Driver}
  A \emph{pseudo–device driver} uses device-driver interfaces, appearing to the \nameref{def:Kernel} to be a device driver, but does not actually control a device.
  Rather, it is an easy way to add kernel-mode code without directly modifying the kernel.
\end{definition}

This balloon memory manager communicates with the VMM and is told to allocate or deallocate memory.
If told to allocate, it allocates memory and tells the operating system to pin the allocated pages into physical memory.
The guest sees memory pressure becauses of these pinned pages, decreasing the amount of physical memory it has available to use.
The guest then may free up other physical memory to be sure it has a sufficient pool of free memory.
Meanwhile, the VMM, knowing that the pages pinned by the balloon process will never be used, removes those physical pages from the guest and allocates them to another guest.
At the same time, the guest is using its own memory-management and paging algorithms to manage the available memory, which is the most efficient option.

If memory pressure within the entire system decreases, the VMM will tell the balloon process within the guest to unpin and free some or all of the memory, allowing the guest more pages for its use.

\paragraph{Page Hashing and Remapping}\label{par:Hash_VMem_Remap}
Another common method for reducing memory pressure is for the \nameref{def:VMM} to determine if the same page has been loaded more than once.
If this is the case, to the VMM reduces the number of copies of the page to one and maps the other users of the page to that one copy.
This can be done by randomly sampling the guests' memory and creating a hash for each page sampled.
The hash of every page examined is compared with other hashes already stored in a hash table.
If there is a match, the pages are compared byte by byte to see if they really are identical.
If they are, one page is freed, and its logical address is mapped to the other’s physical address.

This technique might seem at first to be ineffective, but consider that guests run \nameref{def:Operating_System}s.
If multiple guests run the same operating system, then only one copy of the active operating-system pages need be in memory.

\subsubsection{I/O}\label{subsubsec:VM_I/O}
In the area of I/O, \nameref{def:Hypervisor}s have some leeway and can be less concerned with exactly representing the underlying hardware to their guests.
\nameref{def:Operating_System}s are used to dealing with varying and flexible I/O mechanisms.
Usually, device drivers can be dynamically loaded and unloaded.
\nameref{def:Virtualization} takes advantage of such built-in flexibility by providing specific virtualized devices to guest operating systems.

\nameref{def:VMM}s vary greatly in how they provide I/O to their guests.
I/O in virtual environments is complicated and requires careful VMM design and implementation.
In addition to direct access, VMMs can provide shared access to devices.
The VMM must provide protection while sharing the device, assuring that a guest can access only the blocks specified in the guest’s configuration.
In such instances, the VMM must be part of every I/O, checking it for correctness as well as routing the data to and from the appropriate devices and guests.

In the area of networking, \nameref{def:VMM}s also have work to do.
General-purpose \nameref{def:Operating_System}s typically have one IP address.
Therefore, a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest.
The guests can be ``directly'' connected to the network by an IP address that is seen by the broader network (this is known as bridging).
Alternatively, the VMM can provide a network address translation (NAT) address.

\subsubsection{Storage Management}\label{subsubsec:VM_Storage_Management}
\nameref{def:Type0_Hypervisor}s do tend to allow root disk partitioning, partly because these systems tend to run fewer guests than other systems.
Alternatively, they may have a disk manager as part of the control partition, and that disk manager provides disk space (including boot disks) to the other partitions.

\nameref{def:Type1_Hypervisor}s store the guest root disk (and configuration information) in one or more files within the file systems provided by the \nameref{def:VMM}.
\nameref{def:Type2_Hypervisor}s store the same information within the host operating system's file systems.
Both of these create a disk image, containing all of the contents of the root disk of the guest, containing one file in the VMM.\@
Aside from the potential performance problems that causes, it is a clever solution, because it simplifies copying and moving guests.

Because of the disk images created physical-to-virtual (P-to-V) conversion can be done.
Physical-to-Virtual conversion reads the disk blocks of the physical system's disks and stores them within files that the \nameref{def:VMM} can access.
It is also possible to go the other way, virtual-to-physical (V-to-P) conversion is a procedure for converting a virtualized guest to a physical system.

\subsubsection{Live Migration}\label{subsubsec:VM_Live_Migration}
One feature found \nameref{def:Type0_Hypervisor}s and \nameref{def:Type1_Hypervisor}s is the live migration of a running guest from one system to another.
Live migration is made possible because of the well-defined interfaces between guests and VMMs and the limited state the VMM maintains for the guest.
The VMM migrates a guest via the following steps:
\begin{enumerate}[noitemsep]
\item The source \nameref{def:VMM} establishes a connection with the target VMM and confirms that it is allowed to send a guest.
\item The target creates a new guest by creating a new \nameref{def:VCPU}, new nested page table, and other state storage.
\item The source sends all read-only memory pages to the target.
\item The source sends all read-write pages to the target, marking them as clean on the source.
\item The source repeats step 4, as during transmission, some pages were probably modified by the guest and are now dirty.
  These pages need to be sent again and marked again as clean.
\item When the cycle of steps 4 and 5 becomes very short, the source VMM freezes the guest, sends the VCPU’s final state, sends other state details, sends the final dirty pages, and tells the target to start running the guest.
  Once the target acknowledges that the guest is running, the source terminates the guest.
\end{enumerate}

Lastly, here are a few interesting details and limitations concerning live migration.
First, for network connections to continue uninterrupted, the network infrastructure needs to understand that a MAC address can move between systems.
Before \nameref{def:Virtualization}, this did not happen, as the MAC address was tied to physical hardware.
With virtualization, the MAC must be movable for existing networking connections to continue without resetting.
Modern network switches understand this and route traffic wherever the MAC address is, even accommodating a move.

A limitation of live migration is that no disk information is transferred.
One reason live migration is possible is that most of the guest’s state is maintained within the guest
\begin{itemize}[noitemsep]
\item Open file tables
\item \nameref{def:System_Call} state
\item \nameref{def:Kernel} state
\item etc.
\end{itemize}

Because disk I/O is so much slower than memory access, and used disk space is usually much larger than used memory, disks associated with the guest cannot be reasonably be moved as part of a live migration.
Rather, the disk must be remote to the guest, accessed over the network.
In that case, disk access state is maintained within the guest, and network connections are all that matter to the \nameref{def:VMM}.
The network connections are maintained during the migration, so remote disk access continues.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
