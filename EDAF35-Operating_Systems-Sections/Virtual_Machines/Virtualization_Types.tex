\subsection{Types of Virtualization}\label{subsec:Virtualization_Types}
Here, I discuss the ways that virtual environments are created using \nameref{def:Virtualization} and the \nameref{subsec:Building_Blocks} discussed earlier.
Of course, the hardware on which the virtual machines are running can cause great variation in implementation methods.
Throughout this discussion, it is safe to assume that \nameref{def:VMM}s take advantage of hardware assistance when possible.

\subsubsection{Virtual Machine Life Cycle}\label{subsubsec:VM_Life_Cycle}
Whatever the hypervisor type, at the time a virtual machine is created, its creator gives the \nameref{def:VMM} certain parameters.
These parameters usually include:
\begin{itemize}[noitemsep]
\item Number of CPUs.
\item Amount of memory.
\item Networking details.
\item Storage details.
\end{itemize}

For example, a user might want to create a new guest with:
\begin{itemize}[noitemsep]
\item Two \nameref{def:Virtual_CPU}s
\item \SI{4}{\gibi{} \byte{}} of memory
\item \SI{10}{\gibi{} \byte{}} of disk space
\item One network interface that gets its IP address via DHCP
\item Access to the DVD drive
\end{itemize}

In the case of a \nameref{def:Type0_Hypervisor}, the resources are usually dedicated in hardware.
In this situation, if there are not two virtual CPUs available and unallocated, the creation request will fail.
For other hypervisor types, the resources are dedicated or virtualized, depending on the type.
Obviously, an IP address cannot be shared, but the \nameref{def:Virtual_CPU}s are usually multiplexed on the physical CPUs.
Similarly, memory management usually involves allocating more memory to guests than actually exists in physical memory.
This is more complicated than multiplexing memory like we do the CPU.\@

When the \nameref{def:Virtual_Machine} is no longer needed, it can be deleted.
When this happens, the \nameref{def:VMM} first frees up any used disk space and then removes the configuration associated with the virtual machine, essentially ``forgetting'' the virtual machine.
These steps are quite simple compared with building, configuring, running, and removing physical machines.

\subsubsection{Type 0 Hypervisor}\label{subsubsec:Type0_Hypervisor}
\nameref{def:Type0_Hypervisor}s have existed for many years under many names, including ``partitions'' and ``domains''.
They are a hardware feature, and that brings its own positives and negatives.
The feature set of a \nameref{def:Type0_Hypervisor} tends to be smaller than those of the other types because it is implemented in \nameref{def:Hardware}.

\begin{definition}[Type 0 Hypervisor]\label{def:Type0_Hypervisor}
  A \emph{Type 0 Hypervisor} provides the \nameref{def:Virtualization} required to run \nameref{def:Virtual_Machine}s in \nameref{def:Firmware}.
  This means that there is \textbf{NO} abstraction required and no abstraction overhead incurred in this method.
  However, this type of virtualization is also relatively inflexible, as the number of virtual domains that can be created for execution is limited by firmware.
\end{definition}

\nameref{def:Operating_System}s need do nothing special to take advantage of their features.
The \nameref{def:VMM} itself is encoded in the firmware and loaded at boot time.
In turn, it loads the guest images to run in each partition.
Each guest believes that it has dedicated hardware, because the hardware assigned to that \nameref{def:Virtual_Machine} \textbf{is} dedicated, simplifying many implementation details.

I/O presents some difficulty, because it is not easy to dedicate I/O devices to guests if there are not enough.
Either all guests must get their own I/O devices, or the system must provided I/O device sharing.
In these cases, the \nameref{def:Hypervisor} manages shared access or grants all devices to a control partition.
In the \textbf{control partition}, a guest operating system provides services (such as networking) via \nameref{def:Daemon}s to other guests, and the hypervisor routes I/O requests appropriately.

Because type 0 virtualization is very close to raw hardware execution, it should be considered separately from the other \nameref{def:Virtualization} methods discussed here.
A \nameref{def:Type0_Hypervisor} can run multiple guest operating systems (one in each hardware partition).
All of those guests, because they are running on a subset of the raw hardware, can in turn be \nameref{def:VMM}s.
Because of that, each can have its own guest operating systems.
Other types of hypervisors usually cannot provide this virtualization-within-virtualization functionality.

\subsubsection{Type 1 Hypervisor}\label{subsubsec:Type1_Hypervisor}
\nameref{def:Type1_Hypervisor}s are special-purpose \nameref{def:Operating_System}s that run natively on the hardware.
But rather than providing \nameref{def:System_Call}s and other interfaces for running \nameref{def:Program}s, they create, run, and manage guest operating systems.

\begin{definition}[Type 1 Hypervisor]\label{def:Type1_Hypervisor}
  A \emph{Type 1 Hypervisor} provides the \nameref{def:Virtualization} required to run a \nameref{def:Virtual_Machine} in \nameref{def:Software}.
  Here, the physical resources of the system are managed by the software, either a standalone \nameref{def:Hypervisor}, or a traditional \nameref{def:Kernel} with extensions for virtualization.
  This is the most commonly used type of hypervisor, because of its flexibility.
  It can run almost any number of guests, and is limited only by its ability to schedule and handle the running guests.
  In this case, there are overheads incurred from using this system, as there is some abstraction of hardware resources to form a virtual interface.

  \begin{remark}[Nested Virtualization]
    \nameref{def:Type1_Hypervisor}s \textbf{cannot} be virtually nested inside another \nameref{def:Type1_Hypervisor}.
  \end{remark}
\end{definition}

\nameref{def:Type1_Hypervisor}s run in \nameref{def:Kernel}-mode, taking advantage of hardware protection.
Where the host CPU allows, they use multiple modes to give guest \nameref{def:Operating_System}s their own control and improved performance.
They implement device drivers for the hardware they run on.
Because they are operating systems, they must also provide CPU \nameref{subsec:Scheduling}, memory management, I/O management, \nameref{sec:Protection}, and even \nameref{sec:Security}.

An important benefit is the ability to perform \nameref{def:Consolidation}.

\paragraph{Traditional OSs as VMMs}\label{par:Traditional_OS_VMM}
Some general-purpose \nameref{def:Operating_System}s also provide \nameref{def:VMM} functionality.
For example, RedHat Enterprise Linux, Windows, or Oracle Solaris can perform its normal duties as well as providing a VMM allowing other operating systems to run as guests.
Because of their extra duties, these hypervisors typically provide fewer virtualization features than other \nameref{def:Type1_Hypervisor}s.
In many ways, they treat a guest operating system as just another process, albeit with special handling provided when the guest tries to execute special instructions.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
