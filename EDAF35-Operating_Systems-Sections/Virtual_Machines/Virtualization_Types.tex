\subsection{Types of Virtualization}\label{subsec:Virtualization_Types}
Here, I discuss the ways that virtual environments are created using \nameref{def:Virtualization} and the \nameref{subsec:Building_Blocks} discussed earlier.
Of course, the hardware on which the virtual machines are running can cause great variation in implementation methods.
Throughout this discussion, it is safe to assume that \nameref{def:VMM}s take advantage of hardware assistance when possible.

\subsubsection{Virtual Machine Life Cycle}\label{subsubsec:VM_Life_Cycle}
Whatever the hypervisor type, at the time a virtual machine is created, its creator gives the \nameref{def:VMM} certain parameters.
These parameters usually include:
\begin{itemize}[noitemsep]
\item Number of CPUs.
\item Amount of memory.
\item Networking details.
\item Storage details.
\end{itemize}

For example, a user might want to create a new guest with:
\begin{itemize}[noitemsep]
\item Two \nameref{def:Virtual_CPU}s
\item \SI{4}{\gibi{} \byte{}} of memory
\item \SI{10}{\gibi{} \byte{}} of disk space
\item One network interface that gets its IP address via DHCP
\item Access to the DVD drive
\end{itemize}

In the case of a \nameref{def:Type0_Hypervisor}, the resources are usually dedicated in hardware.
In this situation, if there are not two virtual CPUs available and unallocated, the creation request will fail.
For other hypervisor types, the resources are dedicated or virtualized, depending on the type.
Obviously, an IP address cannot be shared, but the \nameref{def:Virtual_CPU}s are usually multiplexed on the physical CPUs.
Similarly, memory management usually involves allocating more memory to guests than actually exists in physical memory.
This is more complicated than multiplexing memory like we do the CPU.\@

When the \nameref{def:Virtual_Machine} is no longer needed, it can be deleted.
When this happens, the \nameref{def:VMM} first frees up any used disk space and then removes the configuration associated with the virtual machine, essentially ``forgetting'' the virtual machine.
These steps are quite simple compared with building, configuring, running, and removing physical machines.

\subsubsection{Type 0 Hypervisor}\label{subsubsec:Type0_Hypervisor}
\nameref{def:Type0_Hypervisor}s have existed for many years under many names, including ``partitions'' and ``domains''.
They are a hardware feature, and that brings its own positives and negatives.
The feature set of a \nameref{def:Type0_Hypervisor} tends to be smaller than those of the other types because it is implemented in \nameref{def:Hardware}.

\begin{definition}[Type 0 Hypervisor]\label{def:Type0_Hypervisor}
  A \emph{Type 0 Hypervisor} provides the \nameref{def:Virtualization} required to run \nameref{def:Virtual_Machine}s in \nameref{def:Firmware}.
  This means that there is \textbf{NO} abstraction required and no abstraction overhead incurred in this method.
  However, this type of virtualization is also relatively inflexible, as the number of virtual domains that can be created for execution is limited by firmware.
\end{definition}

\nameref{def:Operating_System}s need do nothing special to take advantage of their features.
The \nameref{def:VMM} itself is encoded in the firmware and loaded at boot time.
In turn, it loads the guest images to run in each partition.
Each guest believes that it has dedicated hardware, because the hardware assigned to that \nameref{def:Virtual_Machine} \textbf{is} dedicated, simplifying many implementation details.

I/O presents some difficulty, because it is not easy to dedicate I/O devices to guests if there are not enough.
Either all guests must get their own I/O devices, or the system must provided I/O device sharing.
In these cases, the \nameref{def:Hypervisor} manages shared access or grants all devices to a control partition.
In the \textbf{control partition}, a guest operating system provides services (such as networking) via \nameref{def:Daemon}s to other guests, and the hypervisor routes I/O requests appropriately.

Because type 0 virtualization is very close to raw hardware execution, it should be considered separately from the other \nameref{def:Virtualization} methods discussed here.
A \nameref{def:Type0_Hypervisor} can run multiple guest operating systems (one in each hardware partition).
All of those guests, because they are running on a subset of the raw hardware, can in turn be \nameref{def:VMM}s.
Because of that, each can have its own guest operating systems.
Other types of hypervisors usually cannot provide this virtualization-within-virtualization functionality.

\subsubsection{Type 1 Hypervisor}\label{subsubsec:Type1_Hypervisor}
\nameref{def:Type1_Hypervisor}s are special-purpose \nameref{def:Operating_System}s that run natively on the hardware.
But rather than providing \nameref{def:System_Call}s and other interfaces for running \nameref{def:Program}s, they create, run, and manage guest operating systems.

\begin{definition}[Type 1 Hypervisor]\label{def:Type1_Hypervisor}
  A \emph{Type 1 Hypervisor} provides the \nameref{def:Virtualization} required to run a \nameref{def:Virtual_Machine} in \nameref{def:Software}.
  Here, the physical resources of the system are managed by the software, either a standalone \nameref{def:Hypervisor}, or a traditional \nameref{def:Kernel} with extensions for virtualization.
  This is the most commonly used type of hypervisor, because of its flexibility.
  It can run almost any number of guests, and is limited only by its ability to schedule and handle the running guests.
  In this case, there are overheads incurred from using this system, as there is some abstraction of hardware resources to form a virtual interface.

  \begin{remark}[Nested Virtualization]
    \nameref{def:Type1_Hypervisor}s \textbf{cannot} be virtually nested inside another \nameref{def:Type1_Hypervisor}.
  \end{remark}
\end{definition}

\nameref{def:Type1_Hypervisor}s run in \nameref{def:Kernel}-mode, taking advantage of hardware protection.
Where the host CPU allows, they use multiple modes to give guest \nameref{def:Operating_System}s their own control and improved performance.
They implement device drivers for the hardware they run on.
Because they are operating systems, they must also provide CPU \nameref{subsec:Scheduling}, memory management, I/O management, \nameref{sec:Protection}, and even \nameref{sec:Security}.

An important benefit is the ability to perform \nameref{def:Consolidation}.

\paragraph{Traditional OSs as VMMs}\label{par:Traditional_OS_VMM}
Some general-purpose \nameref{def:Operating_System}s also provide \nameref{def:VMM} functionality.
For example, RedHat Enterprise Linux, Windows, or Oracle Solaris can perform its normal duties as well as providing a VMM allowing other operating systems to run as guests.
Because of their extra duties, these hypervisors typically provide fewer virtualization features than other \nameref{def:Type1_Hypervisor}s.
In many ways, they treat a guest operating system as just another process, albeit with special handling provided when the guest tries to execute special instructions.

\subsubsection{Type 2 Hypervisor}\label{subsubsec:Type2_Hypervisor}
\nameref{def:Type2_Hypervisor}s are less interesting to \nameref{def:Operating_System}s, because there is very little operating-system involvement in these application-level \nameref{def:Virtual_Machine_Manager}s.
This type of VMM is simply another process run and managed by the host, and even the \nameref{def:VM_Host} does not know \nameref{def:Virtualization} is happening within the VMM.\@

\begin{definition}[Type 2 Hypervisor]\label{def:Type2_Hypervisor}
  A \emph{Type 2 Hypervisor} provides \nameref{def:Virtualization} through \nameref{def:Software} as well, however this hypervisor is running as a \nameref{def:User}-level \nameref{def:Process} on top of another \nameref{def:Operating_System}.
  This leads to heavy levels of abstraction, thus slowing down performance significantly.
  However, this also allows relatively unprivileged users to create and manage \nameref{def:Virtual_Machine}s on their system.
\end{definition}

\nameref{def:Type2_Hypervisor}s have limits not associated with some of the other types.
For example, a user needs administrative privileges to access many of the hardware assistance features of modern CPUs.
If the VMM is being run by a standard user without additional privileges, the VMM cannot take advantage of these features.
This limitation, as well as the extra overhead of running the \nameref{def:Operating_System} as well as guest operating systems, type 2 hypervisors tend to have poorer overall performance than type 0 or 1.

However, this is also a benefit, because \nameref{def:Type2_Hypervisor}s run on a variety of general-purpose \nameref{def:Operating_System}s, and running them requires no changes to the \nameref{def:VM_Host} operating system.

\subsubsection{Paravirtualization}\label{subsubsec:Paravirtualization}
Paravirtualization presents the guest with a system that is similar but not identical to the guest's preferred system.
This system does not attempt to ``trick'' the guest \nameref{def:Operating_System} that it has its own system.
Instead, the guest and the host work together for a better \nameref{def:Virtualization} experience.

The guest must be modified to run on the paravirtualized virtual hardware.
The gain for this extra work is more efficient use of resources and a smaller virtualization layer.

Other \nameref{def:VMM}s present virtual devices to guests that \textbf{appear} to be real devices.
Instead of taking that approach, the paravirtualization presents clean and simple device abstractions that allow efficient I/O, as well as good communication between the guest and the VMM about device I/O.
For each device used by each guest, there is a circular buffer shared by the guest and the VMM via \nameref{par:Shared_Memory}.
Read and write data are placed in this buffer.

For memory management, paravirtualization does not implement nested page tables.
Rather, each guest has its own set of page tables, set to read-only.
The \nameref{def:VMM} requires the guest to use a specific mechanism, a \nameref{def:Hypercall} from the guest to the hypervisor VMM, when a page-table change is needed.

\begin{definition}[Hypercall]\label{def:Hypercall}
  A \emph{hypercall} is like a traditional \nameref{def:System_Call}, but instead of going to the \nameref{def:Operating_System}, it the request is passed to the \nameref{def:VM_Host}'s \nameref{def:Virtual_Machine_Manager}.
\end{definition}

This means that the guest \nameref{def:Operating_System}'s \nameref{def:Kernel} code must be changed from the default code to these paravirtualization-specific methods.
To optimize performance, paravirtualization allows the guest to queue up multiple page-table changes asynchronously via \nameref{def:Hypercall}s, then check to ensure that the changes are complete before continuing operation.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../EDAF35-Operating_Systems-Reference_Sheet"
%%% End:
