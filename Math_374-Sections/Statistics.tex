\section{Statistics} \label{sec:Statistics}
In applying probability models to real situations, we perform experiments and collect data to answer questions such as:
	\begin{enumerate}[noitemsep, nolistsep]
		\item What are the values of the parameters of the distribution of a random variable of interest?
			\begin{itemize}[noitemsep, nolistsep]
				\item Mean or Expected value
				\item Variance
			\end{itemize}
		
		\item Is the data set consistent with some model?
			\begin{itemize}[noitemsep, nolistsep]
				\item Some assumed distribution, which must be true, otherwise the model is wrong.
			\end{itemize}
		
		\item Is the data set consistent with some parameter value of the assumed value?
	\end{enumerate}

	\begin{definition}[Random Sample] \label{def:Random Sample}
		A \emph{random sample} is a set of $n$ \nameref{def:Random Variable, Full} or \nameref{def:Statistic} that are drawn with \nameref{def:Independent and Identically Distributed}.
		\begin{equation} \label{eq:Random Sample}
			\mathbf{X}_{n} = \left( X_{1},X_{2},\ldots,X_{n} \right)
		\end{equation}
		\begin{remark}
			This is \emph{similar} to the definition of a \nameref{def:Random Vector}.
			The difference here is that the values in a \nameref{def:Random Sample} must be \nameref{def:Independent and Identically Distributed} and \emph{may} be related to each other somehow.
		\end{remark}
		\begin{remark}[Random Sample Parameters] \label{rmk:Random Sample Parameters}
			These are an additional variable that is added onto the \nameref{def:Probability Density Function} or \nameref{def:Probability Mass Function}.
			When we were using these functions in the previous sections, these parameters were either constant or assumed to be constant.
			When considering these samples in \nameref{sec:Statistics}, you must also account for the \nameref{rmk:Random Sample Parameters}.
		\end{remark}
	\end{definition}
	\begin{definition}[Statistic] \label{def:Statistic}
		A \emph{statistic} $W (\mathbf{X}_{n})$ is a function of the random sample $X_{1},X_{2},\ldots,X_{n}$.
		\begin{equation} \label{eq:Statistic}
			W \left( \mathbf{X}_{n} \right) = g \left( X_{1},X_{2},\ldots,X_{n} \right)
		\end{equation}
	\end{definition}
	\begin{definition}[Unit Variance] \label{def:Unit Variance}
		The \emph{unit variance} means that the standard deviation, $\sigma$ of a sample, as well as the variance, $\sigma^{2}$ will tend towards 1 as the sample size increases to infinity.
	\end{definition}

	\subsection{Sums of Random Variables} \label{subsec:Sums of Random Variables}
		\begin{definition}[Sum of Random Variables] \label{def:Sum of Random Variables}
			The definition of a \emph{sum of random variables} is given in Equation~\eqref{eq:Sum of Random Variables} below.
			Where $X_{i}$ is a random variable,
			\begin{equation} \label{eq:Sum of Random Variables}
				S_{n} = \sum_{i=1}^{n} X_{i} = X_{1} + X_{2} + \ldots + X_{n}
			\end{equation}
		\end{definition}
	
	\subsubsection{Means and Variances of Sums of Random Variables} \label{subsubsec:Means and Variances of Sums of Random Variables}
		\begin{definition}[Mean of Sums of Random Variables] \label{def:Mean of Sums of Random Variables}
			The \emph{mean of sums of random variables} is the same as the \emph{expected value of sums of random variables}.
			\begin{equation}
				\ExpectedValue \left[ S_{n} \right] = \sum_{i=1}^{n} \ExpectedValue \left[ X_{i} \right]
			\end{equation}
			\begin{remark}
				All the properties of \nameref{subsubsec:Properties of Discrete Expected Value} and/or \nameref{subsubsec:Properties of Continuous Expected Value} hold true here as well..
			\end{remark}
		\end{definition}
		\begin{definition}[Variance of Sums of Random Variables] \label{def:Variance of Sums of Random Variables}
			The defintion of the \emph{variance of sums of random variables} is the same as we have been using them previously, \nameref{subsec:Variance of Single Discrete} and \nameref{subsec:Variance of Single Continuous}.
			\begin{equation} \label{eq:Variance of Sums of Random Variables}
				\Variance \left[ S_{n} \right] = \Variance \left[ \sum_{i=1}^{n} X_{i} \right] = \sum_{i=1}^{n} \Variance \left[ X_{i} \right] + \sum_{j=1}^{n} \sum_{\substack{k=1 \\ j \neq k}}^{n} \Covariance \left[ X_{j}, X_{k} \right]
			\end{equation}
			\begin{remark} \label{rmk:Variance of Sums of Independent Random Variables}
				If $X_{1}, X_{2}, \ldots , X_{n}$ are independent, then:
				\begin{equation} \label{eq:Variance of Sums of Independent Random Variables}
					\Variance \left[ \sum_{i=1}^{n} X_{i} \right] = \sum_{i=1}^{n} \Variance \left[ X_{i} \right]
				\end{equation}
			\end{remark}
		\end{definition}
		\begin{definition}[Independent and Identically Distributed] \label{def:Independent and Identically Distributed}
			We say that $X_{1},X_{2},\ldots,X_{n}$ are \emph{Independent and Identically Distributed (iid)} random variables if $X_{i}$ are drawn independently from the same population/probability distribution.
			\begin{subequations}
				\begin{equation} \label{eq:Mean of Independent and Identically Distributed}
					\sum_{i=1}^{n} \ExpectedValue \left[ X_{i} \right] = n \mu
				\end{equation}
				\begin{equation} \label{eq:Variance of Independent and Identically Distributed}
					\Variance \left[ S_{n} \right] = n \sigma^{2}
				\end{equation}
			\end{subequations}
			\begin{itemize}[noitemsep, nolistsep]
				\item $\mu$ is the mean of a random variable $X_{i}$
				\item $\sigma^{2}$ is the variance of a random variable $X_{i}$.
			\end{itemize}
		\end{definition}
	
	\subsection{Sample Mean} \label{subsec:Sample Mean}
		\begin{definition}[Sample Mean] \label{def:Sample Mean}
			The \emph{sample mean} of a sequence is denoted as,
			\begin{equation} \label{eq:Sample Mean}
				\bar{X} = M_{n} = \frac{\sum_{i=1}^{n} X_{i}}{n}
			\end{equation}
		\end{definition}
		\begin{definition}[Expected Value of Sample Mean] \label{def:Expected Value of Sample Mean}
			The \emph{expected value of the sample mean} is defined as:
			\begin{equation} \label{eq:Expected Value of Sample Mean}
				\ExpectedValue \left[ \bar{X} \right]
				= \ExpectedValue \left[ M_{n} \right]
				= \frac{\ExpectedValue \left[ S_{n} \right]}{n}
				= \frac{n \mu}{n}
				= \mu
			\end{equation}
			\begin{remark}
				The sample mean $M_{n}$ is an \emph{\nameref{def:Unbiased Estimator}} of population mean $\mu$.
			\end{remark}
		\end{definition}
		\begin{definition}[Variance of Sample Mean] \label{def:Variance of Sample Mean}
			The \emph{variance of the sample mean} is denoted as:
			\begin{equation} \label{eq:Variance of Sample Mean}
				\Variance \left[ \bar{X} \right]
				= \Variance \left[ M_{n} \right]
				= \Variance \left[ \frac{S_{n}}{n} \right]
				= \frac{1}{n^{2}} \Variance \left[ S_{n} \right]
				= \frac{\sigma^{2}}{n}
			\end{equation}
			\begin{remark}
				The larger $n$ gets, the smaller $\Variance \left[ M_{n} \right]$ gets, and the closer $M_{n}$ gets to $\mu$.
			\end{remark}
		\end{definition}
			
	Also, we can use the \nameref{eq:Chebychev Inequality} to approximate many values. In this case, we change the Chebychev Inequality from Equation~\eqref{eq:Chebychev Inequality} to Equation~\eqref{eq:Statistics Chebychev Inequality} like so:
		\begin{equation} \label{eq:Statistics Chebychev Inequality}
			P \left[ \lvert M_{n} - \ExpectedValue \left[ M_{n} \right] \rvert \geq \varepsilon \right] \leq \frac{\Variance \left[ M_{n} \right]}{\varepsilon^{2}}
		\end{equation}
		
	\subsection{Important Probability and Statistics Theorems} \label{subsec:Important Probability and Statistics Theorems}
		There are 3 very import theorems that are used quite frequently in both \nameref{sec:Probability Theory} and \nameref{sec:Statistics}.
		\begin{enumerate}[noitemsep, nolistsep]
			\item \nameref{thm:Weak Law of Large Numbers}
			\item \nameref{thm:Strong Law of Large Numbers}
			\item \nameref{thm:Central Limit Theorem}
		\end{enumerate}
		\begin{theorem}[Weak Law of Large Numbers] \label{thm:Weak Law of Large Numbers}
			Let $X_{1},X_{2},\ldots,X_{n}$ be a sequence of \nameref{def:Independent and Identically Distributed} random variables form a population with mean $\ExpectedValue \left[ X \right] = \mu$, then for $\varepsilon > 0$,
			\begin{equation} \label{eq:Weak Law of Large Numbers}
				\lim\limits_{n \rightarrow \infty} P \left[ \lvert M_{n} - \mu \rvert < \varepsilon \right] = 1
			\end{equation}
			\begin{remark*}
				In words this means, for large enough fixed values of $n$, $M_{n}$ is close to $\mu$ with high probability.
			\end{remark*}
		\end{theorem}
		\begin{theorem}[Strong Law of Large Numbers] \label{thm:Strong Law of Large Numbers}
			Let $X_{1},X_{2},\ldots,X_{n}$ be a sequence of \nameref{def:Independent and Identically Distributed} random variables form a population with mean $\ExpectedValue \left[ X \right] = \mu$ and finite variance, then
			\begin{equation} \label{eq:Strong Law of Large Numbers}
				P \left[ \lim\limits_{n \rightarrow \infty} M_{n} = \mu \right] = 1
			\end{equation}
			\begin{remark*}
				With probability 1, every sequence of sample mean calculations will eventually approach and stay close to the population mean.
			\end{remark*}
		\end{theorem}
		\begin{theorem}[Central Limit Theorem] \label{thm:Central Limit Theorem}
			Let $X_{1},X_{2},\ldots,X_{n}$ be a sequence of \nameref{def:Independent and Identically Distributed} random variables form a population with mean $\ExpectedValue \left[ X \right] = \mu < \infty$ and finite variance $\sigma^{2}$ and let
			\begin{equation*}
				Z_{n} = \frac{S_{n} - n\mu}{\sigma \sqrt{n}} = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
			\end{equation*}
			then,
			\begin{equation} \label{eq:Central Limit Theorem}
				\lim\limits_{n \rightarrow \infty} P \left[ Z_{n} \leq z \right] = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z} e^{-\frac{x^{2}}{2}} dx
			\end{equation}
			\begin{remark*}
				This means that over time, as you gain more and more sample means, they will start to resemble the \nameref{def:Gaussian Random Variable}, or the Normal Random Variable.
			\end{remark*}
		\end{theorem}
	
	\subsection{Estimators} \label{subsec:Estimators}
		\begin{itemize}[noitemsep, nolistsep]
			\item A \nameref{def:Statistic} is a function of the data $X_{1},X_{2},\ldots,X_{n}$
			\item An \emph{estimator} for a parameter, $\theta$, usually denoted $\hat{\theta}$, is also a statistic
		\end{itemize}
		\begin{definition}[Unbiased Estimator] \label{def:Unbiased Estimator}
			In general we say that a \nameref{def:Statistic} $\Theta (X)$ (a function of data $X_{1},X_{2},\ldots,X_{n}$) is an \emph{unbiased estimator} of a parameter $\theta$ if $\ExpectedValue \left[ W \left( X \right) \right] = \theta$.
			\begin{remark}[What makes a good estimator of any parameter, $\theta$?]
				A \emph{good estimator} of any parameter, $\theta$, should:
				\begin{itemize}[noitemsep, nolistsep]
					\item Give the correct value of $\theta$
					\item Not vary too much around $\theta$
				\end{itemize}
			\end{remark}
			\begin{remark}
				This is the definition of \emph{unbiased}, drawn from the definition of \nameref{def:Bias}
			\end{remark}
		\end{definition}
	
		\subsubsection{Goodness of an Estimator} \label{subsubsec:Estimator Goodness}
		There are 4 measures we use to determine how good our estimator is.
			\begin{enumerate}[noitemsep, nolistsep]
				\item \nameref{def:Bias}
				\item \nameref{def:Variance of Sample Mean}
				\item \nameref{def:Mean Squared Error}
				\item \nameref{def:Consistency}
			\end{enumerate}
		If our estimator is an \nameref{def:Unbiased Estimator}, then:
			\begin{itemize}
				\item Accuracy is defined as $\Bias [\hat{\theta} ] = \ExpectedValue [\hat{\theta}] - \theta$
				\item Precision is defined as $\Variance [\hat{\theta}]$
			\end{itemize}
			\begin{definition}[Bias] \label{def:Bias}
				\emph{Bias} is defined as:
				\begin{equation} \label{eq:Accuracy}
					\Bias [ \hat{\Theta} ] = \ExpectedValue [ \hat{\Theta} ] - \theta
				\end{equation}
				\begin{remark} \label{rmk:Unbiased}
					The estimator $\hat{\Theta}$ is \emph{unbiased} for $\theta$ if
					\begin{equation} \label{Unbiased}
						\ExpectedValue [ \hat{\Theta} ] = \theta
					\end{equation}
				\end{remark}
			\end{definition}
					
			\begin{definition}[Mean Squared Error] \label{def:Mean Squared Error}
				The \emph{Mean Squared Error} of an estimator for parameter $\hat{\theta}$ is:
				\begin{equation} \label{eq:Mean Squared Error}
					\MeanSqErr [ \hat{\theta} ]
					= \ExpectedValue \left[ \left( \hat{\theta} - \theta \right)^{2} \right]
					= \Variance \left[ \hat{\theta} \right] + \Bias^{2} \left[ \hat{\theta} \right]
				\end{equation}
			\end{definition}
			\begin{remark*}
				When doing statistical analysis, there is something called the \emph{Bias-Variance Tradeoff}.
				When doing the analysis, if you try to minimize bias, your variance will increase and vice-versa.
				There is a happy medium, which is not discussed in this class.
			\end{remark*}
			\begin{definition}[Consistency] \label{def:Consistency}
				$\hat{\theta}$ is a \emph{consistent estimator} for $\theta$ if $\hat{\theta}$ converges to $\theta$ in probability.
				\begin{equation} \label{eq:Consistency}
					\lim\limits_{n \rightarrow \infty} \Prob \left[ \lvert \hat{\theta} - \theta \rvert > \varepsilon \right] = 0
				\end{equation}
			\end{definition}
	
	\subsection{How to Find a Good Estimator} \label{subsec:Find Good Estimator}
	There are several methods, two of which are:
		\begin{enumerate}[noitemsep, nolistsep]
			\item Method of Moments
				\begin{itemize}[noitemsep, nolistsep]
					\item Sample Moments and Population Moments, $\bar{X}_{n} = \mu$
					\item You needs as many moments as parameters to get enough equations
				\end{itemize}
			\item \nameref{def:Maximum Likelihood Estimation}
		\end{enumerate}
	
		\subsubsection{Maximum Likelihood Estimation} \label{subsubsec:Maximum Likelihood Estimation}
			\begin{definition}[Maximum Likelihood Estimation] \label{def:Maximum Likelihood Estimation}
				Let $ X_{1},X_{2},\ldots,X_{n} \DrawnIID f \left( x \Given \theta \right) $.
				\begin{equation} \label{eq:Maximum Likelihood Estimation}
					\hat{\Theta}_{\MaxLikeEstim} = \argmax_{\theta \in \Theta} \Likelihood \left( \theta \Given x_{1},x_{2},\ldots,x_{n} \right)
				\end{equation}
				\begin{remark} \label{rmk:Likelihood}
					Likelihood, denoted $\Likelihood$ is defined as the \nameref{def:Joint PDF} of the \nameref{def:Random Sample} and its \nameref{rmk:Random Sample Parameters}.
					\begin{equation} \label{eq:Likelihood}
						\Likelihood \left( \theta \Given x_{1},x_{2},\ldots,x_{n} \right) = f_{X_{1}} \left( x_{1} \Given \theta \right) \cdot f_{X_{2}} \left( x_{2} \Given \theta \right) \cdot \ldots \cdot f_{X_{n}} \left( x_{n} \Given \theta \right)
					\end{equation}
				\end{remark}
				\begin{remark}
					It is often easier to maximize $\hat{\Theta}_{\MaxLikeEstim}$ over the log-likelihood.
					\begin{equation*}
						\hat{\Theta}_{\MaxLikeEstim} = \argmax_{\theta \in \Theta} \log \Likelihood \left( \theta \Given x_{1},x_{2},\ldots,x_{n} \right)
					\end{equation*}
				\end{remark}
				\begin{remark}
					\[ \argmax_{\theta \in \Theta} \] is the global maxima of the function. This is further described in Definition~\ref{def:argmax}.
				\end{remark}
			\end{definition}
	
	\subsection{Confidence Interval} \label{subsec:Confidence Interval}
		