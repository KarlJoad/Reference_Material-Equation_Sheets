\section{Multiple Random Variables} \label{sec:Multiple Random Variables}
	\subsection[Joint PMF]{Joint Probability Mass Function} \label{subsec:Joint PMF}
		\begin{definition}[Joint Probability Mass Function] \label{def:Joint PMF}
			The \emph{joint probability mass function (joint PMF)} of 2 discrete random variables $X$, $Y$ is defined as:
			\begin{equation} \label{eq:Joint PMF}
				p_{X,Y} = P \left[ \lbrace X=x \rbrace \cap \lbrace Y=y \rbrace \right] \text{ for all } x,y \in S_{X,Y}
			\end{equation}
			\begin{itemize}[noitemsep, nolistsep]
				\item This satisfies ALL properties of single random variable PMFs
			\end{itemize}
		\end{definition}
		\begin{example}[Problem 5.1]{Joint PMF}
			Problem 5.1, Part a \& Part B.
		\end{example}
	
		\subsubsection[Marginal PMF]{Marginal Probability Mass Function} \label{subsubsec:Marginal PMF}
			\begin{definition}[Marginal Probability Mass Function] \label{def:Marginal PMF}
				Given a joint PMF of discrete random variables $X$, $Y$, the \emph{Marginal Probability Mass Function (Marginal PMF)} of $X$ is defined as:
				\begin{equation} \label{eq:Marginal PMF}
					p_{X} \left( x_{i} \right) = P \left[ X = x_{i} \right] \text{ for } x_{i} \in S_{X}
				\end{equation}
				and is calculated as:
				\begin{equation} \label{eq:Calculate Marginal PMF}
					p \left( x_{i} \right) = \sum_{y \in S_{Y}} p_{X,Y} \left( x_{i}, y \right)
				\end{equation}
			\end{definition}
			\begin{example}[Problem 5.11]{Marginal PMFs}
				Problem 5.11, Part a \& Part b \& Part c.
			\end{example}
		
	\subsection[Joint CDF]{Joint Cumulative Distribution Function} \label{subsec:Joint CDF}
		\begin{definition}[Joint Cumulative Distribution Function] \label{def:Joint CDF}
			The \emph{Joint Cumulative Distribution Function (Joint CDF)} of $X$ and $Y$ is defined as the probability of the event $ \lbrace X \leq x \rbrace \cap \lbrace Y \leq y \rbrace $
			\begin{equation} \label{eq:Joint CDF}
				\begin{aligned}
					F_{X,Y} \left( x, y \right) &= P \left[ \lbrace X \leq x \rbrace \cap \lbrace Y \leq y \rbrace \right] \text{ for all } \left( x,y \right) \in \mathbb{R}^2 \\
					&= P \left[ \lbrace X \leq x \rbrace , \lbrace Y \leq y \rbrace \right]
				\end{aligned}
			\end{equation}
		\end{definition}
		
		\subsubsection{Properties of Joint Cumulative Distribution Functions} \label{subsubsec:Properties of Joint Cumulative Distribution Functions}
			\begin{propertylist}
				\item $F_{X,Y} \left( x,y \right)$ is non decreasing.
					\begin{equation} \label{eq:Joint CDF Property 1}
						F_{X,Y} \left( x_{1},y_{1} \right) \leq F_{X,Y} \left( x_{2},y_{2} \right) \text{ if } x_{1} \leq x_{2} \text{ and } y_{1} \leq y_{2}
					\end{equation}
				\item \begin{equation} \label{eq:Joint CDF Property 2}
						\begin{aligned}
							\lim\limits_{y \rightarrow -\infty} F_{X,Y} \left( x,y \right) &= 0 \\
							\lim\limits_{x \rightarrow -\infty} F_{X,Y} \left( x,y \right) &= 0 \\
							\lim\limits_{\left( x,y \right) \rightarrow \left( \infty, \infty \right)} F_{X,Y} \left( x,y \right) &= 1 \\
						\end{aligned}
					\end{equation}
				\item The Marginal CDFs can be obtained from the Joint CDF by removing restrictions for all but one variable.
					\begin{equation} \label{eq:Joint CDF Property 3}
						\begin{aligned}
							F_{X} \left( x \right) &= P \left[ \lbrace X \leq x \rbrace, \lbrace Y \text{ is anything} \rbrace \right] \\
													   &= P \left[ \lbrace X \leq x \rbrace, \lbrace -\infty \leq y \leq \infty \rbrace \right] \\
													   &= \lim\limits_{y \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
							F_{Y} \left( y \right) &= \lim\limits_{x \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
						\end{aligned}
					\end{equation}
				\item The Joint CDF is continuous from $\infty$ to $-\infty$.
					\begin{equation} \label{eq:Joint CDF Property 4}
						\begin{aligned}
							\lim\limits_{x \rightarrow a^{+}} F_{X,Y} \left( x,y \right) &= F_{X,Y} \left( a,y \right) \\
							\lim\limits_{y \rightarrow b^{+}} F_{X,Y} \left( x,y \right) &= F_{X,Y} \left( x,b \right) \\
						\end{aligned}
					\end{equation}
				\item The probability of the ``rectangle'' $\lbrace x_{1} \leq X \leq x_{2}, y_{1} \leq Y \leq y_{2} \rbrace$
					\begin{equation} \label{eq:Joint CDF Property 5}
						\begin{aligned}
							P \left[ \lbrace x_{1} \leq X \leq x_{2}, y_{1} \leq Y \leq y_{2} \rbrace \right] &= P \left[ \lbrace X \leq x_{2}, Y \leq y_{2} \rbrace \right] - P \left[ \lbrace X \leq x_{1}, Y \leq y_{2} \rbrace \right] - \\
							&P \left[ \lbrace X \leq x_{2}, Y \leq y_{1} \rbrace \right] + P \left[ \lbrace X \leq x_{1}, Y \leq y_{1} \rbrace \right] \\
							&= F_{X,Y} \left( x_{2}, y_{2} \right) - F_{X,Y} \left( x_{1}, y_{2} \right) - F_{X,Y} \left( x_{2}, y_{1} \right) + F_{X,Y} \left( x_{1}, y_{1} \right)
						\end{aligned}
					\end{equation}
			\end{propertylist}
	
		\subsubsection[Marginal CDF]{Marginal Cumulative Distribution Function} \label{subsubsec:Marginal CDF}
			\begin{definition}[Marginal Cumulative Distribution Function] \label{def:Marginal CDF}
				We obtain the \emph{Marginal Cumulative Distribution Functions (Marginal CDFs)} by removing the constraint on one of the variables. 
				\begin{equation} \label{eq:Marginal CDF}
					\begin{aligned}
						F_{X} \left( x \right) &= P \left[ \lbrace X \leq x \rbrace, \lbrace Y \text{ is anything} \rbrace \right] \\
						&= P \left[ \lbrace X \leq x \rbrace, \lbrace -\infty \leq y \leq \infty \rbrace \right] \\
						&= \lim\limits_{y \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
						F_{Y} \left( y \right) &= \lim\limits_{x \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
					\end{aligned}
				\end{equation}
			\end{definition}
			\begin{example}[Problem 5.20]{Marginal CDFs}
				Problem 5.20, Part b from Homework 8.
			\end{example}
		
	\subsection[Joint PDF]{Joint Probability Density Function} \label{subsec:Joint PDF}
		\begin{definition}[Joint Probability Density Function] \label{def:Joint PDF}
			We say that $X$, $Y$ are jointly continuous if the probabilities of events involving $X$ and $Y$ can be expressed as an integral of a \emph{Joint Probability Density Function (Joint PDF)}. \newline
			i.e. THere exists soem nonnegative function $f_{X,Y} \left( x,y \right)$, which we call the joint PDF, that is defined on the real plane such that tfor every event $B$ which is a subset of the xy plane
			\begin{equation}\label{eq:Joint PDF}
				P \left[ \left( X,Y \right) \text{in } B \right] = \iint_{B} f_{X,Y} \left( x,y \right) dx dy
			\end{equation}
			\begin{remark}
				The probability mass of an event is found by integrating the PDF over the region in the xy plane corresponding to your event.
			\end{remark}
		\end{definition}
		
	
		\subsubsection{Properties of Joint Probability Density Functions} \label{subsubsec:Joint PDF Properties}
			\begin{propertylist}
				\item 
					\begin{equation}
						\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y} \left( x,y \right) = 1
					\end{equation}
			\end{propertylist}
				\begin{example}[Example 5.16]{Find Normalizing Constant 1}
					Example 5.16 from the textbook.
				\end{example}
				\begin{example}[Problem 5.27]{Find Normalizing Constant 2}
					Problem 5.27, Part a form Homework 8.
				\end{example}
			\begin{propertylist}[resume]
				\item 
					\begin{equation}
						F_{X,Y} \left( x,y \right) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y} \left( s,t \right) dt ds
					\end{equation}
			\end{propertylist}
				\begin{example}[Problem 5.27]{Find Joint CDF}
					Problem 5.27, Part b from Homework 8.
				\end{example}
			\begin{propertylist}[resume]
				\item 
					\begin{equation}
						f_{X,Y} = \frac{\partial^{2} f_{X,Y} \left( x,y \right)}{\partial x \partial y} 
					\end{equation}
			\end{propertylist}
			
		\subsubsection{Marginal PDF} \label{subsubsec:Marginal PDF}
			\begin{definition}[Marginal Probability Density Function] \label{def:Marginal PDF}
				The \emph{Marginal Probability Density Functions (Marginal PDFs)} $f_{X} \left( x \right)$ and $f_{Y} \left( y \right)$ are obtained by taking the derivative of the marginal CDFs.
				\begin{equation}
					\begin{aligned}
						f_{X} \left( x \right) &= \frac{d}{dx} F_{X} \left( x \right) \\
						&= \frac{d}{dx} \int_{-\infty}^{x} \left[ \int_{-\infty}^{\infty} f_{X,Y} \left( s,t \right) dt ds \right] \\
						&= \frac{d}{dx} \int_{-\infty}^{x} \int_{-\infty}^{\infty} f_{X,Y} \left( s,t \right) dt ds \\
						&\text{Simplified with \nameref{def:2nd Fundamental Theorem of Calculus}} \\
						&= \int_{-\infty}^{\infty} f_{X,Y} \left( x,t \right) dt \\
						f_{X} &= \int_{-\infty}^{\infty} f_{X,Y} \left( x,t \right) dt \\
					\end{aligned}
				\end{equation}
			\end{definition}
			\begin{example}[Problem 5.27]{Find Marginal PDFs}
				Problem 5.27, Part c from Homework 8.
			\end{example}
		
	\subsection{Independence of Multiple Random Variables} \label{subsec:Independence of Multiple Random Variables}
		\begin{definition}[Independent Random Variables] \label{def:Independence of Multiple Random Variables}
			\emph{$X$ and $Y$ are independent random variables} if \emph{\textbf{ANY}} event $A_{1}$ defined in terms of $S$ is independent of \emph{\textbf{ANY}} event $A_{2}$ defined in terms of $Y$.
			\begin{equation} \label{eq:Independence of Multiple Random Variables}
				P \left[ X \in A_{1}, Y \in A_{2} \right] = P \left[ X \in A_{1} \right] * P \left[ Y \in A_{2} \right]
			\end{equation}
		\end{definition}
	There are 3 ways to phrase this:
	\begin{enumerate}[noitemsep, nolistsep]
		\item For discrete random variables $X$ and $Y$, $X$ and $Y$ are independent if and only if:
			\begin{equation} \label{eq:Independence of Multiple Discrete Random Variables Using PMF}
				p_{X,Y} \left( x,y \right) = p_{X} \left( x \right) * p_{Y} \left( y \right)
			\end{equation}
	
		\item For general random variables $X$ and $Y$, $X$ and $Y$ are independent if and only if:
			\begin{equation} \label{eq:Independence of Multiple General Random Variables Using CDF}
				F_{X,Y} \left( x,y \right) = F_{X} \left( x \right) * F_{Y} \left( y \right)
			\end{equation}
	\end{enumerate}
		\begin{example}[Final Exam Practice, Problem 7]{Confirm Independence by PDF}
			Let $X$ and $Y$ have joint Probability Density Function (PDF):
			\begin{equation*}
				f_{X,Y} \left( x,y \right) = \begin{cases}
					12x \left( 1-x \right)y & 0 < x < 1, 0 < y < 1 \\
					0 & \text{otherwise}
				\end{cases}
			\end{equation*}
			Are $X$ and $Y$ independent?
		\end{example}
	\begin{enumerate}[resume]		
		\item For (continuous) random variables $X$ and $Y$, $X$ and $Y$ are independent if and only if:
			\begin{equation} \label{eq:Independence of Multiple Continuous Random Variables Using PDF}
				f_{X,Y} \left( x,y \right) = f_{X} \left( x \right) * f_{Y} \left( y \right)
			\end{equation}
	\end{enumerate}
		\begin{example}[Final Exam Practice, Problem 7]{Confirm Independence by CDF}
			Let $X$ and $Y$ have joint Cumulative Distribution Function (CDF):
			\begin{equation*}
				F_{X,Y} \left( x,y \right) = \begin{cases}
					6y^{2} \left( \frac{x^{2}}{2} - \frac{x^{3}}{3} \right) & 0 < x < 1, 0 < y < 1 \\
					0 & \text{otherwise}
				\end{cases}
			\end{equation*}
			Are $X$ and $Y$ independent?
		\end{example}
	You can prove \nameref{eq:Independence of Multiple Discrete Random Variables Using PMF}, \Cref{eq:Independence of Multiple Discrete Random Variables Using PMF}.
	\begin{proof}[Independence of Discrete Random Variables with PMF] \label{proof:Independence of Discrete Random Variables with PMF}
		
	\end{proof}
	\begin{theorem}[Independence of Random Functions] \label{thm:Independence of Random Functions}
		If random variables $X$, $Y$ are independent, then $g\left( X \right)$ and $h \left( Y \right)$ are also independent.
	\end{theorem}

	\subsection{Expected Value of Functions with 2 Random Variables} \label{subsec:Expected Value of Functions with 2 Random Variables}
		\begin{definition}[Expectation of a Function with 2 Random Variables] \label{def:Expectation of a Function with 2 Random Variables}
			Let $Z$ be a random variable described by the function $Z = g \left( X,Y \right)$.
			\begin{equation} \label{eq:Expected Value of a Function with 2 Random Variables}
				\ExpectedValue = 
				\begin{cases}
					\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g \left( x,y \right) \cdot f_{X,Y} \left( x,y \right) dx dy &
						\text{if $X$ and $Y$ are jointly continuous} \\
					\sum\limits_{i \in S_{X}} \sum\limits_{j \in S_{Y}} g \left( x_{i}, y_{j} \right) \cdot p_{X,Y} \left( x,y \right) &
						\text{if $X$ and $Y$ are both discrete} \\
				\end{cases}
			\end{equation}
			\begin{remark}[Expected Value of Sum of Random Variables] \label{rmk:Expected Value of Sum of Random Variables}
				You \emph{\textbf{do not}} need to assume independence to say:
				\begin{equation} \label{eq:Expected Value of Sum of Random Variables}
					\ExpectedValue \left[ X_{1}+X_{2}+\ldots+X_{n} \right] = \ExpectedValue \left[ X_{1} \right] + \ExpectedValue \left[ X_{2} \right] + \ldots + \ExpectedValue \left[ X_{n} \right]
				\end{equation}
			\end{remark}
			\begin{remark}[Expected Value of Product of Random Variables] \label{rmk:Expected Value of Product of Random Variables}
				If $X$ and $Y$ are independent, then
				\begin{equation} \label{eq:Expected Value of Product of Random Variables}
					\ExpectedValue \left[ g \left( X \right) h \left( Y \right) \right] = \ExpectedValue \left[ g \left( X \right) \right] \cdot \ExpectedValue \left[ h \left( Y \right) \right]
				\end{equation}
			\end{remark}
		\end{definition}

	\subsection{Joint Moments, Correlation, and Covariance} \label{subsec:Joint Moments, Correlation, and Covariance}
		\subsubsection{Joint Moments} \label{subsubsec:Joint Moments}
			\begin{definition}[The j,kth Moment] \label{def:jkth Moment}
				The \emph{j,kth moment of $X$ and $Y$} is:
				\begin{equation} \label{eq:jkth Moment}
					\ExpectedValue \left[ X^{j} Y^{k} \right] =
					\begin{cases}
						\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^{j} y^{k} \cdot f_{X,Y} \left( x,y \right) dx dy &
							\text{if $X$, $Y$ are jointly continuous} \\
						\sum\limits_{i \in S_{X}} \sum\limits_{\ell \in S_{Y}} x_{i}^{j} y_{l}^{k} \cdot p_{X,Y} \left( x_{i},y_{\ell} \right) & 
							\text{if $X$, $Y$ are discrete} \\
					\end{cases}
				\end{equation}
			\end{definition} 
		
		\subsubsection{Correlation} \label{subsubsec:Correlation}
			\begin{definition}[Correlation] \label{def:Correlation}
				The \emph{Correlation of $X$ and $Y$} is defined as the $1,1$ moment, i.e. $\ExpectedValue \left[ X^{1} Y^{1} \right]$.
				\begin{remark}
					If $X$, $Y$ are such that $\ExpectedValue \left[ X^{1} Y^{1} \right] = 0$, then we say that $X$, $Y$ are \emph{orthogonal}.
				\end{remark}
				\begin{remark}[Uncorrelated] \label{rmk:Uncorrelated}
					If $X$, $Y$ are such that $\ExpectedValue \left[ XY \right] = \ExpectedValue \left[ X \right] \ExpectedValue \left[ Y \right]$, then $X$ and $Y$ are \emph{uncorrelated}.
				\end{remark}
				\begin{remark}
					If $X$, $Y$ are independent, then they are uncorrelated; but if $X$ and $Y$ are uncorrelated, \emph{\textbf{they are not always independent}}.
				\end{remark}
			\end{definition}
			\begin{definition}[Correlation Coefficient] \label{def:Correlation Coefficient}
				The \emph{correlation coefficient of $X$, $Y$} is defined as
				\begin{equation} \label{eq:Correlation Coefficient}
					\rho_{X,Y} = \frac{\text{Cov} \left[ X,Y \right]}{\sigma_{X} \sigma_{Y}}
				\end{equation}
				\begin{remark}
					$\rho_{X,Y}$ only ranges $-1 \leq \rho_{X,Y} \leq 1$
				\end{remark}
				\begin{remark}
					The closer $\rho_{X,Y}$ is to $+1$, the closer $X$ and $Y$ are to having a positive linear relationship (Positive slope). \newline
					The closer $\rho_{X,Y}$ is to $-1$, the closer $X$ and $Y$ are to having a negative linear relationship (Negative slope). \newline
					If $\rho_{X,Y} = 0$, the $\text{Cov}\left[ X,Y \right] = 0$, which means that $X$ and $Y$ are \emph{uncorrelated}.
				\end{remark}
			\end{definition}
	
		\subsubsection{Covariance} \label{subsubsec:Covariance}
			\begin{definition}[Covariance] \label{def:Covariance}
				The \emph{covariance of $X$ and $Y$} is denoted:
				\begin{equation} \label{eq:Covariance-Form 1}
					\text{Cov} \left[ X,Y \right] = \ExpectedValue \left[ \left( X - \ExpectedValue \left[ X \right] \right) \left( Y - \ExpectedValue \left[ Y \right] \right)\right]
				\end{equation}
				\begin{equation} \label{eq:Covariance-Form 2}
					\text{Cov} \left[ X,Y \right] = \ExpectedValue \left[ XY \right] - \ExpectedValue \left[ X \right] \ExpectedValue \left[ Y \right]
				\end{equation}
			\end{definition}
		
		\subsection{Conditional Probability Functions} \label{subsec:Multiple Variable Conditional Probability Functions}
		There are 3 major cases for these:
			\begin{enumerate}[noitemsep, nolistsep]
				\item \nameref{subsubsec:2 Discrete Random Variables}
				\item \nameref{subsubsec:1 Discrete 1 Continuous Random Variables}
				\item \nameref{subsubsec:2 Continuous Random Variables}
			\end{enumerate}
					
			\subsubsection{2 Discrete Random Variables} \label{subsubsec:2 Discrete Random Variables}
				\begin{definition}[Conditional Probability Mass Function] \label{def:2 Discrete-Conditional PMF}
					The \emph{conditional Probability Mass Function (Conditional PMF)} of $Y$ given that $X=x$ is:
					\begin{equation} \label{eq:2 Discrete-Conditional PMF}
						p_{Y} \left( y \Given x \right)
							= \frac{P \left[ \lbrace Y=y \rbrace \cap \lbrace X=x \rbrace \right]}{P \left[ X=x \right]} 
							= \frac{p_{X,Y} \left( x,y \right)}{p_{X} \left( x \right)}
					\end{equation}
					\begin{remark}
						This also implies that
						\begin{equation} \label{eq:2 Discrete-Joint PMF}
							p_{X,Y} \left( x,y \right) = p_{Y} \left( y \Given x \right) \cdot p_{X} \left( x \right)
						\end{equation}
					\end{remark}
					\begin{remark}
						If $X$ and $Y$ are \emph{independent}, then:
						\begin{equation} \label{eq:2 Discrete-Independent Conditional PMF}
							p_{X} \left( y \Given x \right)
							= \frac{p_{X,Y} \left( x,y \right)}{p_{X} \left( x \right)}
							= \frac{p_{X} \left( x \right) p_{Y} \left( y \right)}{p_{X} \left( x \right)}
							= p_{Y} \left( y \right)
						\end{equation}
					\end{remark}
					\begin{remark}
						The \nameref{def:2 Discrete-Conditional PMF} of 2 discrete random variables satisfies all \nameref{subsubsec:Properties of Probability Mass Functions}.
					\end{remark}
				\end{definition}

			\subsubsection{1 Discrete and 1 Continuous Random Variable} \label{subsubsec:1 Discrete 1 Continuous Random Variables}
			For this section, let $X$ be a discrete random variable and $Y$ a continuous random variable.
				\begin{definition}[Conditional Cumulative Distribution Function] \label{def:1 Discrete 1 Continuous-Conditional CDF}
					The \emph{conditional Cumulative Distribution Function (Conditional CDF)} of $Y$ given that $X=x$ is:
					\begin{equation} \label{eq:1 Discrete 1 Continuous-Conditional CDF}
						F_{Y} \left( y \Given x \right) = P \left[ Y \leq y \Given X=x \right]
						= \frac{P \left[ \lbrace Y \leq y \rbrace \cap \lbrace X=x \rbrace \right]}{P \left[ X=x \right]}
					\end{equation}
					\begin{remark}
						If $X$ and $Y$ are \emph{independent}, then:
						\begin{equation} \label{eq:1 Discrete 1 Continuous-Independent Conditional CDF}
							F_{Y} \left( y \Given x \right)
							= \frac{F_{X,Y} \left( x,y \right)}{p_{X} \left( x \right)}
							= \frac{F_{Y} \left( y \right) p_{X} \left( x \right)}{p_{X} \left( x \right)}
							= F_{Y} \left( y \right)
						\end{equation}
						This also means that:
						\begin{equation*}
							P \left[ Y \leq y \Given X=x \right]
							= P \left[ Y \leq y \right] \cdot P \left[ X=x \right]
						\end{equation*}
					\end{remark}
					\begin{remark}
						The similar relations for independent random variables with their conditional and marginal probability functions does not hold true with this.
					\end{remark}
					\begin{remark}
						The \nameref{def:1 Discrete 1 Continuous-Conditional CDF} of 1 discrete random variable and 1 continuous random variable satisfies all \nameref{subsubsec:Properties of Cumulative Distribution Functions}.
					\end{remark}
				\end{definition}
				\begin{definition}[Conditional Probability Density Function] \label{def:1 Discrete 1 Continuous-Conditional PDF}
					The \emph{conditional Probability Distribution Function (Conditional PDF)} of $Y$ given $X=x$ is
					\begin{equation} \label{eq:1 Discrete 1 Continuous-Conditional PDF}
						f_{Y} \left( y \Given x \right) = \frac{d}{dy} F_{Y} \left( y \Given x \right)
					\end{equation}
					This also means,
					\begin{equation*}
						P \left[ Y \leq y \Given X=x \right]
						= \int_{y \in A} f_{Y} \left( y \Given x \right) dy
					\end{equation*}
					\begin{remark}
						The \nameref{def:1 Discrete 1 Continuous-Conditional PDF} of 1 discrete random variable and 1 continuous random variable satisfies all \nameref{subsubsec:Properties of Probability Density Functions}.
					\end{remark}
				\end{definition}
			
			\subsubsection{2 Continuous Random Variables} \label{subsubsec:2 Continuous Random Variables}
				\begin{definition}[Conditional Cumulative Distribution Function] \label{def:2 Continuous-Conditional CDF}
					The \emph{conditional Cumulative Distribution Function (Conditional CDF)} of $Y$ given $X=x$ for $X$ and $Y$ continuous random variables is:
					\begin{equation} \label{eq:2 Continuous-Conditional CDF}
						F_{Y} \left(y \Given x \right)
						= \lim\limits_{h\rightarrow 0} F_{Y} \left( y \Given x < X \leq \left( x+h \right) \right)
						= \frac{\int_{-\infty}^{y} f_{X,Y} \left( x,v \right) dv}{f_{X} \left( x \right)}
					\end{equation}
					\begin{remark}
						The \nameref{def:2 Continuous-Conditional CDF} of 2 continuous random variables satisfies all \nameref{subsubsec:Properties of Cumulative Distribution Functions}.
					\end{remark}
					\begin{remark}
						The similar relations for the conditional and marginal probability functions do not hold up for 2 continuous random variables too well.
					\end{remark}
				\end{definition}
			
				\begin{definition}[Conditional Probability Density Function] \label{def:2 Continuous-Conditional PDF}
					The \emph{conditional Probability Density Function (Conditional PDF)} of $Y$ given $X=x$ for $X$ and $Y$ continuous random variables is:
					\begin{equation} \label{eq:2 Continuous-Conditional PDF}
						f_{Y} \left( y \Given x \right)
						= \frac{d}{dy} F_{Y} \left( y \Given x \right)
						= \frac{f_{X,Y} \left( x,y \right)}{f_{X} \left( x \right)}
					\end{equation}
					\begin{remark}
						If $X$ and $Y$ are independent, then:
						\begin{equation} \label{eq:2 Continuous-Independent Conditional PDF}
							f_{X} \left( y \Given x \right)
							= \frac{f_{X,Y} \left( x,y \right)}{f_{X} \left( x \right)}
							= \frac{f_{X} \left( x \right) f_{Y} \left( y \right)}{f_{X} \left( x \right)}
							= f_{Y} \left( y \right)
						\end{equation}
					\end{remark}
					\begin{remark}
						The \nameref{def:2 Continuous-Conditional PDF} of 2 continuous random variables satisfies all \nameref{subsubsec:Properties of Probability Density Functions}.
					\end{remark}
				\end{definition}
			
	\subsection{Conditional Expectation of Multiple Random Variables} \label{subsec:Conditional Expectation of Multiple Variables}
		\begin{definition}[Conditional Expectation] \label{def:Conditional Expectation of Multiple Variables}
			The \emph{conditional expectation} of $Y$ given $X$ is:
			\begin{equation} \label{eq:Conditional Expectation of Multiple Variables}
				\ExpectedValue \left[ Y \Given X=x \right] = \int_{-\infty}^{\infty} y \cdot f_{Y} \left( y \Given x \right) dy
			\end{equation}
			\begin{remark}[Special Case]
				There is a special case when \emph{\textbf{both}} $X$ and $Y$ are discrete random variables.
				\begin{equation} \label{eq:Conditional Expectation of Multiple Discrete Variables}
					\ExpectedValue \left[ Y \Given X=x \right] = \sum_{y \in S_{Y}} y \cdot p_{Y} \left( y \Given x \right)
				\end{equation}
			\end{remark}
			\begin{remark}
				When calculating the \nameref{subsec:Conditional Expectation of Multiple Variables}, and they as for $\ExpectedValue \left[ Y \Given X=x \right]$, that means you \emph{\textbf{must}} consider all possible values that $X$ can take.
				This can be generalized to the equation below.
				\begin{equation} \label{eq:General Conditional Expectation of Multiple Variables}
					\ExpectedValue \left[ Y \Given X=x \right] = \sum_{x \in S_{X}} \left( \sum_{y \in S_{Y}} y \cdot p_{Y} \left( y \Given x \right) \right)
				\end{equation}
				This can be described. You must take a single value for $x$, and take it over all $y$'s, then take the next value for $x$, until you have exhausted all values in both $S_{X}$ and $X_{Y}$. \newline
				This can also be translated into the continuous case, but the discrete case is a little simpler to understand this generality.
			\end{remark}
			\begin{remark}
				$\ExpectedValue \left[ Y \Given X=x \right]$ is a function of $X$, so it can be written as $g \left( x \right) = \ExpectedValue \left[ Y \Given X=x \right]$.
				Thus, we can also say
				\begin{equation} \label{eq:Expected Value of Conditional Expected Value of Multiple Variables}
					\ExpectedValue \left[ g \left( x \right) \right] = \ExpectedValue \left[ \ExpectedValue \left[ Y \Given X \right] \right] = \ExpectedValue \left[ Y \right]
				\end{equation}
				\begin{subequations}
					\begin{align} \label{eq:Joint PDF of Multiple Continuous Random Variables}
						\ExpectedValue \left[ Y \right]
						= \ExpectedValue \left[ \ExpectedValue \left[ Y \Given X \right] \right]
						&= \int_{-\infty}^{\infty} \ExpectedValue \left[ Y \Given x \right] f_{X} \left( x \right) dx
						= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y f_{Y} \left( y \Given x \right) dy f_{X} \left( x \right) dx \\
					 \label{eq:Joint PDF of Multiple Discrete Random Variables}
						\ExpectedValue \left[ Y \right]
						= \ExpectedValue \left[ \ExpectedValue \left[ Y \Given X \right] \right]
						&= \sum_{x \in S_{X}} \ExpectedValue \left[ Y \Given x \right] p_{X} \left( x \right)
						= \sum_{x_{j} \in S_{X}} \sum_{y_{i} \in S_{Y}} y_{i} p_{Y} \left( y_{i} \Given x_{k} \right) p_{X} \left( x_{j} \right)
					\end{align}
				\end{subequations}
			\end{remark}
		\end{definition}
		\begin{proof}[Prove Expectation of Conditional Expected Value] \label{proof:Expected Value of Conditional Expected Value of Multiple Variables}
		\end{proof}