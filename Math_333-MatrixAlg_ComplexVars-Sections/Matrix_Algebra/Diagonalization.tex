\subsection{Diagonalization}\label{subsec:Diagonalization}
To start off with, we need some background knowledge on what we mean by \nameref{def:Diagonalization}.

\begin{definition}[Diagonal Matrix]\label{def:Diagonal_Matrix}
  A \emph{diagonal matrix} is a \nameref{def:Matrix} whose only non-zero elements are on the main diagonal of the matrix.

  In general, this is seen as:
  \begin{equation}\label{eq:Diagonal_Matrix}
    A =
    \begin{pmatrix}
      a_{1,1} & 0 & 0 & \cdots \\
      0 & a_{2,2} & 0 & \cdots \\
      \vdots & \ddots & \ddots & \vdots \\
      0 & \cdots & \cdots & a_{n, n}
    \end{pmatrix}
  \end{equation}
\end{definition}

\begin{definition}[Diagonalizable]\label{def:Diagonalizable}
  Let the \nameref{def:Matrix} $A_{n \by n}$.
  We say $A$ is \emph{diagonalizable} to a \nameref{def:Diagonal_Matrix} $D$ if there exists an invertible matrix $P$ such that \Cref{eq:Diagonalizable} holds true.

  \begin{equation}\label{eq:Diagonalizable}
    P^{-1} A P = D
  \end{equation}
\end{definition}

\begin{definition}[Diagonalization]\label{def:Diagonalization}
  Let the matrix $A_{n \by n}$ be \nameref{def:Diagonalizable} by an invertible \nameref{def:Matrix} $P$ to form a \nameref{def:Diagonal_Matrix} $D$.
  $P$ is called a matrix that implements the \emph{diagonalization} shown in \Cref{eq:Diagonalization}.

  \begin{equation}\label{eq:Diagonalization}
    AP = PD
  \end{equation}
\end{definition}

Now that we have the terms and definitions out of the way, we can see something interesting about \Cref{eq:Diagonalization}.
\begin{blackbox}
  Let $A_{2 \by 2}$, $P =
  \begin{pmatrix}
    p_{1} & p_{2}
  \end{pmatrix}
  $, and $D =
  \begin{pmatrix}
    d_{1} & 0 \\
    0 & d_{2}
  \end{pmatrix}$.

  This means that if we apply \Cref{eq:Diagonalization}:
  \begin{align*}
    AP &= PD \\
    A_{2 \by 2}
    \begin{pmatrix}
      p_{1} & p_{2}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      p_{1} & p_{2}
                    \end{pmatrix}
                              \begin{pmatrix}
                                d_{1} & 0 \\
                                0 & d_{2} \\
                              \end{pmatrix} \\
    \begin{pmatrix}
      A p_{1} & A p_{2}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      d_{1} p_{1} & d_{2} p_{2}
                    \end{pmatrix}
  \end{align*}

  This means that:
  \begin{align*}
    A p_{1} &= d_{1} p_{1} \\
    A p_{2} &= d_{2} p_{2}
  \end{align*}

  If we study this, we can see that $d_{1}$ and $d_{2}$ are \nameref{def:Eigenvalue}s of $A$ (Remember, $AX = \lambda X$).
  Therefore, $P$ is a \nameref{def:Matrix} that implements the \nameref{def:Diagonalization} using a matrix of corresponding \nameref{def:Eigenvector}s.
\end{blackbox}

\begin{example}[Lecture 19, Example 1]{Matrices Implementing Diagonalization}
  Let $A_{3 \by 3}$.
  Is $A$ \nameref{def:Diagonalizable}?
  Can $A$ be diagonalized to $B$, $C$, $E$, or $F$?
  If yes, what are $D$ and $P$?
  \begin{equation*}
    A =
        \begin{pmatrix}
          1 & 3 & 12 \\
          0 & 6 & 8 \\
          0 & 0 & 2
        \end{pmatrix}
  \end{equation*}
  \begin{align*}
    B &=
        \begin{pmatrix}
          0 & 0 & 0 \\
          0 & 2 & 0 \\
          0 & 0 & 6
        \end{pmatrix} &
    C &=
        \begin{pmatrix}
          6 & 0 & 0 \\
          0 & 6 & 0 \\
          0 & 0 & 2
        \end{pmatrix} \\
    E &=
        \begin{pmatrix}
          2 & 0 & 0 \\
          0 & 6 & 0 \\
          0 & 0 & 2
        \end{pmatrix} &
    F &=
        \begin{pmatrix}
          6 & 0 & 0 \\
          0 & 2 & 0 \\
          0 & 0 & 2
        \end{pmatrix}
  \end{align*}
  \tcblower{}
  Start by finding the \nameref{def:Eigenvalue}s of $A$.
  \begin{align*}
    A - \lambda I &=
                    \begin{pmatrix}
                      2 - \lambda & 3 & 12 \\
                      0 & 6 - \lambda & 8 \\
                      0 & 0 & 2 - \lambda \\
                    \end{pmatrix} \\
    \det(A - \lambda I) &= (2 - \lambda) (6 - \lambda) (2 - \lambda) \\
    &= {(2 - \lambda)}^{2} (6 - \lambda)
  \end{align*}

  To get \nameref{def:Eigenvalue}s, $\det(A - \lambda I) = 0$.
  Therefore, the eigenvalues of $A$ are:
  \begin{align*}
    \lambda &= 2, \; \text{Algebraic Multiplicity 2} \\
    \lambda &= 6, \; \text{Algebraic Multiplicity 1}
  \end{align*}

  Now, we see that $B$ and $C$ are not possible \nameref{def:Diagonalization}s.
  \begin{description}[noitemsep]
  \item[$B$] Not all columns are \nameref{def:Eigenvalue}s.
  \item[$C$] Not the proper amount of $2$s in the matrix.
  \end{description}

  $E$ and $F$ are \textit{possible} \nameref{def:Diagonalization}s, but we need to know more about the corresponding \nameref{def:Eigenvector}s before we can answer them.
  So, let's find the eigenvectors.

  For $\lambda = 6$:
  \begin{align*}
    A - \lambda I &=
                    \begin{pmatrix}
                      -4 & 3 & 12 \\
                      0 & 0 & 8 \\
                      0 & 0 & -4 \\
                    \end{pmatrix} \\
    \begin{pmatrix}
      -4 & 3 & 12 \\
      0 & 0 & 8 \\
      0 & 0 & -4 \\
    \end{pmatrix}
    \begin{pmatrix}
      x_{1} \\ x_{2} \\ x_{3}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      0 \\ 0 \\ 0
                    \end{pmatrix}
  \end{align*}

  This matrix equation yields the system of equations:
  \begin{align*}
    -4x_{1} + 3x_{2} + 12x_{3} &= 0 \\
    8x_{3} &= 0 \\
    -4x_{3} &= 0
  \end{align*}

  This means that $x_{3} = 0$, implying $x_{1} = \frac{3}{4} x_{2}$.
  Therefore,
  \begin{align*}
    \begin{pmatrix}
      x_{1} \\ x_{2} \\ x_{3}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      \frac{3}{4} x_{2} \\ x_{2} \\ 0
                    \end{pmatrix} \\
    &= x_{2}
      \begin{pmatrix}
        \frac{3}{4} \\ 1 \\ 0
      \end{pmatrix} \\
    \intertext{Remember, any non-zero scalar multiple is allowed, so we can simplify this matrix.}
    &= x_{2}
      \begin{pmatrix}
        3 \\ 4 \\ 0
      \end{pmatrix}
  \end{align*}

  Now, we find the \nameref{def:Eigenvector} for $\lambda = 2$:
  \begin{align*}
    A - \lambda I &=
                    \begin{pmatrix}
                      0 & 3 & 12 \\
                      0 & 4 & 8 \\
                      0 & 0 & 0
                    \end{pmatrix} \\
    \begin{pmatrix}
      0 & 3 & 12 \\
      0 & 4 & 8 \\
      0 & 0 & 0
    \end{pmatrix}
              \begin{pmatrix}
                x_{1} \\ x_{2} \\ x_{3}
              \end{pmatrix} &=
                              \begin{pmatrix}
                                0 \\ 0 \\ 0
                              \end{pmatrix}
    0x_{1} + 3x_{2} + 12x_{3} &= 0 \\
    0x_{1} + 4x_{2} + 8x_{3} &= 0 \\
    0x_{1} + 0x_{2} + 0x_{3} &= 0 \\
  \end{align*}

  Therefore, we say:
  \begin{align*}
    \begin{pmatrix}
      x_{1} \\ x_{2} \\ x_{3}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      x_{1} \\ 0 \\ 0
                    \end{pmatrix} \\
    &= x_{1}
      \begin{pmatrix}
        1 \\ 0 \\ 0
      \end{pmatrix}
  \end{align*}

  Now, we can construct $P$ using any multiple of the \nameref{def:Eigenvector}'s scalar.
  \begin{align*}
    P &=
        \begin{pmatrix}
          \lambda = 6 & \lambda = 2 & \lambda = 2 \\
          3 & 1 & 1 \\
          4 & 0 & 0 \\
          0 & 0 & 0
        \end{pmatrix} \\
    &=
      \begin{pmatrix}
        1 & 3 & 1 \\
        0 & 4 & 0 \\
        0 & 0 & 0
      \end{pmatrix}
  \end{align*}

  Therefore, \textbf{both} $E$ and $F$ are possible \nameref{def:Diagonalization}s.
  However, because $P$ has two proportional columns, $\det(P) = 0$.

  The \nameref{def:Determinant} being equal to zero means $P$ is not invertible.
  Therefore, $A$ is \textbf{not} \nameref{def:Diagonalizable}.
\end{example}

\begin{example}[Lecture 19, Example 2]{Diagonalizable Matrix}
  Is the \nameref{def:Matrix} $B$ \nameref{def:Diagonalizable}?
  If so, what is $D$ and $P$?
  \begin{equation*}
    B = \begin{pmatrix}
      2 & 3 & 6 \\
      0 & 6 & 8 \\
      0 & 0 & 2
    \end{pmatrix}
  \end{equation*}
  \tcblower{}
  First, we start by finding the \nameref{def:Eigenvalue}s.
  \begin{align*}
    B - \lambda I &=
                    \begin{pmatrix}
                      2-\lambda & 3 & 6 \\
                      0 & 6-\lambda & 8 \\
                      0 & 0 & 2-\lambda
                    \end{pmatrix} \\
    \det(B - \lambda I) &= (2-\lambda) (6-\lambda) (2-\lambda)
  \end{align*}

  We get \nameref{def:Eigenvalue}s if and only if $\det(B-\lambda I) = 0$.
  Thus, our eigenvalues are:
  \begin{description}[noitemsep]
  \item $\lambda = 2$, with algebraic multiplicity 2
  \item $\lambda = 6$, with algebraic multiplicity 1
  \end{description}

  Now we find the \nameref{def:Eigenvector}s. \\
  For $\lambda = 6$:
  \begin{align*}
    \begin{pmatrix}
      -4 & 3 & 6 \\
      0 & 0 & 8 \\
      0 & 0 & -4
    \end{pmatrix}
              \begin{pmatrix}
                x_{1} \\ x_{2} \\ x_{3}
              \end{pmatrix} &=
                              \begin{pmatrix}
                                0 \\ 0 \\ 0
                              \end{pmatrix} \\
    \intertext{Turning this into a system of linear equations.}
    -4x_{1} + 3x_{2} + 6x_{3} &= 0 \\
    0x_{1} + 0x_{2} + 8x_{3} &= 0 \\
    0x_{1} + 0x_{2} - 4x_{3} &= 0 \\
    \intertext{Thus, $x_{3} = 0$.}
    -4x_{1} + 3x_{2} + 0 &= 0 \\
    x_{1} &= \frac{3}{4} x_{2}
  \end{align*}

  Now, we can construct our solutions vector.
  \begin{align*}
    \begin{pmatrix}
      x_{1} \\ x_{2} \\ x_{3}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      \frac{3}{4}x_{2} \\ x_{2} \\ 0
                    \end{pmatrix} \\
    &= x_{2}
      \begin{pmatrix}
        \frac{3}{4} \\ 1 \\ 0
      \end{pmatrix} \\
    \intertext{By the definition of an eigenvector, we can scale this general vector by any scalar value and still have an eigenvector.}
    \begin{pmatrix}
      3 \\ 4 \\ 0
    \end{pmatrix}
  \end{align*}

  For $\lambda = 2$:
  \begin{align*}
    \begin{pmatrix}
      0 & 3 & 6 \\
      0 & 4 & 8 \\
      0 & 0 & 0
    \end{pmatrix}
              \begin{pmatrix}
                x_{1} \\ x_{2} \\ x_{3}
              \end{pmatrix} &=
                              \begin{pmatrix}
                                0 \\ 0 \\ 0
                              \end{pmatrix} \\
    \intertext{Turning this into a system of linear equations.}
    0x_{1} + 3x_{2} + 6x_{3} &= 0 \\
    0x_{1} + 4x_{2} + 8x_{3} &= 0 \\
    0x_{1} + 0x_{2} + 0x_{3} &= 0 \\
    \intertext{The first two equations are scalar multiples of the same equation.}
    x_{2} + 2x_{3} &= 0 \\
    x_{2} &= -2x_{3} \\
    \intertext{Lastly, $x_{1}$ must be a free variables.}
    x_{1} &= x_{1}
  \end{align*}

  Now, we can construct our solutions vector.
  \begin{align*}
    \begin{pmatrix}
      x_{1} \\ x_{2} \\ x_{3}
    \end{pmatrix} &=
                    \begin{pmatrix}
                      x_{1} \\ -2x_{3} \\ x_{3}
                    \end{pmatrix} \\
    &= x_{1}
      \begin{pmatrix}
        1 \\ 0 \\ 0
      \end{pmatrix} + x_{3}
    \begin{pmatrix}
      0 \\ -2 \\ 1
    \end{pmatrix}
  \end{align*}

  Now we can construct $P$ out of the \nameref{def:Eigenvector}s we found, and construct $D$ in correlation to $P$.
  \begin{equation*}
    P =
    \begin{pmatrix}
      1 & 0 & 3 \\
      0 & -2 & 4 \\
      0 & 1 & 0
    \end{pmatrix}
  \end{equation*}
  The columns are \nameref{def:Linearly_Independent}, because they are distinct \nameref{def:Eigenvector}s.
  Thus, $P$ must be invertible.
  Let's verify that.
  \begin{align*}
    \det(P) &= 1 c_{1,1} + 0 c_{2,1} + 0 c_{3,1} \\
    &= 1 {(-1)}^{1+1} \det
      \begin{pmatrix}
        -2 & 4 \\
        1 & 0
      \end{pmatrix} \\
            &= 1 \bigl( -2 (0) - 4 (1) \bigr) \\
            &= -4 \\
            &\neq 0
  \end{align*}

  Thus, $P$ is a \nameref{def:Matrix} of \nameref{def:Eigenvector}s which \textbf{is} invertible.
  Therefore $A$ is \nameref{def:Diagonalizable}.
  From the theory $P^{-1}AP=D$, we do not need to perform the matrix multiplication to get $D$.
  \begin{equation*}
    D =
    \begin{pmatrix}
      2 & 0 & 0 \\
      0 & 2 & 0 \\
      0 & 0 & 6
    \end{pmatrix}
  \end{equation*}
\end{example}

\subsubsection{Matrix Exponentials}\label{subsubsec:Matrix_Exponentials}
A reason to use \nameref{def:Diagonalization} is to solve for exponentials of a \nameref{def:Matrix} $A$.
From \Cref{eq:Diagonalizable}, we can see that:
\begin{align*}
  P^{-1} A P &= D \\
  AP &= PD \\
  A &= P D P^{-1}
\end{align*}

This is actually quite a useful tool for solving a variety of matrix problem.
\begin{equation}\label{eq:Diagonalizable_A}
  A = P D P^{-1}
\end{equation}

Say we wanted to compute $A^{n}$.
We could do it the way we have in the past (\Cref{subsubsec:Cayley-Hamilton_Express_Higher_Powers}), but that requires repeated substitution, and therefore repeated computation for earlier elements.
If we want a direct approach to calculate this, we now have all the tools for it.

\begin{blackbox}
  As an example, start with $A^{2}$ and $A^{3}$, where $A$ is any $m \by m$ matrix.
  \begin{align*}
    A^{2} &= A A \\
          &= (P A P^{-1}) (P A P^{-1}) \\
    \intertext{Only the middle $P^{-1}$ and $P$ cancel because matrix multiplication is \textbf{not} commutative.}
          &= P D^{2} P^{-1} \\
    A^{3} &= A A^{2} \\
          &= (P D P^{-1}) (P D^{2} P^{-1}) \\
          &= P D^{3} P^{-1}
  \end{align*}
\end{blackbox}

This is a general algorithm, where we can determine $A^{n}$ using \Cref{eq:Matrix_Exponent_Diagonalized}
\begin{equation}\label{eq:Matrix_Exponent_Diagonalized}
  A^{n} = P D^{n} P^{-1}
\end{equation}

Now, since $D$ is a \nameref{def:Diagonal_Matrix}, $D^{n}$ is very easy to compute.
\begin{align*}
  D_{2 \by 2} &=
                \begin{pmatrix}
                  d_{1} & 0 \\
                  0 & d_{2}
                \end{pmatrix} \\
  D^{2} &=
          \begin{pmatrix}
            d_{1} & 0 \\
            0 & d_{2}
          \end{pmatrix}
                \begin{pmatrix}
                  d_{1} & 0 \\
                  0 & d_{2}
                \end{pmatrix} \\
              &=
                \begin{pmatrix}
                  d_{1}^{2} & 0 \\
                  0 & d_{2}^{2}
                \end{pmatrix} \\
  D^{n} &=
          \begin{pmatrix}
            d_{1}^{n} & 0 \\
            0 & d_{2}^{n}
          \end{pmatrix}
\end{align*}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Math_333-MatrixAlg_ComplexVars-Reference_Sheet"
%%% End:
