\subsection{Orthogonal Matrices}\label{subsec:Orthogonal_Matrices}
There is a particular class of matrices that is very helpful, nice to work with, and has an interesting property.
This is an \nameref{def:Orthogonal_Matrix}.

\begin{definition}[Orthogonal Matrix]\label{def:Orthogonal_Matrix}
  An \emph{orthogonal matrix} is a \nameref{def:Matrix} that is made up of \nameref{def:Orthonormal_Set}s.
  Such a matrix, has the property shown in \Cref{eq:Orthogonal_Matrix_Property}.

  \begin{equation}\label{eq:Orthogonal_Matrix_Property}
    \Transpose{A} = A^{-1}
  \end{equation}
\end{definition}

\begin{definition}[Orthonormal Set]\label{def:Orthonormal_Set}
  An \emph{orthonormal set} is one whose members have unit length (i.e.\ $1$), and whose members are \nameref{def:Linearly_Independent}.

  \begin{remark}
    This is a combination of an orthogonal set and a normal set.
    \begin{description}[noitemsep]
    \item[Orthogonal] The set's component elements are \nameref{def:Linearly_Independent}.
      For vectors (single-column/row matrices), this can be proven by performing the dot product operation on each vector and verifying that it is equal to 0.
    \item[Normal] The set's component elements have a magnitude (vector magnitude in this case) of $1$.
    \end{description}
  \end{remark}
\end{definition}

To show that an \nameref{def:Orthogonal_Matrix} \textbf{requires} \nameref{def:Orthonormal_Set}s, take a small example.

\begin{blackbox}
  Let $A$ be an \nameref{def:Orthogonal_Matrix}.
  Define $A_{n \by 1}$, as:
  \begin{equation*}
    A =
    \begin{pmatrix}
      r_{1} \\ r_{2} \\ \vdots \\ r_{n}
    \end{pmatrix}
  \end{equation*}

  Then,
  \begin{equation*}
    \Transpose{A} =
    \begin{pmatrix}
      r_{1} & r_{2} & \cdots & r_{n}
    \end{pmatrix}
  \end{equation*}

  By the definition of an \nameref{def:Orthogonal_Matrix} and \nameref{def:Inverse_Matrix}, we know that $A \Transpose{A} = I$.
  This means that when multiplying these two matrices together on the $i$th row and $j$th column, we have 2 cases:
  \begin{equation*}
    A_{i \by 1} \Transpose{A}_{1 \by j} =
    \begin{cases}
      1 & \text{ when } i = j \\
      0 & \text{ when } i \neq j
    \end{cases}
  \end{equation*}

  This means that:
  \begin{description}[noitemsep]
  \item When $i \neq j$, then $r_{i} \perp r_{j}$.
  \item When $i = j$, then $r_{i} \Transpose{r_{j}} = 1$, meaning each row has unit length.
  \end{description}
\end{blackbox}

\begin{example}[Lecture 19, Example 3]{Verify Orthogonal Matrix}
  Verify that $A$ is an \nameref{def:Orthogonal_Matrix}?
  \begin{equation*}
    A =
    \begin{pmatrix}
      \frac{2}{3} & \frac{2}{3} & \frac{-1}{3} \\
      \frac{-1}{3} & \frac{2}{3} & \frac{2}{3} \\
      \frac{2}{3} & \frac{-1}{3} & \frac{2}{3}
    \end{pmatrix}
  \end{equation*}
  \tcblower{}
  Start by verifying the magnitude of each row.
  \begin{align*}
    \Magnitude{r_{1}} &= \sqrt{{(\frac{2}{3})}^{2} + {(\frac{2}{3})}^{2} + {(\frac{-1}{3})}^{2}} \\
                      &= \sqrt{\frac{4}{9} + \frac{4}{9} + \frac{1}{9}} \\
                      &= \sqrt{1} \\
                      &= 1
  \end{align*}
  Realistically, we should verify $\Magnitude{r_{2}} = 1$ and $\Magnitude{r_{3}} = 1$.
  However, because their numbers are the same, their magnitude will be the same.
  Thus, the sets are normal.

  Now, check if the sets are orthogonal.
  \begin{align*}
    r_{1} \cdot r_{2} &= \frac{-2}{9} + \frac{4}{9} - \frac{2}{9} \\
                     &= 0
  \end{align*}
  Realistically, we should verify $r_{2} \cdot r_{3} = 0$ and $r_{1} \cdot r_{3} = 0$.
  However, because their numbers are the same, their results will be the same.
  Thus, the sets are orthogonal.

  $\therefore$ $A$ is an \nameref{def:Orthogonal_Matrix}.
\end{example}

\subsubsection{Quadratic Forms}\label{subsubsec:Quadratic_Forms}
We can use a \nameref{def:Real_Symmetric_Matrix} to represent a \nameref{def:Quadratic_Form}.

\begin{definition}[Real Symmetric Matrix]\label{def:Real_Symmetric_Matrix}
  A \emph{real, symmetric matrix} is a matrix whose non-diagonal elements are mirrored across the diagonal.
  For example:
  \begin{equation*}
    \begin{pmatrix}
      1 & 3 & 5 \\
      3 & 28 & 7 \\
      5 & 7 & 15 \\
    \end{pmatrix}
  \end{equation*}
\end{definition}

A real symmetric matrix has two important properties.
\begin{propertylist}
\item The \nameref{def:Eigenvalue}s of the matrix are real ($\in \RealNumbers$).\label{prop:Real_Symmetric_Matrix-Real_Eigenvalues}
\item \nameref{def:Eigenvector}s corresponding to \textbf{distinct} \nameref{def:Eigenvalue}s are orthogonal to each other.\label{prop:Real_Symmetric_Matrix-Distinct_Eigenvectors}
\end{propertylist}

\begin{definition}[Quadratic Form]\label{def:Quadratic_Form}
  A \emph{quadratic form} is a polynomial equation.
  We can use a \nameref{def:Real_Symmetric_Matrix} and \nameref{def:Diagonalization} to turn the polynomial into a real symmetric matrix that we can then use to yield a conic section in the Cartesian plane.

  For our purposes, we choose to say $a, b, c \in \RealNumbers$.
  \begin{equation}\label{eq:Quadratic_Form}
    ax_{1}^{2} + 2bx_{2} + c =
    \begin{pmatrix}
      x_{1} & x_{2}
    \end{pmatrix}
    \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix}
    \begin{pmatrix}
      x_{1} \\ x_{2}
    \end{pmatrix}
  \end{equation}

  \begin{remark}[Complex Quadratic Form]\label{rmk:Complex_Quadratic_Form}
    A \nameref{def:Quadratic_Form} may be complex if $a$, $b$, or $c$ are complex.
    These can be solved in a similar fashion, but must be done with a \nameref{def:Hermitian_Matrix} instead.
  \end{remark}
\end{definition}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Math_333-MatrixAlg_ComplexVars-Reference_Sheet"
%%% End:
