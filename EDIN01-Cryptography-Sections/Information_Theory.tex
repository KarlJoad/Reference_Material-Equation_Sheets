\section{Information Theory}\label{sec:Information_Theory}
Information theory is heavily influenced by probability and its theories.
For \textbf{much} greater detail on some of the concepts presented here, refer to the \href{file:./Math_374-Reference_Sheet.pdf}{Math 374 - Probability and Statistics} document.

\begin{definition}[Random Experiment]\label{def:Random_Experiment}
  A \emph{random experiment} is an experiment whose outcome varies in an unpredictable fashion when performed under the same conditions.
\end{definition}

\subsection{Sample Space}\label{subsec:Sample_Space}
\begin{definition}[Sample Space]\label{def:Sample_Space}
  The \emph{sample space} is the set of \textbf{all} possible outcomes, the \nameref{def:Elementary_Event}s, denoted
  \begin{equation}\label{eq:Sample_Space}
    \Omega = \lbrace \omega_{1}, \omega_{2}, \ldots, \omega_{n} \rbrace
  \end{equation}
\end{definition}

\begin{definition}[Elementary Event]\label{def:Elementary_Event}
  The possible outcomes in a \nameref{def:Sample_Space} are called \emph{elementary event}s.
  In \Cref{eq:Sample_Space}, they are denoted
  \begin{equation}\label{eq:Elementary_Event}
    \omega_{i}
  \end{equation}
\end{definition}

\begin{definition}[Event]\label{def:Event}
  An \emph{event} is any subset of $\Omega$.
  \begin{equation}\label{eq:Event}
    E \subset \Omega
  \end{equation}
\end{definition}

\begin{definition}[Probability]\label{def:Probability}
  The \emph{probability} of an \nameref{def:Event} $E$ is given by
  \begin{equation}\label{eq:Probability}
    \Prob(E) = \sum\limits_{\omega \in E} \Prob(\omega)
  \end{equation}
\end{definition}

\begin{definition}[Random Variable]\label{def:Random_Variable}
  A \emph{random variable} $X$ is a function that assigns a real number $X(\zeta)$ to each outcome $\zeta$ in the \nameref{def:Sample_Space} of the \nameref{def:Random_Experiment}.
\end{definition}

\subsection{Discrete Random Variables}\label{subsec:Discrete_Random_Variables}
\begin{definition}[Discrete Random Variable]\label{def:Discrete_Random_Variable}
  A \emph{discrete random variable} is a \nameref{def:Random_Variable} that assumes values from a finite set, $\mathcal{X}$
  It is a mapping
  \begin{equation}\label{eq:Discrete_Random_Variable}
    X(\omega) : \Omega \mapsto \mathcal{X}
  \end{equation}
\end{definition}

\begin{definition}[Probability Distribution]\label{def:Probability_Distribution}
  A \nameref{def:Discrete_Random_Variable} has a \emph{probability distribution} $P(X=x)$
  \begin{equation}\label{eq:Probability_Distribution}
    \Prob(X=x) = \sum\limits_{\omega : X(\omega) = x} \Prob(x)
  \end{equation}

  If $E \subset \mathcal{X}$, then $X \in E$ is an \nameref{def:Event} and
  \begin{equation}\label{eq:7}
    \Prob(X \in E) = \sum\limits_{x \in E} \Prob(x)
  \end{equation}
\end{definition}

\begin{propertylist}
\item A pair of \nameref{def:Random_Variable}s defined on the same \nameref{def:Sample_Space} can be considered as a single \nameref{def:Random_Variable}, say $Z = (X,Y)$.
\item $Z$ takes values in $\mathcal{X} \times \mathcal{Y}$, with $Z(\omega) = \bigl( X(\omega), Y(\omega) \bigr)$
\item $\Prob(Z) = \Prob(X,Y)$
\item Consider the joint \nameref{def:Event} $G$ as both \nameref{def:Event}s $E$ and $F$ occurring. Then $G$ corresopnds to the \nameref{def:Event} $G = E \cap F$.
\item The probability of the joint event $\Prob(G) = \Prob(E,F) = \Prob(E \cap F)$
\end{propertylist}

\begin{definition}[Expected Value/Mean of Single Discrete Random Variable]\label{def:Expected_Value_of_Single_Discrete}
  The \emph{expected value} or \emph{mean} of a single discrete random variable $X$ is defined by
  \begin{equation}\label{eq:Expected_Value_of_Single_Discrete}
    m_{X} = \ExpectedValue \left[ X \right] = \sum_{x \in S_{X}} x \cdot p_{X} \left( x \right)
  \end{equation}
  \begin{remark}\label{rmk:Expected_Value_of_Single_Discrete_Countably_Infinite}
    If $X$ is countably infinite, you will have an infinite series that exists only if
    \begin{equation}\label{eq:Expected_Value_of_Single_Discrete_Countably_Infinite}
      \sum_{s \in S_{X}} \lvert x \rvert \cdot p_{X} \left( x \right)
    \end{equation}
    is absolutely convergent.
  \end{remark}
\end{definition}

\subsubsection{Independent Discrete Random Variables}\label{subsubsec:Independent_Discrete_Random_Variables}
\begin{definition}[Independent]\label{def:Independent}
  2 \nameref{def:Event}s, $E$ and $F$, are said to be \emph{independent} if
  \begin{equation}\label{eq:Independent}
    \Prob(E,F) = \Prob(E) \Prob(F)
  \end{equation}

  2 \nameref{def:Random_Variable}s, $X$ and $Y$, are said to be \emph{independent \nameref{def:Random_Variable}s} if
  \begin{equation}\label{eq:Independent_Random_Variables}
    \Prob(X=x, Y=y) = \Prob(X=x) \Prob(Y=y), \forall x \in \mathcal{X}, y \in \mathcal{Y}
  \end{equation}
\end{definition}

\subsubsection{Conditional Probability of  Discrete Random Variables}\label{subsubsec:Conditional_Probability_Discrete_Random_Variables}
\begin{definition}[Conditional Probability]\label{def:Conditional_Probability}
  The \emph{conditional probability} $\Prob(E \Given F)$ (Read as ``the probability of $E$ given $F$'') is defined as
  \begin{equation}\label{eq:Conditional_Probability}
    \Prob(E \Given F) = \frac{\Prob(E,F)}{\Prob(F)} \:\: \text{where } \Prob(F) \neq 0
  \end{equation}

  For \nameref{def:Random_Variable}s, a similar definition is used.
  \begin{equation}\label{eq:Conditional_Probability_Random_Variables}
    \Prob(X=x \Given Y=y) = \frac{\Prob(X=x, Y=y)}{\Prob(Y=y)}
  \end{equation}
\end{definition}

\subsection{Entropy}\label{subsec:Entropy}
\begin{definition}[Entropy]\label{def:Entropy}
  The \emph{entropy} $\Entropy(X)$ of a \nameref{def:Discrete_Random_Variable} $X$ is defined as
  \begin{equation}\label{eq:Entropy}
    \Entropy(X) = - \sum\limits_{x \in \mathcal{X}} \Prob(x) \log_{2}\bigl( \Prob(x) \bigr)
  \end{equation}

  \begin{remark}[Probability is 0]\label{rmk:Entropy_Probability_0}
    By convention,
    \begin{equation}\label{eq:Entropy_Probability_0}
      0 \log_{2} (0) = 0
    \end{equation}

    This can be justified by saying $\lim\limits_{x \rightarrow 0} x \log_{2}(x) = 0$
  \end{remark}

  \begin{remark}[Entropy Bits]\label{rmk:Entropy_Bits}
    Since this logarithm is expressed in base-2, the output has the units of ``bits''.
    \textbf{These are not the same as bits in a computer}.
    They are just a unit, if the natural logarithm $\ln$ were used, the unit would be ``nats''.
  \end{remark}

  \Cref{eq:Entropy} can be expressed using the \nameref{def:Expected_Value_of_Single_Discrete}.
  \begin{equation}\label{eq:Entropy-Expected_Value}
    \Entropy(X) = \ExpectedValue \left[ - \log_{2}\bigl( \Prob(X) \bigr) \right]
  \end{equation}

  The \nameref{def:Entropy} of multiple \nameref{def:Discrete_Random_Variable}s can be expressed as
  \begin{equation}\label{eq:Entropy-Multiple_Discrete_Random_Variables}
    \Entropy(X,Y) = - \sum\limits_{x \in \mathcal{X}, y \in \mathcal{Y}} \Prob(x, y) \log_{2} \bigl( \Prob(x, y) \bigr)
  \end{equation}
\end{definition}

\begin{itemize}[noitemsep]
\item The actual values of $X$ are not used or needed in the calculation of \nameref{def:Entropy}, only the \nameref{def:Probability_Distribution}
\item The \nameref{def:Entropy} of $X$ can be calculated even if $X$ does not take on numerical values.
\item An interpretation of \nameref{def:Entropy} can be seen as \emph{uncertainty} about the outcome of the \nameref{def:Random_Variable}.
\item If $X$ takes one value with $\Prob(X=x) = 1$, and all other values have $\Prob(X \neq x) = 0$, then the \nameref{def:Entropy} is 0 bits.
\item If the probabilities are evenly distributed among the possible outcomes, then the \nameref{def:Entropy} is $\log_{2}(\text{Num Outcomes})$.
\end{itemize}

\subsubsection{Properties of \nameref*{subsec:Entropy}}\label{subsubsec:Entropy_Properties}
\begin{propertylist}
\item If $X$ is a \nameref{def:Discrete_Random_Variable} taking values in the set $\mathcal{X} = \lbrace x_{1}, x_{2}, \ldots, x_{\SetOrder{\mathcal{X}}} \rbrace$, then\label{prop:Entropy_Bounds}
  \begin{equation}\label{eq:Entropy_Bounds}
    0 \leq \Entropy(X) \leq \log_{2} \SetOrder{\mathcal{X}}
  \end{equation}
\item $\Entropy(X) = 0$ if and only if $\Prob(x) = 1$ \textbf{for some} $x \in \mathcal{X}$.\label{prop:Entropy_Min_Bound}
\item $\Entropy(X) = \log_{2} \SetOrder{\mathcal{X}}$ if and only if $\Prob(x) = \frac{1}{\SetOrder{\mathcal{X}}}$ \textbf{for all} $x \in \mathcal{X}$.\label{prop:Entropy_Max_Bound}
\item If there are multiple \nameref{def:Discrete_Random_Variable}s, the \nameref{def:Entropy} of their combination can be broken up\label{prop:Entropy_Splitting}
  \begin{equation}\label{eq:Entropy_Splitting}
    \Entropy(X_{1}X_{2}\ldots X_{n}) = \Entropy(X_{1}) + \Entropy(X_{2} \Given X_{1}) + \Entropy(X_{3} \Given X_{1}X_{2}) + \cdots + \Entropy(X_{n} \Given X_{1}X_{2} \ldots X_{n-1})
  \end{equation}
\item \Cref{prop:Entropy_Splitting} leads to the very useful equation\label{prop:Entropy_Multiple_Variable_Equivalence}
  \begin{equation}\label{eq:Entropy_Multiple_Variable_Equivalence}
    \Entropy(XY) = \Entropy(X) + \Entropy(Y \Given X) = \Entropy(Y) + \Entropy(X \Given Y)
  \end{equation}
\item Due to \nameref{prop:Conditional_Entropy_Bounded_By_Entropy} in \Cref{par:Conditional_Entropy_Properties}, if and only if $X$ and $Y$ are \nameref{def:Independent},\label{prop:Entropy_Independent_Random_Variable}
  \begin{equation}\label{eq:Entropy_Independent_Random_Variable}
    \Entropy(XY) = \Entropy(X) + \Entropy(Y)
  \end{equation}
\end{propertylist}

\begin{definition}[Entropy of Binary \nameref*{def:Discrete_Random_Variable}]\label{def:Entropy_Binary_Discrete_Random_Variable}
  In the special case, when $\SetOrder{\mathcal{X}} = 2$, there is a special notation
  \begin{equation}\label{eq:24}
    \BinaryEntropy(p) = -p \log_{2} (p) - (1-p) \log_{2} (1-p)
  \end{equation}
\end{definition}

\subsubsection{Conditional Entropy}\label{subsubsec:Conditional_Entropy}
\begin{definition}[Conditional Entropy]\label{def:Conditional_Entropy}
  The \emph{conditional \nameref{def:Entropy}}, $\Entropy(X \Given Y)$ (Read as ``the entropy of $X$ given $Y$ occurs'') is defined as
  \begin{equation}\label{eq:Conditional_Entropy}
    \Entropy(X \Given Y) = - \sum\limits_{x \in \mathcal{X}, y \in \mathcal{Y}} \Prob(x, y) \log_{2} \bigl( \Prob(x \Given y) \bigr)
  \end{equation}

  \Cref{eq:Conditional_Entropy} can be expressed with \nameref{def:Expected_Value_of_Single_Discrete}.
  \begin{equation}\label{eq:Conditional_Entropy-Expected_Value}
    \Entropy(X \Given Y) = \ExpectedValue \left[ -\log_{2} \bigl( \Prob(X \Given Y) \bigr) \right]
  \end{equation}

  If $\Prob(y) \neq 0$, we can introduce the notation
  \begin{equation}\label{eq:Conditional_Entropy-Nonzero_Given}
    \Entropy(X \Given Y) = \sum\limits_{y \in \mathcal{Y}} \Prob(y) \Entropy(X \Given Y=y)
  \end{equation}
\end{definition}

\paragraph{Properties of \nameref*{subsubsec:Conditional_Entropy}}\label{par:Conditional_Entropy_Properties}
\begin{propertylist}
\item If $X$ and $Y$ are \nameref{def:Discrete_Random_Variable}s taking values from the sets $\mathcal{X} = \lbrace x_{1}, x_{2}, \ldots, x_{\SetOrder{\mathcal{X}}} \rbrace$ and $\mathcal{Y} = \lbrace y_{1}, y_{2}, \ldots, y_{\SetOrder{\mathcal{Y}}} \rbrace$, respectively, then\label{prop:Conditional_Entropy_Bounds}
  \begin{equation}\label{eq:Conditional_Entropy_Bounds}
    0 \leq \Entropy(X \Given Y) \leq \log_{2} \SetOrder{\mathcal{X}}
  \end{equation}
\item $\Entropy(X \Given Y) = 0$ if and only if, \textbf{for every} $y$, $\Prob(x \Given Y) = 1$ \textbf{for some} $x \in \mathcal{X}$.\label{prop:Conditional_Entropy_Min_Bound}
\item $\Entropy(x \Given Y) = \log_{2} \SetOrder{\mathcal{X}}$ if and only if \textbf{for every} $y$, $\Prob(x \Given y = \frac{1}{\SetOrder{\mathcal{X}}}$ \textbf{for all} $x \in \mathcal{X}$.\label{prop:Conditional_Entropy_Max_Bound}
\item Similar properties hold for $\Entropy(X \Given Y=y)$
\item The \nameref{def:Entropy} of a \nameref{def:Random_Variable}, $X$, can never increase by knowledge of the outcome of another \nameref{def:Random_Variable}.\label{prop:Conditional_Entropy_Bounded_By_Entropy}
  \begin{equation}\label{eq:Conditional_Entropy_Bounded_By_Entropy}
    \Entropy(X \Given Y) \leq \Entropy(X)
  \end{equation}
\end{propertylist}

\subsection{Relative Entropy}\label{subsec:Relative_Entropy}
\begin{definition}[Relative Entropy]\label{def:Relative_Entropy}
  The \emph{relative entropy} between 2 \nameref{def:Probability_Distribution}s of 2 \nameref{def:Discrete_Random_Variable}s, $P(X)$ and $Q(X)$ is defined as
  \begin{equation}\label{eq:Relative_Entropy}
    \RelativeEntropy \bigl( P(X) \Between Q(X) \bigr) = \sum\limits_{x \in \mathcal{X}} P(x) \log_{2} \left( \frac{P(x)}{Q(x)} \right)
  \end{equation}

  \Cref{eq:Relative_Entropy} can also be written using \nameref{def:Expected_Value_of_Single_Discrete}.
  \begin{equation}\label{eq:Relative_Entropy-Expected_Value}
    \RelativeEntropy \bigl( P(X) \Between Q(X) \bigr) =  \ExpectedValue_{P} \Biggl[ \log_{2} \left( \frac{P(x)}{Q(x)} \right) \Biggr]
  \end{equation}

  \begin{remark}[Distance]\label{rmk:Relative_Entropy_Distance}
    The \nameref{def:Relative_Entropy} is a measure of ``distance'' between 2 \nameref{def:Probability_Distribution}s.
    It can be thought of as the inefficiency of assuming the \nameref{def:Probability_Distribution} $Q(x)$ when $P(x)$ is actually correct.
  \end{remark}
\end{definition}


\subsection{Mutual Information}\label{subsec:Mutual_Information}
\subsubsection{Properties of \nameref*{subsec:Mutual_Information}}\label{subsubsec:Mutual_Information_Properties}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../EDIN01-Cryptography-Reference_Sheet"
%%% End:
