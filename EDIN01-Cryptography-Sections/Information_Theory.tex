\section{Information Theory}\label{sec:Information_Theory}
Information theory is heavily influenced by probability and its theories.
For \textbf{much} greater detail on some of the concepts presented here, refer to the \href{file:./Math_374-Reference_Sheet.pdf}{Math 374 - Probability and Statistics} document.

\begin{definition}[Random Experiment]\label{def:Random_Experiment}
  A \emph{random experiment} is an experiment whose outcome varies in an unpredictable fashion when performed under the same conditions.
\end{definition}

\subsection{Sample Space}\label{subsec:Sample_Space}
\begin{definition}[Sample Space]\label{def:Sample_Space}
  The \emph{sample space} is the set of \textbf{all} possible outcomes, the \nameref{def:Elementary_Event}s, denoted
  \begin{equation}\label{eq:Sample_Space}
    \Omega = \lbrace \omega_{1}, \omega_{2}, \ldots, \omega_{n} \rbrace
  \end{equation}
\end{definition}

\begin{definition}[Elementary Event]\label{def:Elementary_Event}
  The possible outcomes in a \nameref{def:Sample_Space} are called \emph{elementary event}s.
  In \Cref{eq:Sample_Space}, they are denoted
  \begin{equation}\label{eq:Elementary_Event}
    \omega_{i}
  \end{equation}
\end{definition}

\begin{definition}[Event]\label{def:Event}
  An \emph{event} is any subset of $\Omega$.
  \begin{equation}\label{eq:Event}
    E \subset \Omega
  \end{equation}
\end{definition}

\begin{definition}[Probability]\label{def:Probability}
  The \emph{probability} of an \nameref{def:Event} $E$ is given by
  \begin{equation}\label{eq:Probability}
    \Prob(E) = \sum\limits_{\omega \in E} \Prob(\omega)
  \end{equation}
\end{definition}

\begin{definition}[Random Variable]\label{def:Random_Variable}
  A \emph{random variable} $X$ is a function that assigns a real number $X(\zeta)$ to each outcome $\zeta$ in the \nameref{def:Sample_Space} of the \nameref{def:Random_Experiment}.
\end{definition}

\subsection{Discrete Random Variables}\label{subsec:Discrete_Random_Variables}
\begin{definition}[Discrete Random Variable]\label{def:Discrete_Random_Variable}
  A \emph{discrete random variable} is a \nameref{def:Random_Variable} that assumes values from a finite set, $\mathcal{X}$
  It is a mapping
  \begin{equation}\label{eq:Discrete_Random_Variable}
    X(\omega) : \Omega \mapsto \mathcal{X}
  \end{equation}
\end{definition}

\begin{definition}[Probability Distribution]\label{def:Probability_Distribution}
  A \nameref{def:Discrete_Random_Variable} has a \emph{probability distribution} $P(X=x)$
  \begin{equation}\label{eq:Probability_Distribution}
    \Prob(X=x) = \sum\limits_{\omega : X(\omega) = x} \Prob(x)
  \end{equation}

  If $E \subset \mathcal{X}$, then $X \in E$ is an \nameref{def:Event} and
  \begin{equation}\label{eq:7}
    \Prob(X \in E) = \sum\limits_{x \in E} \Prob(x)
  \end{equation}
\end{definition}

\begin{propertylist}
\item A pair of \nameref{def:Random_Variable}s defined on the same \nameref{def:Sample_Space} can be considered as a single \nameref{def:Random_Variable}, say $Z = (X,Y)$.
\item $Z$ takes values in $\mathcal{X} \times \mathcal{Y}$, with $Z(\omega) = \bigl( X(\omega), Y(\omega) \bigr)$
\item $\Prob(Z) = \Prob(X,Y)$
\item Consider the joint \nameref{def:Event} $G$ as both \nameref{def:Event}s $E$ and $F$ occurring. Then $G$ corresopnds to the \nameref{def:Event} $G = E \cap F$.
\item The probability of the joint event $\Prob(G) = \Prob(E,F) = \Prob(E \cap F)$
\end{propertylist}

\begin{definition}[Expected Value/Mean of Single Discrete Random Variable]\label{def:Expected_Value_of_Single_Discrete}
  The \emph{expected value} or \emph{mean} of a single discrete random variable $X$ is defined by
  \begin{equation}\label{eq:Expected_Value_of_Single_Discrete}
    m_{X} = \ExpectedValue \left[ X \right] = \sum_{x \in S_{X}} x \cdot p_{X} \left( x \right)
  \end{equation}
  \begin{remark}\label{rmk:Expected_Value_of_Single_Discrete_Countably_Infinite}
    If $X$ is countably infinite, you will have an infinite series that exists only if
    \begin{equation}\label{eq:Expected_Value_of_Single_Discrete_Countably_Infinite}
      \sum_{s \in S_{X}} \lvert x \rvert \cdot p_{X} \left( x \right)
    \end{equation}
    is absolutely convergent.
  \end{remark}
\end{definition}

\subsubsection{Independent Discrete Random Variables}\label{subsubsec:Independent_Discrete_Random_Variables}
\begin{definition}[Independent]\label{def:Independent}
  2 \nameref{def:Event}s, $E$ and $F$, are said to be \emph{independent} if
  \begin{equation}\label{eq:Independent}
    \Prob(E,F) = \Prob(E) \Prob(F)
  \end{equation}

  2 \nameref{def:Random_Variable}s, $X$ and $Y$, are said to be \emph{independent \nameref{def:Random_Variable}s} if
  \begin{equation}\label{eq:Independent_Random_Variables}
    \Prob(X=x, Y=y) = \Prob(X=x) \Prob(Y=y), \forall x \in \mathcal{X}, y \in \mathcal{Y}
  \end{equation}
\end{definition}

\subsubsection{Conditional Probability of  Discrete Random Variables}\label{subsubsec:Conditional_Probability_Discrete_Random_Variables}
\begin{definition}[Conditional Probability]\label{def:Conditional_Probability}
  The \emph{conditional probability} $\Prob(E \Given F)$ (Read as ``the probability of $E$ given $F$'') is defined as
  \begin{equation}\label{eq:Conditional_Probability}
    \Prob(E \Given F) = \frac{\Prob(E,F)}{\Prob(F)} \:\: \text{where } \Prob(F) \neq 0
  \end{equation}

  For \nameref{def:Random_Variable}s, a similar definition is used.
  \begin{equation}\label{eq:Conditional_Probability_Random_Variables}
    \Prob(X=x \Given Y=y) = \frac{\Prob(X=x, Y=y)}{\Prob(Y=y)}
  \end{equation}
\end{definition}

\subsection{Entropy}\label{subsec:Entropy}
\begin{definition}[Entropy]\label{def:Entropy}
  The \emph{entropy} $\Entropy(X)$ of a \nameref{def:Discrete_Random_Variable} $X$ is defined as
  \begin{equation}\label{eq:Entropy}
    \Entropy(X) = - \sum\limits_{x \in \mathcal{X}} \Prob(x) \log_{2}\bigl( \Prob(x) \bigr)
  \end{equation}

  \begin{remark}[Probability is 0]\label{rmk:Entropy_Probability_0}
    By convention,
    \begin{equation}\label{eq:Entropy_Probability_0}
      0 \log_{2} (0) = 0
    \end{equation}

    This can be justified by saying $\lim\limits_{x \rightarrow 0} x \log_{2}(x) = 0$
  \end{remark}

  \begin{remark}[Entropy Bits]\label{rmk:Entropy_Bits}
    Since this logarithm is expressed in base-2, the output has the units of ``bits''.
    \textbf{These are not the same as bits in a computer}.
    They are just a unit, if the natural logarithm $\ln$ were used, the unit would be ``nats''.
  \end{remark}

  \Cref{eq:Entropy} can be expressed using the \nameref{def:Expected_Value_of_Single_Discrete}.
  \begin{equation}\label{eq:Entropy-Expected_Value}
    \Entropy(X) = \ExpectedValue \left[ - \log_{2}\bigl( \Prob(X) \bigr) \right]
  \end{equation}

  The \nameref{def:Entropy} of multiple \nameref{def:Discrete_Random_Variable}s can be expressed as
  \begin{equation}\label{eq:Entropy-Multiple_Discrete_Random_Variables}
    \Entropy(X,Y) = - \sum\limits_{x \in \mathcal{X}, y \in \mathcal{Y}} \Prob(x, y) \log_{2} \bigl( \Prob(x, y) \bigr)
  \end{equation}
\end{definition}

\begin{itemize}[noitemsep]
\item The actual values of $X$ are not used or needed in the calculation of \nameref{def:Entropy}, only the \nameref{def:Probability_Distribution}
\item The \nameref{def:Entropy} of $X$ can be calculated even if $X$ does not take on numerical values.
\item An interpretation of \nameref{def:Entropy} can be seen as \emph{uncertainty} about the outcome of the \nameref{def:Random_Variable}.
\item If $X$ takes one value with $\Prob(X=x) = 1$, and all other values have $\Prob(X \neq x) = 0$, then the \nameref{def:Entropy} is 0 bits.
\item If the probabilities are evenly distributed among the possible outcomes, then the \nameref{def:Entropy} is $\log_{2}(\text{Num Outcomes})$.
\end{itemize}

\subsubsection{Properties of \nameref*{subsec:Entropy}}\label{subsubsec:Entropy_Properties}

\subsubsection{Conditional Entropy}\label{subsubsec:Conditional_Entropy}
\paragraph{Properties of \nameref*{subsubsec:Conditional_Entropy}}\label{par:Conditional_Entropy_Properties}

\subsubsection{Relative Entropy}\label{subsubsec:Relative_Entropy}

\subsection{Mutual Information}\label{subsec:Mutual_Information}
\subsubsection{Properties of \nameref*{subsec:Mutual_Information}}\label{subsubsec:Mutual_Information_Properties}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../EDIN01-Cryptography-Reference_Sheet"
%%% End:
