\documentclass[10pt,letterpaper,final,twoside,notitlepage]{article}
\usepackage[margin=.5in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm} % Gives us plain, definition, and remark to use in \theoremstyle{style}
\usepackage{graphicx}

\usepackage{hyperref} % Generate hyperlinks to referenced items
\usepackage{nameref} % Can make references by name to places
\usepackage{indentfirst} % Indents the first line of new paragraphs
\usepackage{ctable} % Greater control over tables and how they look
\usepackage{subcaption} % Allows for multiple figures in one Figure environment
\usepackage{textcomp}
\usepackage{gensymb} % Gives access to some characters, for example, the degree symbol
\usepackage{enumitem} % Provides [noitemsep, nolistsep] for more compact lists
\usepackage{chngcntr} % Allows us to tamper with the counter a little more
\usepackage{empheq} % Allow boxing of equations in special math environments

%\graphicspath{{./Drawings/Math_374}} % Uncomment this to use pictures in this document
%\numberwithin{equation}{section} % Uncomment this to number equations with section numbers too

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{definition}{Defn}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[definition]
%\counterwithin{definition}{subsection} % Uncomment to have definitions use section/subsection numbering

\newcounter{example}[section]
\newenvironment{example}[1]
	{\begin{center}
			#1 \\ [1ex]
			\begin{tabular}{|p{1.0\textwidth}|}
				 \hline \\
			 
		 	{
			 \newline \newline \hline}
		 \end{tabular}
	 \end{center}}

\author{Karl Hallsby}
\title{Reference Material}

\begin{document}
\section{Relative Frequency} \label{sec:Relative Frequency}
\begin{itemize}[noitemsep, nolistsep]
	\item $f_k (n) = \frac{N_k (n)}{n}$ $\leftarrow$ {\large \textbf{Relative Frequency}}
	\begin{itemize}[noitemsep, nolistsep]
		\item $k$ is the outcome
		\item $N_k (n)$ is the number of times outcome $k$
	\end{itemize}
	\item $\lim\limits_{n \rightarrow \infty} f_k (n) = p_k$ $\leftarrow$ {\large \textbf{Statistical Regularity}}
	\begin{itemize}[noitemsep, nolistsep]
		\item $p_k$ is the probability of event $k$ occurring
	\end{itemize}
\end{itemize}

	\subsection{Properties of Relative Frequencies} \label {subsec:Properties Relative Frequency}
	\begin{enumerate}[noitemsep, nolistsep]
		\item $f_k (n) = \frac{N_k (n)}{n}$
		\item $0 \leq N_k (n) \leq n$
		\item $0 \leq f_k (n) \leq 1 = \frac{0}{n} \leq \frac{N_k (n)}{n} \leq \frac{n}{n}$
		\item $\sum_{k=1}^{k} f_k (n) = \sum_{k=1}^{k} \frac{N_k (n)}{n} = \frac{\sum_{k=1}^{k} N_k (n)}{n} = \frac{n}{n} = 1$
		\item $\sum_{k=1}^{k} f_k (n) = 1$
		\item If events A and B are disjoint and event C is "A or B", then $F_C = F_A (n) + F_B (n)$
	\end{enumerate}

\section{Set Theory} \label{sec:Set Theory}
\begin{itemize}[noitemsep, nolistsep]
	\item A \emph{set} is a collection of objects, denoted by capital letters
	\item Denote the \emph{universal set, $U$}; consisting of all possible objects of interest in a given setting/application
	\item For any set $A$, we say that \emph{``$x$ is an element of $A$''}, denoted $x \in A$ if object $x$ of the universal set $U$ is contained in $A$
	\item We say that \emph{``$x$ is not an element of $A$''}, denoted $x \notin A$ if object $x$ of the universal set $U$ is not contained in $A$
	\item We say that \emph{``$A$ is a subset of $B$''}, denoted $A \subset B$ if every element in $A$ also belongs to $B$, $x \in A \rightarrow x \in B$
	\item The \emph{empty set, $\emptyset$} is defined as the set with no elements
		\begin{itemize}[noitemsep, nolistsep]
			\item The empty set is a subset of every set
		\end{itemize}
	\item Sets \emph{$A$ and $B$ are equal} if they contain the same elements. To show this:
		\begin{enumerate}[noitemsep, nolistsep]
			\item Enumerate the elements of each set
			\item Thm: $A=B \iff A \subset B$ AND $B \subset A$
		\end{enumerate}
	\item The \emph{union of 2 sets $A$, $B$}, denoted $A \cup B$ is defined as the set of outcomes that are either in $A$, or in $B$, or both
	\item The \emph{intersection fo 2 sets, $A$, $B$}, denoted $A \cap B$ is defined as the set of outcomes in $A$ and $B$
	\item The 2 sets $A$, $B$ are said to be \emph{disjoint or mutually exclusive} if $A \cap B = \emptyset$
	\item The \emph{complement of a set $A$}, denoted $A^{C}$ is defined as the set of elements of $U$ not in $A$
		\begin{itemize}[noitemsep, nolistsep]
			\item $A^{C} = \lbrace x \in U \vert x \notin A \rbrace$
		\end{itemize}
	\item \emph{Relative complement} or \emph{difference}, denoted $A-B$, is the set of elements in $A$ that are not in $B$
		\begin{itemize}[noitemsep, nolistsep]
			\item $A-B = A \cap B^{C}$
			\item $A^{C} = U - A$
		\end{itemize}
\end{itemize}

	\subsection{Properties of Set Operations} \label{subsec:Properties of Set Ops}
	Set Operators are:
	\begin{enumerate}
		\item Commutative, Equation~\eqref{eq:Set Ops-Commutative}
			\begin{equation} % Commutative
				\begin{aligned}
					A \cup B &= B \cup A \\
					A \cap B &= B \cap A \\
				\end{aligned}
				\label{eq:Set Ops-Commutative}
			\end{equation}
			
			\item Associative, Equation~\eqref{eq:Set Ops-Associative}
				\begin{equation} % Associative
					\begin{aligned}
						A \cup \left( B \cup C \right) &= \left( A \cup B \right) \cup C \\
						A \cap \left( B \cap C \right) &= \left( A \cap B \right) \cap C \\
					\end{aligned}
					\label{eq:Set Ops-Associative}
				\end{equation}
			
			\item Distributive, Equation~\eqref{eq:Set Ops-Distributive}
				\begin{equation} % Distributive
					\begin{aligned}
						A \cup \left( B \cap C \right) &= \left( A \cup B \right) \cap \left( A \cup C \right) \\
						A \cap \left( B \cup C \right) &= \left( A \cap B \right) \cup \left( A \cap C \right) \\
					\end{aligned}
					\label{eq:Set Ops-Distributive}
				\end{equation}
			
			\item Set Operations obey De Morgan's Laws, Equation~\eqref{eq:Set Ops-De Morgan's}
				\begin{equation} % De Morgan's
					\begin{aligned}
						\left( A \cup B \right)^{C} &= A^{C} \cap B^{C} \\
						\left( A \cap B \right)^{C} &= A^{C} \cup B^{C} \\
					\end{aligned}
					\label{eq:Set Ops-De Morgan's}
				\end{equation}
			
	\end{enumerate}
	Additionally, 
	\begin{definition}[Union of $n$ Sets] \label{def:Union of n Sets}
		The \emph{union of $n$ sets} $\bigcup_{k=1}^{n} A_{k} = A_{1} \cup A_{2} \cup A_{3} \cup \ldots \cup A_{n}$ is the set consisting of all elements such that $x \in A_{k}$ for some $1 \leq k \leq n$.
		\begin{itemize}[noitemsep, nolistsep]
			\item All sets need to be empty to make $\bigcup_{k=1}^{n} A_{k} = \emptyset$
		\end{itemize}
	\end{definition}
	\begin{definition}[Intersection of $n$ Sets] \label{def:Intersection of n Sets}
		The \emph{intersection of $n$ sets} $\bigcap_{k=1}^{n} A_{k} = A_{1} \cap A_{2} \cap A_{3} \cap \ldots \cap A_{n}$ is the set consisting of all elements such that $x \in a_{k}$ for all $1 \leq k \leq n$
		\begin{itemize}[noitemsep, nolistsep]
			\item Just one set needs to be empty to make $\bigcap_{k=1}^{n} A_{k} = \emptyset$
		\end{itemize}
	\end{definition}
	
\section{Probability Theory} \label{sec:Probability Theory}
There are 3 main components to \nameref{sec:Probability Theory}.
\begin{enumerate}[noitemsep, nolistsep]
	\item Set Theory
	\item Axioms of Probability
	\item Conditional Probability and Independence
\end{enumerate}

	\subsection{Random Experiments} \label{subsec:Random Experiments}
	\begin{definition}[Random Experiment] \label{def:Random Experiment}
		A \emph{random experiment} is an experiment whose outcome varies in an unpredictable fashion when performed under the same conditions.
	\end{definition}
	\begin{definition}[Sample Space] \label{def:Sample Space}
		A \emph{sample space, $S$} of a random experiment is the set of all possible experiments.
	\end{definition}
	\begin{definition}[Outcome/Sample Point] \label{def:Outcome}
		An \emph{outcome}, or \emph{sample point} of a random experiment is a result that cannot be decomposed into other results.
	\end{definition}
	\begin{definition}[Event] \label{def:Event}
		An \emph{event} corresponds to a subset of the sample space. We say an event occurs if and only if (iff) the outcome of the experiment is in the subset representing the event.
	\end{definition}
	\begin{definition}[Event Classes] \label{def:Event Classes}
		An \emph{event class} $\mathcal{F}$ is the collection of the all the events' sets. $\mathcal{F}$ should be closed under unions, intersections, and complements.
		\begin{itemize}[noitemsep, nolistsep]
			\item For $S$ finite, or countably infinite, then we can let $\mathcal{F}$ be all subsets of $S$.
			\item For $S$ uncountably infinite, instead we can let $\mathcal{F}$ consist of the subsets that can be obtained as countable unions and intersections of some sets of $\mathcal{F}$.
		\end{itemize}
	\end{definition}
	\begin{definition}[Probability Law] \label{def:Probability Law}
		A \emph{probability law} for a random experiment $E$, with sample space $S$, and an event class $\mathcal{F}$ is a rule that assigns to each event $A \in \mathcal{F}$ a number $P \left[A \right]$, called the probability of $A$ that satisfies the axioms:
		\begin{enumerate}[label=Axiom~\Roman*:, align=left, noitemsep, nolistsep] \label{subdef:Probability Law Axioms}
			\item $0 \leq P\left[ A \right]$
			\item $P \left[ S \right] = 1$
			\item If $A \cap B = \emptyset$, then $P \left[ A \cup B \right] = P \left[ A \right] + P \left[ B \right]$
			\item[Axiom III':] If $A_{1}$, $A_{2}$, $\ldots$ is a sequence of events such that $A_{i} \cap A_{j} = \emptyset$ for all $i \neq j$, then $P \left[ \bigcup_{k=1}^{\infty} A_{k} \right] = \sum_{k=1}^{\infty} P \left[ A_{k} \right]$
		\end{enumerate}
	\end{definition}
	
	\subsection{Probability Law Corollaries} \label{subsec:Probability Law Corollary}
		\begin{enumerate}[label=Axiom~\Roman*:, align=left, noitemsep, nolistsep] % Probability Law Axioms
			\item $0 \leq P\left[ A \right]$
			\item $P \left[ S \right] = 1$
			\item If $A \cap B = \emptyset$, then $P \left[ A \cup B \right] = P \left[ A \right] + P \left[ B \right]$
			\item[Axiom III':] If $A_{1}$, $A_{2}$, $\ldots$ is a sequence of events such that $A_{i} \cap A_{j} = \emptyset$ for all $i \neq j$, then $P \left[ \bigcup_{k=1}^{\infty} A_{k} \right] = \sum_{k=1}^{\infty} P \left[ A_{k} \right]$
		\end{enumerate}
		\begin{corollary} \label{cor:Probability Parts}
			$P \left[ A^{C} \right] = 1 - P \left[ A \right]$
		\end{corollary}
		\begin{corollary} \label{cor:Probability of Event}
			$P \left[ A \right] \leq 1$
		\end{corollary}
		\begin{corollary} \label{cor:Probability of Empty Set}
			$P \left[ \emptyset \right] = 0$
		\end{corollary}
		\begin{corollary} \label{cor: Probability Addition of Disjoint Pairs}
			If $A_{1}$, $A_{2}$, $\ldots$, $A_{n}$ are pairwise mutually exclusive ($A_{1} \cap A_{2} \cap \ldots \cap A_{n} = \emptyset$), then $P \left[ \bigcup_{k=1}^{n} \right] = \sum_{k=1}^{n} P \left[ A_{k} \right]$ for $n \geq 2$
		\end{corollary}
		\begin{corollary} \label{cor:Inclusion-Exclusion Principle to 2 Sets}
			$P \left[ A \cup B \right] = P \left[ A \right] + P \left[ B \right] - P \left[ A \cap B \right]$
		\end{corollary}
		\begin{corollary} \label{cor:Inclusion-Exclusion Principle to n Sets}
			$P \left[ A \cup B \right] = \sum_{j=1}^{n} P \left[ A_{j} \right] - \sum_{j<k} P \left[A_{j} \cap A_{k} \right] + \ldots + \left( -1 \right)^{n+1} P \left[ A_{1} \cap \ldots \cap A_{n} \right]$
		\end{corollary}
		\begin{corollary} \label{cor:Subset Probability to Superset}
			If $A \subset B$, then $P \left[ A \right] \leq P \left[ B \right]$
		\end{corollary}
	
	\subsection{Conditional Probability} \label{subsec:Conditional Probability}
		\begin{definition}[Conditional Probability] \label{def:Conditional Probability}
			The \emph{conditional probability} of event $A$ \textbf{GIVEN THAT} event $B$ occurred is denoted $P \left[ A \vert B \right]$ and is defined as
			\begin{equation} 
				P \left[ A \vert B \right] = \frac{P \left[ A \cap B \right]}{P \left[ B \right]}
			\end{equation}
		\end{definition}
		\begin{theorem}[Theorem of Total Probability]
			Let $B_{1}$, $B_{2}$, $\ldots$, $B_{n}$ be mutually exclusive events whose union equals the sample space $S$, i.e. $B_{1}$, $B_{2}$, $\ldots$, $B_{n}$ is a \emph{partition of $S$}.
		\end{theorem}
		\begin{definition}[Baye's Rule]
			Let $B_{1}$, $B_{2}$, $\ldots$, $B_{n}$ be a partition of sample space $S$.
			\begin{equation}
				P \left[ B_{j} \vert A \right] = \frac{P \left[ A \cap B_{j} \right]}{P \left[ A \right]} = \frac{P \left[ A \vert B_{j} \right] * P \left[ B_{j} \right]}{\sum_{k=1}^{n} P \left[ A \vert B_{k} \right] * P \left[ B_{k} \right]}
			\end{equation}
		\end{definition}
	
	\subsection{Event Independence} \label{subsec:Event Independence}
		\begin{definition}[Independent] \label{def:Event Independence}
			Two events $A$ and $B$ are \emph{independent} if 
			\begin{equation} \label{eq:Event Independence}
				P \left[ A \cap B \right] = P \left[ A \right] * P \left[ B \right], P\left[ A \right] \neq 0, P\left[ B \right] \neq 0
			\end{equation}
			\begin{itemize}[noitemsep, nolistsep]
				\item If $A \cap B = \emptyset$, the $A$ and $B$ are \textbf{dependent}.
				\item If checking for independence between more than 2 events, you must check each pair, each triple, etc. until you check the independence of each event against each other. For 3 events, $A$, $B$, $C$:
					\begin{itemize}[noitemsep, nolistsep]
						\item Check $P \left[ A \cap B \cap C \right] = P \left[ A \right] * P \left[ B \right] * P \left[ C \right]$
						\item Also need to check:
							\begin{enumerate}[noitemsep, nolistsep]
								\item $P \left[ A \cap B \right] = P \left[ A \right] * P \left[ B \right]$
								\item $P \left[ B \cap C \right] = P \left[ B \right] * P \left[ C \right]$
								\item $P \left[ A \cap C \right] = P \left[ A \right] * P \left[ C \right]$
							\end{enumerate}
					\end{itemize}
			\end{itemize}
		\end{definition}

\section{Counting} \label{sec:Counting}
	\subsection[Permutations]{Ordered Sampling with Replacement} \label{subsec:Ordered Sampling with Replacement} \label{subsec:Permutations}
		\begin{definition}[Permutations] \label{def:Ordered Sampling with Replacement}
			The number of distinct outcomes of an experiment, where the elements being samples are replaced between each sampling.
			\begin{equation*}
				\text{If } k = n \\
			\end{equation*}
			\begin{equation} \label{eq:Ordered Sampling with Replacement}
				\frac{n}{First} * \frac{n-1}{Second} * \frac{n-2}{Third} * \ldots * \frac{n-k-1}{kth \text{ Item}} = n!
			\end{equation}
		\end{definition}
	\subsection{Ordered Sampling without Replacement} \label{subsec:Ordered Sampling without Replacement}
		\begin{definition} \label{def:Ordered Sampling without Replacement}
			Choose $k$ elements in succession wihtout replacement from a population of $n$ distinct objects, where $k \leq n$
			\begin{equation} \label{eq:Ordered Sampling without Replacement}
				\frac{n}{First} * \frac{n-1}{Second} * \frac{n-2}{Third} * \ldots * \frac{n-k-1}{kth \text{ Item}}
			\end{equation}
		\end{definition}
	\subsection{Unordered Sampling with Replacement} \label{subsec:Unordered Sampling with Replacement}
		
	\subsection{Unordered Sampling without Replacement} \label{subsec:Unordered Sampling without Replacement}		
		\begin{definition} \label{def:Unordered Sampling with Replacement}
			The number of ways to choose $k$ items out of $n$ items.
			Said $n$ choose $k$:
			\begin{equation} \label{eq:Unordered Sampling with Replacement}
			\binom{n}{k} = \frac{n * (n-1) * (n-2) * \ldots * (n-k+1)}{k!} = \frac{n!}{k! \left( n-k \right)!}
			\end{equation}
		\end{definition}
		\begin{equation}
			\binom{n}{k} = \binom{n}{n-k}
		\end{equation}
		
\section{Single Discrete Random Variables} \label{sec:Single Discrete Random Variables}
	\begin{definition}[Random Variable]
		A \emph{random variable} $X$ is a function that assigns a real number $X \left( \zeta \right)$ to each outcome $\zeta$ in the sample space of the random experiment.
	\end{definition}
\section{Single Continuous Random Variables} \label{sec:Single Continuous Random Variables}

\section{Multiple Random Variables} \label{sec:Multiple Random Variables}
	\subsection[Joint PMF]{Joint Probability Mass Function} \label{subsec:Joint PMF}
		\begin{definition}[Joint Probability Mass Function] \label{def:Joint PMF}
			The \emph{joint probability mass function (joint PMF)} of 2 discrete random variables $X$, $Y$ is defined as:
			\begin{equation} \label{eq:Joint PMF}
				p_{X,Y} = P \left[ \lbrace X=x \rbrace \cap \lbrace Y=y \rbrace \right] \text{ for all } x,y \in S_{X,Y}
			\end{equation}
			\begin{itemize}[noitemsep, nolistsep]
				\item This satisfies ALL propoerties of single random variable PMFs
			\end{itemize}
		\end{definition}
	
		\subsubsection[Marginal PMF]{Marginal Probability Mass Function} \label{subsubsec:Marginal PMF}
			\begin{definition}[Marginal Probability Mass Function] \label{def:Marginal PMF}
				Given a joint PMF of discrete random variables $X$, $Y$, the \emph{Marginal Probability Mass Function (Marginal PMF)} of $X$ is defined as:
				\begin{equation} \label{eq:Marginal PMF}
					p_{X} \left( x_{i} \right) = P \left[ X = x_{i} \right] \text{ for } x_{i} \in S_{X}
				\end{equation}
				and is calculated as:
				\begin{equation} \label{eq:Calculate Marginal PMF}
					p \left( x_{i} \right) = \sum_{y \in S_{Y}} p_{X,Y} \left( x_{i}, y \right)
				\end{equation}
			\end{definition}
		
	\subsection[Joint CDF]{Joint Cumulative Distribution Function} \label{subsec:Joint CDF}
		\begin{definition}[Joint Cumulative Distribution Function] \label{def:Joint CDF}
			The \emph{Joint Cumulative Distribution Function (Joint CDF)} of $X$ and $Y$ is defined as the probability of the event $ \lbrace X \leq x \rbrace \cap \lbrace Y \leq y \rbrace $
			\begin{equation} \label{eq:Joint CDF}
				\begin{aligned}
					F_{X,Y} \left( x, y \right) &= P \left[ \lbrace X \leq x \rbrace \cap \lbrace Y \leq y \rbrace \right] \text{ for all } \left( x,y \right) \in \mathbb{R}^2 \\
					&= P \left[ \lbrace X \leq x \rbrace , \lbrace Y \leq y \rbrace \right]
				\end{aligned}
			\end{equation}
		\end{definition}
		\begin{enumerate}[label=\textbf{(\roman*)}, noitemsep, nolistsep]
			\item $F_{X,Y} \left( x,y \right)$ is non decreasing.
				\begin{equation} \label{eq:Joint CDF Property 1}
					F_{X,Y} \left( x_{1},y_{1} \right) \leq F_{X,Y} \left( x_{2},y_{2} \right) \text{ if } x_{1} \leq x_{2} \text{ and } y_{1} \leq y_{2}
				\end{equation}
			\item \begin{equation} \label{eq:Joint CDF Property 2}
					\begin{aligned}
						\lim\limits_{y \rightarrow -\infty} F_{X,Y} \left( x,y \right) &= 0 \\
						\lim\limits_{x \rightarrow -\infty} F_{X,Y} \left( x,y \right) &= 0 \\
						\lim\limits_{\left( x,y \right) \rightarrow \left( \infty, \infty \right)} F_{X,Y} \left( x,y \right) &= 1 \\
					\end{aligned}
				\end{equation}
			\item The Marginal CDFs can be obtained from the Joint CDF by removing restrictions for all but one variable.
				\begin{equation} \label{eq:Joint CDF Property 3}
					\begin{aligned}
						F_{X} \left( x \right) &= P \left[ \lbrace X \leq x \rbrace, \lbrace Y \text{ is anything} \rbrace \right] \\
													   &= P \left[ \lbrace X \leq x \rbrace, \lbrace -\infty \leq y \leq \infty \rbrace \right] \\
													   &= \lim\limits_{y \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
						F_{Y} \left( y \right) &= \lim\limits_{x \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
					\end{aligned}
				\end{equation}
			\item The Joint CDF is continuous from $\infty$ to $-\infty$.
				\begin{equation} \label{eq:Joint CDF Property 4}
					\begin{aligned}
						\lim\limits_{x \rightarrow a^{+}} F_{X,Y} \left( x,y \right) &= F_{X,Y} \left( a,y \right) \\
						\lim\limits_{y \rightarrow b^{+}} F_{X,Y} \left( x,y \right) &= F_{X,Y} \left( x,b \right) \\
					\end{aligned}
				\end{equation}
			\item The probability of the ``rectangle'' $\lbrace x_{1} \leq X \leq x_{2}, y_{1} \leq Y \leq y_{2} \rbrace$
				\begin{equation} \label{eq:Joint CDF Property 5}
					\begin{aligned}
						P \left[ \lbrace x_{1} \leq X \leq x_{2}, y_{1} \leq Y \leq y_{2} \rbrace \right] &= P \left[ \lbrace X \leq x_{2}, Y \leq y_{2} \rbrace \right] - P \left[ \lbrace X \leq x_{1}, Y \leq y_{2} \rbrace \right] - \\
						&P \left[ \lbrace X \leq x_{2}, Y \leq y_{1} \rbrace \right] + P \left[ \lbrace X \leq x_{1}, Y \leq y_{1} \rbrace \right] \\
						&= F_{X,Y} \left( x_{2}, y_{2} \right) - F_{X,Y} \left( x_{1}, y_{2} \right) - F_{X,Y} \left( x_{2}, y_{1} \right) + F_{X,Y} \left( x_{1}, y_{1} \right)
					\end{aligned}
				\end{equation}
		\end{enumerate}
	
		\subsubsection[Marginal CDF]{Marginal Cumulative Distribution Function} \label{subsubsec:Marginal CDF}
			\begin{definition}[Marginal Cumulative Distribution Function] \label{def:Marginal CDF}
				We obtain the \emph{Marginal Cumulative Distribution Functions (Marginal CDFs)} by removing the constraint on one of the variables. 
				\begin{equation} \label{eq:Marginal CDF}
					\begin{aligned}
						F_{X} \left( x \right) &= P \left[ \lbrace X \leq x \rbrace, \lbrace Y \text{ is anything} \rbrace \right] \\
						&= P \left[ \lbrace X \leq x \rbrace, \lbrace -\infty \leq y \leq \infty \rbrace \right] \\
						&= \lim\limits_{y \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
						F_{Y} \left( y \right) &= \lim\limits_{x \rightarrow \infty} F_{X,Y} \left( x,y \right) \\
					\end{aligned}
				\end{equation}
			\end{definition}
	\subsection[Joint PDF]{Joint Probability Density Function} \label{subsec:Joint PDF}
		\begin{definition}[Joint Probability Density Function] \label{def:Joint PDF}
			We say that $X$, $Y$ are jointly continuous if the probabilities of events involving $X$ and $Y$ can be expressed as an integral of a \emph{Joint Probability Density Function (Joint PDF)}. \newline
			i.e. THere exists soem nonnegative function $f_{X,Y} \left( x,y \right)$, which we call the joint PDF, that is defined on the real plane such that tfor every event $B$ which is a subset of the xy plane
			\begin{equation}\label{eq:Joint PDF}
				P \left[ \left( X,Y \right) in B \right] = \iint_{B} f_{X,Y} \left( x,y \right) dx dy
			\end{equation}
			\begin{remark}
				The probability mass of an event is found by integrating the PDF over the region in the xy plane corresponding to your event.
			\end{remark}
		\end{definition}
	
		\subsubsection{Properties} \label{subsubsec:Joint PDF Properties}
			\begin{gather}
				\iint_{B} f_{X,Y} \left( x,y \right) = 1 \\
				x \geq 0, y \geq 0 \forall x \forall y \\
			\end{gather}
			
		\subsubsection{Facts about Joint PDFs} \label{subsubsec:Joint PDF Facts}
			\begin{align}
				\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} &f_{X,Y} \left( x,y \right) = 1 \\
				F_{X,Y} \left( x,y \right) &= \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y} \left( s,t \right) dt ds \\
				f_{X,Y} &= \frac{\partial^{2} f_{X,Y} \left( x,y \right)}{\partial x \partial y} \\
			\end{align}
			
		\subsubsection{Marginal PDF} \label{subsubsec:Marginal PDF}
			\begin{definition}[Marginal Probability Density Function] \label{def:Marginal PDF}
				The \emph{Marginal Probability Density Functions (Marginal PDFs)} $f_{X} \left( x \right)$ and $f_{Y} \left( y \right)$ are obtained by taking the derivative of the marginal CDFs.
				\begin{equation}
					\begin{aligned}
						f_{X} \left( x \right) &= \frac{d}{dx} F_{X} \left( x \right) \\
						&= \frac{d}{dx} \int_{-\infty}^{x} \left[ \int_{-\infty}^{\infty} f_{X,Y} \left( s,t \right) dt ds \right] \\
						&= \frac{d}{dx} \int_{-\infty}^{x} \int_{-\infty}^{\infty} f_{X,Y} \left( s,t \right) dt ds \\
						&\text{Simplified with Fundametal Theorem of Calculus} \\
						&= \int_{-\infty}^{\infty} f_{X,Y} \left( x,t \right) dt \\
						f_{X} &= \int_{-\infty}^{\infty} f_{X,Y} \left( x,t \right) dt \\
					\end{aligned}
				\end{equation}
			\end{definition}

%====================================APPENDIX====================================
\appendix
\counterwithin{equation}{section}
\section{Trigonometry} \label{app:Trig}
	\subsection{Trigonometric Formulas} \label{subsec:Trig Formulas}
		\begin{align}
			\sin \left( \alpha \right) + \sin \left( \beta \right) &= 2 \sin \left( \frac{\alpha + \beta}{2} \right) \cos\left( \frac{\alpha - \beta}{2} \right)  \label{eq:Sin plus Sin with diff Angles}
		\end{align}
\section{Calculus} \label{app:Calculus}
\end{document}