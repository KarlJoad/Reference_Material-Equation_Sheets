\subsection{Queuing Disciplines}\label{subsec:Queuing_Disciplines}
\begin{definition}[Queuing Discipline]\label{def:Queuing_Discipline}
  A \emph{queuing discipline} is a way to order the packets to send them in a particular order.
  Depending on how we order, namely how we schedule the packets, we achieve different things.
  However, the overarching requirement is to minimize delay, improve throughput, increase busy time, and decrease idle time.

  There are 2 types of queuing disciplines:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:Work_Conserving}
  \item \nameref{def:Non_Work_Conserving}
  \end{enumerate}
\end{definition}

\begin{definition}[Work Conserving]\label{def:Work_Conserving}
  A \emph{work conserving} \nameref{def:Queuing_Discipline} always handles packets right away.
  Meaning, if there is a packet waiting in the queue and the server is sitting idle, it will always serve the packet.

  This is in contrast to a \nameref{def:Non_Work_Conserving} \nameref{def:Queuing_Discipline}.
\end{definition}

\begin{definition}[Non-Work Conserving]\label{def:Non_Work_Conserving}
  A \emph{non-work conserving} \nameref{def:Queuing_Discipline} may not handle packets right away.
  Meaning, if there is a packet waiting in the queue and the server is sitting idle, the server might not server the packet right away.
  Packets are switched only at \textbf{the right time towards the right destination}.

  While this type of \nameref{def:Queuing_Discipline} may not make intuitive sense, it has some advantages.
  \begin{itemize}[noitemsep]
  \item Reduces jitter
  \item It \emph{shapes} the traffic, making it predictable by holding traffic back until a certain time.
  \end{itemize}

  This is contrast to a \nameref{def:Work_Conserving} \nameref{def:Queuing_Discipline}.
\end{definition}

Traditionally, \nameref{def:FIFO} was used.
But, there are some other ones possible that improve the \nameref{def:Queuing_System} in various ways.

\subsubsection{First-In First-Out}\label{subsubsec:FIFO}
\begin{definition}[First-In First-Out]\label{def:FIFO}
  In a \emph{first-in first-out} \nameref{def:Queuing_Discipline}, the first packet that enters the queue is the first one served.
  This is a \nameref{def:Work_Conserving} \nameref{def:Queuing_Discipline}.
  This follows Kleinrock's Conservation Law.
  \begin{equation}\label{eq:Kleinrocks_Conservation_Law}
    C = \sum\limits_{n=1}^{N} \rho_{n} q_{n}
  \end{equation}
  \begin{itemize}[noitemsep]
  \item $C$: A constant value of throughput for the system.
  \item $\rho$: The \nameref{def:Occupancy} for each flow.
  \item $q$: The mean scheduler delay.
  \end{itemize}
  
  However, there are some problems:
  \begin{itemize}[noitemsep]
  \item Small packets are held up by large packets ahead of them in the queue. This leads to:
    \begin{itemize}[noitemsep]
    \item Larger average delays for smaller packets.
    \item Greater throughput for larger packets.
    \item Flows of greedy packets get better service.
    \end{itemize}
  \item Greedy TCP connections crowd out more altruistic connections.
    \begin{itemize}[noitemsep]
    \item If one connection does not back off, others may back off more.
    \item This is a feature of the TCP protocol, because this manages network congestion.
    \end{itemize}
  \end{itemize}
\end{definition}

\subsubsection{Fair Queuing (FQ)}\label{subsubsec:Fair_Queuing}
\begin{definition}[Fair Queuing]\label{def:Fair_Queuing}
  The \emph{fair queuing} \nameref{def:Queuing_Discipline} is a \nameref{def:Work_Conserving} one.
  There are multiple queues for each port, one for each source or flow of data to that port.
  The various queues are serviced in a round-robin fashion, where one packet from a queue is handled in a single cycle, before moving onto the next queue.
  This achieves load balancing among the different flows, meaning there is no advantage for a particular flow to be greedy.

  However, the drawback with this is that short packets are penalized as only one packet from one queue is sent per round.
\end{definition}

\subsubsection{Priority Queuing (PQ)}\label{subsubsec:Priority_Queuing}
\begin{definition}[Priority Queuing]\label{def:Priority_Queuing}
  \emph{Priority queuing} is a \nameref{def:Work_Conserving} \nameref{def:Queuing_Discipline}.
  There are $K$ queues that lead to the server.
  The $k$th queue is constrained by $1 \leq k \leq K$.
  The $k+1$th queue has a higher priority than the $k$th queue.
  The higher priority queue is served first.

  This has a simple implementation with low processing overhead.
  There needs to be an extra step where the packets are sorted into the correct priority queue, but that is quite efficient now.

  However, there is no fairness among the packets.
  The lower priority queues can be starved of service time if the higher queues are always full.
\end{definition}

\subsubsection{Processor Sharing (PS)}\label{subsubsec:Processor_Sharing}
\begin{definition}[Processor Sharing]\label{def:Processor_Sharing}
  A \emph{processor sharing} \nameref{def:Queuing_Discipline} is \nameref{def:Work_Conserving}.
  However, it is not practical, because it is modeled on sending individual bits rather than whole packets.
  It is more of a meta-\nameref{def:Queuing_Discipline}, because it forms the basis for \nameref{def:Bit_Round_Fair_Queuing}.

  In this discipline, there are multiple queues, just like in \nameref{def:Fair_Queuing}.
  One bit from each queue is sent per round of the round-robin.
  This prevents longer packets from getting an advantage.

  The system needs to figure out the virtual start and finish times for a given packet, $i$, of the $\alpha$ queue.
  \begin{equation}\label{eq:PS_Finish_Time}
    F_{i}^{\alpha} = S_{i}^{\alpha} + P_{i}^{\alpha}
  \end{equation}
  where
  \begin{itemize}[noitemsep]
  \item $F_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's finish time.
  \item $S_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's start time, which is defined in \Cref{eq:PS_Start_Time}.
  \item $P_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's time to send.
  \end{itemize}

  \begin{equation}\label{eq:PS_Start_Time}
    S_{i}^{\alpha} = \max \left( F_{i-1}^{\alpha}, A_{i}^{\alpha} \right)
  \end{equation}
\end{definition}

\subsubsection{Bit-Round Fair Queuing (BRFQ)}\label{subsubsec:Bit_Round_Fair_Queuing}
\begin{definition}[Bit-Round Fair Queuing]\label{def:Bit_Round_Fair_Queuing}
  The \emph{bit-round fair queuing} \nameref{def:Queuing_Discipline} is based off the \nameref{def:Processor_Sharing} discipline and inherits its \nameref{def:Work_Conserving} property.
  However, instead of sending individual bits, we send whole packets, but choose which to send based on the same calculations as \nameref{def:Processor_Sharing}
  Each data flow gets approximately $\frac{1}{N}$ of the overall bandwidth in the system, given there are $N$ flows.

  The virtual start and finish times are still computed.
  However, unlike the \nameref{def:Fair_Queuing} that \nameref{def:Processor_Sharing} is inspired by, the next packet that is sent is the one with the \textbf{earliest finish time}.

  The same equations are used as in \nameref{def:Processor_Sharing} (\Crefrange{eq:PS_Finish_Time}{eq:PS_Start_Time}).
\end{definition}

\subsubsection{Generalized Processor Sharing (GPS)}\label{subsubsec:Generalized_Processor_Sharing}
\begin{definition}[Generalized Processor Sharing]\label{def:Generalized_Processor_Sharing}
  \emph{Generalized processor sharing} is a \nameref{def:Work_Conserving} meta-\nameref{def:Queuing_Discipline}.
  Like in \nameref{def:Processor_Sharing}, we send some number of bits per round, specified by the weight $w_{\alpha}$.
  If a queue has a weighting of 5, then $w_{\alpha} = 5$ means 5 bits are sent from the $\alpha$th queue each round.
  This solves the problem in \nameref{def:Bit_Round_Fair_Queuing} of not being able to handle queues that output different amounts of information during each round.
  This is used to form the basis of \nameref{def:Weighted_Fair_Queuing}.

  There are slight generalizations that need to be made to \Crefrange{eq:PS_Finish_Time}{eq:PS_Start_Time}.
  They are shown below.
  \begin{equation}\label{eq:GPS_Finish_Time}
    F_{i}^{\alpha} = S_{i}^{\alpha} + \frac{P_{i}^{\alpha}}{w_{\alpha}}
  \end{equation}
  where
  \begin{itemize}[noitemsep]
  \item $F_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's finish time.
  \item $S_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's start time, defined in \Cref{eq:GPS_Start_Time}.
  \item $P_{i}^{\alpha}$: The $\alpha$th queue's $i$th packet's time to send.
  \item $w_{\alpha}$: The weight (number of bits sent per round) of the $\alpha$th queue.
  \end{itemize}

  \begin{equation}\label{eq:GPS_Start_Time}
    S_{i}^{\alpha} = \max \left( F_{i-1}^{\alpha}, A_{i}^{\alpha} \right)
  \end{equation}

  \begin{remark}[Service Differentiation]\label{rmk:GPS_Service_Differentiation}
    The ability to attach different weights to different queues means we now have a way to respond to different service levels.
    This leads to the concept of \emph{service differentiation}.
  \end{remark}
\end{definition}

\subsubsection{Weighted Fair Queuing (WFQ)}\label{subsubsec:Weighted_Fair_Queuing}
\begin{definition}[Weighted Fair Queuing]\label{def:Weighted_Fair_Queuing}
  \emph{Weighted fair queuing} is a \nameref{def:Work_Conserving} implementation of \nameref{def:Generalized_Processor_Sharing}.
  It also uses the same packet management strategy as \nameref{def:Bit_Round_Fair_Queuing}, of sending the packet with the \textbf{earliest virtual finish time}.

  By attaching a weight for the number of possible bits to send, we can guarantee a bound on delay.
  The maximum buffer/queue size needed is proportional to this delay.
\end{definition}

\begin{example}[Lecture 5]{WFQ vs. BRFQ}
  Given the packet reception table below, which arrive at the same time on the same link, find the packet transmission order?
  \begin{center}
    \begin{tabular}{ccc}
      \toprule
      Packet & Size & Flow \\
      \midrule
      1 & 100 & 1 \\
      2 & 100 & 1 \\
      3 & 60 & 2 \\
      4 & 120 & 2 \\
      5 & 60 & 2 \\
      \bottomrule
    \end{tabular}
\end{center}


  You may assume that the link can send one unit of each packet per time instance.
  \tcblower{}
\end{example}

\begin{example}[Assignment 2]{Packet Transmission Order}
  A router has 3 data flows and uses \nameref{def:Weighted_Fair_Queuing}.
  Flow~2 has 3 times as much bandwidth as Flow~1, and Flow~3 had 2 times as much bandwidth as Flow~1.
  Assuming all these packets arrive at the router at roughly the same time, in what order will the packets be sent?

  \begin{center}
    \begin{tabular}{ccc}
      \toprule
      \textbf{Packet} & \textbf{Size (bits)} & \textbf{Flow} \\
      \midrule
      1 & 120 & 1 \\
      2 & 150 & 1 \\
      3 & 60 & 2 \\
      4 & 90 & 2 \\
      5 & 120 & 2 \\
      6 & 120 & 3 \\
      7 & 40 & 3 \\
      8 & 200 & 3 \\
      \bottomrule
    \end{tabular}
\end{center}

  \tcblower{}
  First, we need to determine the weightings, $w_{\alpha}, \alpha \in [1, 3]$
  \begin{align*}
    w_{1} &= 1 \\
    w_{2} &= 3 \\
    w_{3} &= 2
  \end{align*}

  Now, we construct our ``transmission table''.
  We can assume that the first packet in each flow has a start time, $S_{i} = 0$.
  \begin{center}
    \begin{tabular}{cccccc}
      \toprule
      \textbf{Packet} & \textbf{Size} & \textbf{Flow} & $\mathbf{S_{i}}$ & \textbf{$\frac{P_{i}}{w_{i}}$} & \textbf{$F_{i}$} \\
      \midrule
      1 & 120 & 1 & 0 & & \\
      2 & 150 & 1 & & \\
      3 & 60 & 2 & 0 & & \\
      4 & 90 & 2 & & & \\
      5 & 120 & 2 & & & \\
      6 & 120 & 3 & 0 & \\
      7 & 40 & 3 & & & \\
      8 & 200 & 3 & & & \\
      \bottomrule
    \end{tabular}
\end{center}

\end{example}

\subsubsection{Class-Based Queuing (CBQ)}\label{subsubsec:Class_Based_Queuing}
\begin{definition}[Class-Based Queuing]\label{def:Class_Based_Queuing}
  \emph{Class-based queuing} is a meta-\nameref{def:Queuing_Discipline}, that is neither \nameref{def:Work_Conserving} or \nameref{def:Non_Work_Conserving}.
  It is a method of assigning fractions of overall bandwidth to a set of nodes within a class.
  Nodes that are within the same class may borrow bandwidth if the allocated node is not using it.
\end{definition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../ETSN10-Network_Architecture_Performance-Reference_Sheet"
%%% End:
