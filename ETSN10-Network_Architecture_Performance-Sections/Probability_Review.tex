\section{Probability Review}\label{sec:Probability_Review}
This section is meant to quick review and introduce the equations that will be used throughout this course.
It is not meant to be comprehensive and/or in-depth.
For more information about the topic of probability and statistics, refer to the \href{file:./Math_374-Reference_Sheet.pdf}{Math 374 - Probability and Statistics} document.

\subsection{Axioms of Probability}\label{subsec:Axioms_of_Probability}
\begin{definition}[Sample Space]\label{def:Sample_Space}
  The \emph{sample space} is the set of all possible outcomes in a random experiment.
  It is denoted with the capital Greek omega.

  \begin{equation}\label{eq:Sample_Space}
    \Omega
  \end{equation}
\end{definition}

\begin{definition}[Event]\label{def:Event}
  An \emph{event} is a subset of the \nameref{def:Sample_Space} that we are interested in.
  These are generally denoted with capital letters.

  \begin{equation}\label{eq:Event}
    A \subseteq \Omega
  \end{equation}
\end{definition}

\begin{definition}[Mutually Exclusive]\label{def:Mutually_Exclusive}
  Any two \nameref{def:Event}s are \emph{mutually exclusive} if the equation below holds.

  \begin{equation}\label{eq:Mutually_Exclusive}
    \Prob(A \cup B) = \Prob(A) + \Prob(B)
  \end{equation}
\end{definition}

Laws that follow from the above definitions (\Crefrange{def:Sample_Space}{def:Mutually_Exclusive}).
\begin{enumerate}[noitemsep]
\item The conjugate of the \nameref{def:Event} occurring, i.e.\ the \nameref{def:Event} \textbf{not} occurring is:
  \begin{equation}\label{eq:Conjugate_Event}
    \Prob(\bar{A}) = 1 - \Prob(A)
  \end{equation}

\item The probability of the union of 2 \nameref{def:Event}s is:
  \begin{equation}\label{eq:Union_Probability}
    \Prob(A \cap B) = \Prob(A) + \Prob(B) - \Prob(A \cup B)
  \end{equation}
  \begin{itemize}[noitemsep]
  \item If $A$ and $B$ are \nameref{def:Mutually_Exclusive}, then $\Prob(A \cup B) = 0$.
  \end{itemize}
\end{enumerate}

\subsection{Conditional Probability}\label{subsec:Conditional_Probability}
\begin{definition}[Conditional Probability]\label{def:Conditional_Probability}
  \emph{Conditional probability} is the probability of an \nameref{def:Event} occurring when it is known that another \nameref{def:Event} occurred.

  \begin{equation}\label{eq:Conditional_Probability}
    \Prob(A \Given B) = \frac{\Prob(A \cap B)}{\Prob(B)}
  \end{equation}
\end{definition}

\begin{definition}[Independent]\label{def:Events_Independent}
  \nameref{def:Event}s are \emph{independent} if the probability of the events' intersection is the same as their probabilities multipled together.

  \begin{equation}\label{eq:Events_Independent}
    \Prob(A \cap B) = \Prob(A) \Prob(B)
  \end{equation}

  \begin{remark}[\nameref*{def:Conditional_Probability} and \nameref*{def:Events_Independent} Events]
    If $A$ and $B$ are \nameref{def:Event}s and are \nameref{def:Events_Independent}, then

    \begin{equation}\label{eq:Conditional_Probability_Events_Independent}
      \begin{aligned}
        \Prob(A \Given B) &= \Prob(A) \\
        \Prob(B \Given A) &= \Prob(B) \\
      \end{aligned}
    \end{equation}
  \end{remark}
\end{definition}

\subsection{Random Variables}\label{subsec:Random_Variables}
\begin{definition}[Random Variable]\label{def:Random_Variable}
  A \emph{random variable} is a mapping from an \nameref{def:Event}'s outcome to a real number.

  There are 2 types of random variables, based on what the mapping ends up with:
  \begin{enumerate}[noitemsep]
  \item \nameref{def:Discrete_Random_Variable}s are mapped to integers, $\AllIntegers$.
  \item \nameref{def:Continuous_Random_Variable}s are mapped to the real numbers, $\RealNumbers$.
  \end{enumerate}
\end{definition}

\subsubsection{Discrete Random Variables}\label{subsubsec:Discrete_Random_Variables}
\begin{definition}[Discrete Random Variable]\label{def:Discrete_Random_Variable}
  A \emph{Discrete Random Variable} is one whose values are mapped from an \nameref{def:Event}'s outcome to the integer numbers ($\AllIntegers$).
  These \nameref{def:Random_Variable}s are drawn from outcomes that are finite (sides on a die) or countably infinite.

  The probability of a single value of the discrete random variable is denoted differently here than in the course material.
  The subscript refers to which discrete random variable we are working with (in this case $X$) and the variable in parentheses is the value we are calculating for (in this case $x \in X$).
  \begin{equation}\label{eq:Discrete_Random_Variable-Single_Value}
    p_{X}(x)
  \end{equation}

  The sum of all probabilities for values that the discrete random variable can take \textbf{must} sum to 1.
  \begin{equation}\label{eq:Discrete_Random_Variable-Sum_to_One}
    \sum\limits_{x \in X} p_{X}(x) = 1
  \end{equation}

  The mean or expected value of a discrete random variables is shown below:
  \begin{equation}\label{eq:Discrete_Random_Variable-Expected_Value}
    \begin{aligned}
      \mu = \sum\limits_{x \in X} x p_{X}(x) \\
      \ExpectedValue[x] = \sum\limits_{x \in X} x p_{X}(x) \\
    \end{aligned}
  \end{equation}

  The variance of a discrete random variable is how ``off'' a value from the random variable is from the mean/expected value.
  \begin{equation}\label{eq:Discrete_Random_Variable-Variance}
    \begin{aligned}
      \sigma^{2} &= \sum\limits_{x \in X} {\left( x - \mu \right)}^{2} p_{X}(x) \\
      \Variance[x] &= \sum\limits_{x \in X} {\bigl( x - \ExpectedValue[x] \bigr)}^{2} p_{X}(x) \\
    \end{aligned}
  \end{equation}

  The standard deviation is the square root of the variance.
  \begin{equation}\label{eq:Discrete_Random_Variable-Standard_Deviation}
    \begin{aligned}
      \sigma = \sqrt{\sigma^{2}} &= \sqrt{\sum\limits_{x \in X} {\left( x - \mu \right)}^{2} p_{X}(x)} \\
      \StdDev[x] = \sqrt{\Variance[x]} &= \sqrt{\sum\limits_{x \in X} {\bigl( x - \ExpectedValue[x] \bigr)}^{2} p_{X}(x)} \\
    \end{aligned}
  \end{equation}
\end{definition}

There are 5 different \nameref{def:Discrete_Random_Variable} distributions that we will be heavily utilizing in this course.

\paragraph{Uniform Random Variable}\label{par:Uniform_Random_Variable}
\begin{definition}[Uniform Random Variable]\label{def:Uniform_Random_Variable}
  The \emph{uniform random variable} is a \nameref{def:Discrete_Random_Variable} whose probabilities for each outcome is equal.

  For a \nameref{def:Discrete_Random_Variable} $X$, which has $\SetOrder{X}$ possible values,
  \begin{equation}\label{eq:Uniform_Random_Variable-Probability_Distribution}
    p_{X}(x) = \frac{1}{\SetOrder{X}}
  \end{equation}
\end{definition}

\begin{example}[Lecture 1]{Uniform Random Variable}
  For example, the roll of a die is typically modelled as a uniform random variable.
  Find the probability distribution function, the expected value, and the variance.
  \tcblower{}
  Let's assume this is a 6-sided die.
  And let's map each side's number to a value in the range of $X \in [1, 6]$.

  Using \Cref{eq:Uniform_Random_Variable-Probability_Distribution}, we can find the probability distribution easily.
  \begin{equation*}
    p_{X}(x) =
    \begin{cases}
      \frac{1}{6} & x = 1 \\
      \frac{1}{6} & x = 2 \\
      \frac{1}{6} & x = 3 \\
      \frac{1}{6} & x = 4 \\
      \frac{1}{6} & x = 5 \\
      \frac{1}{6} & x = 6 \\
    \end{cases}
  \end{equation*}

  Using \Cref{eq:Discrete_Random_Variable-Expected_Value}, we can find the the expected value/mean.
  \begin{equation*}
    \begin{aligned}
      \mu = \ExpectedValue[x] &= \sum\limits_{x=1}^{6}x p_{X}(x) \\
      &= \frac{1}{6} + \frac{2}{6} + \frac{3}{6} + \frac{4}{6} + \frac{5}{6} + \frac{6}{6} \\
      &= \frac{1+2+3+4+5+6}{6} \\
      &= \frac{21}{6} = 3.5
    \end{aligned}
  \end{equation*}

  Using \Cref{eq:Discrete_Random_Variable-Variance}, we can find the variance.
  \begin{equation*}
    \begin{aligned}
      \sigma^{2} = \Variance[x] &= \sum\limits_{x=1}^{6} {\bigl(x - \ExpectedValue[x] \bigr)}^{2} p_{X}(x) \\
      &= \sum\limits_{x=1}^{6} {(x-3.5)}^{2} \left( \frac{1}{6} \right) \\
      &= 2.91667
    \end{aligned}
  \end{equation*}

  Using \Cref{eq:Discrete_Random_Variable-Standard_Deviation}, we can find the standard deviation.
  \begin{equation*}
    \begin{aligned}
      \sigma = \StdDev[x]&= \sqrt{\sum\limits_{x=1}^{6} {\bigl(x - \ExpectedValue[x] \bigr)}^{2} p_{X}(x)} \\
      &= \sqrt{\sum\limits_{x=1}^{6} {(x-3.5)}^{2} \left( \frac{1}{6} \right)} \\
      &= \sqrt{2.91667} \\
      &= 1.70783
    \end{aligned}
  \end{equation*}
\end{example}

\paragraph{Bernoulli Random Variable}\label{par:Bernoulli_Random_Variable}
\begin{definition}[Bernoulli Random Variable]\label{def:Bernoulli_Random_Variable}
  The \emph{Bernoulli random variable} is one where \textbf{only one} test occurs, and there are only 2 outcomes.

  The probability of success is denoted
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Success}
    p_{X}(\text{success}) = p
  \end{equation}

  The probability of failure is denoted
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Failure}
    p_{X}(\text{failure}) = 1-p
  \end{equation}

  The mean/expected value is:
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Expected_Value}
    \mu = \ExpectedValue[x] = p
  \end{equation}

  The variance is:
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Variance}
    \sigma^{2} = \Variance[x] = (1-p) p
  \end{equation}
\end{definition}

\paragraph{Binomial Random Variable}\label{par:Binomial_Random_Variable}
\begin{definition}[Binomial Random Variable]\label{def:Binomial_Random_Variable}
  The \emph{binomial random variable} is one where $n$ trials are run with no stops for a success, where the \nameref{def:Random_Variable} in each run is a \nameref{def:Bernoulli_Random_Variable}.

  The probability of $k$ successes with $n$ trials is
  \begin{equation}\label{eq:Binomial_Random_Variable-Probability_Success}
    \binom{n}{k} p^{k} {(1-p)}^{n-k} = \frac{n!}{k! (n-k)!} {(1-p)}^{n-k}
  \end{equation}

  The mean/expected value after $n$ trials is
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Expected_Value}
    \mu = \ExpectedValue[x] = np
  \end{equation}

  The variance after $n$ trials is
  \begin{equation}\label{eq:Bernoulli_Random_Variable-Variance}
    \sigma^{2} = \Variance[x] = np (1-p)
  \end{equation}
\end{definition}

\paragraph{Geometric Random Variable}\label{par:Geometric_Random_Variable}
\begin{definition}[Geometric Random Variable]\label{def:Geometric_Random_Variable}
  The \emph{geometric random variable} is one where $n$ trials are run, where the $n$th trial is a success, meaning there are $n-1$ previous failures.
  The \nameref{def:Random_Variable} in each run is a \nameref{def:Bernoulli_Random_Variable}.

  This means \textbf{each trial} has a probability of success of
  \begin{equation}\label{eq:Geometric_Random_Variable-Probability_Success}
    p_{X}(\text{success}) = p
  \end{equation}

  And \textbf{each trial} has a probability of failure of
  \begin{equation}\label{eq:Geometric_Random_Variable-Probability_Failure}
    p_{X}(\text{failure}) = 1-p
  \end{equation}

  The mean/expected value is
  \begin{equation}\label{eq:Geometric_Random_Variable-Expected_Value}
    \mu = \ExpectedValue[x] = \frac{1}{p}
  \end{equation}

  The variance is
  \begin{equation}\label{eq:Geometric_Random_Variable-Variance}
    \sigma^{2} = \Variance[x] = \frac{1-p}{p^{2}}
  \end{equation}
\end{definition}

\paragraph{Poisson Random Variable}\label{par:Poisson_Random_Variable}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ETSN10-Network_Architecture_Performance-Reference_Sheet"
%%% End:
